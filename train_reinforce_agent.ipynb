{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoK10It3TUN2",
    "outputId": "115d4998-5725-4fe6-d9c4-7f5597f5eeff"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_built_with_cuda()\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from save_best_model import SaveBestModel\n",
    "from sandpile import Sandpile, run_sandpile_alone\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import datetime\n",
    "from rl_agents import Policy\n",
    "from agents import RLPolicyAgent\n",
    "from util import Directions\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "#RUN THIS ON COLAB\n",
    "ON_COLAB = False\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/Phase ML Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4YR0pTPWV3Dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /staging_area/reinforce-agent/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "model_nickname = 'reinforce-agent'\n",
    "\n",
    "output_dir = f'/staging_area/{model_nickname}/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# checkpoint_dir = 'checkpoints/'\n",
    "# if not os.path.exists(output_dir+checkpoint_dir):\n",
    "#     os.makedirs(output_dir+checkpoint_dir)\n",
    "\n",
    "\n",
    "best_model_name = 'best_agent.tar'\n",
    "save_best_model = SaveBestModel(output_dir+best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "Total Trainable Params: 23557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=102, out_features=64, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.0, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (10): GELU(approximate='none')\n",
       "    (11): Dropout(p=0.0, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (13): GELU(approximate='none')\n",
       "    (14): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def enum_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(rl_policy)\n",
    "\n",
    "\n",
    "# SET UP POLICY AGENT\n",
    "N_grid = 10\n",
    "num_hidden_layers = 4\n",
    "hidden_dim = 64\n",
    "input_dim = N_grid**2 + 2# The number of input variables. \n",
    "output_dim = len(Directions) # The number of output variables. \n",
    "\n",
    "rl_policy = Policy(\n",
    "    input_dim=input_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    device=device\n",
    ")\n",
    "enum_parameters(rl_policy)\n",
    "rl_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(-1.6959, device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXIMUM_GRAINS = 4\n",
    "max_nmoves_per_episode = 1000\n",
    "\n",
    "rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "agents = [rl_policy_agent]\n",
    "\n",
    "# start new sandpile with initial grid\n",
    "sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=10)\n",
    "rl_policy.select_action(sandpile, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xJFUThsu9tzX",
    "outputId": "c4a12851-65aa-495b-95ca-c2d18abfe757",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial grid\n",
      "[[3. 1. 3. 3. 3. 2. 2. 0. 2. 2.]\n",
      " [3. 3. 2. 3. 1. 3. 2. 2. 2. 2.]\n",
      " [2. 2. 1. 2. 0. 3. 1. 0. 2. 1.]\n",
      " [2. 2. 3. 3. 2. 2. 3. 3. 0. 2.]\n",
      " [3. 3. 2. 2. 3. 1. 1. 0. 2. 2.]\n",
      " [3. 1. 3. 3. 1. 2. 3. 3. 2. 3.]\n",
      " [2. 2. 2. 3. 3. 3. 2. 2. 2. 0.]\n",
      " [3. 3. 1. 2. 2. 3. 3. 3. 3. 3.]\n",
      " [2. 2. 1. 3. 2. 3. 3. 1. 2. 1.]\n",
      " [1. 1. 2. 1. 2. 1. 2. 3. 3. 0.]]\n",
      "preset_sandgrain_locs [[3 2]\n",
      " [9 4]\n",
      " [4 2]\n",
      " ...\n",
      " [5 9]\n",
      " [3 6]\n",
      " [6 1]]\n",
      "\n",
      "Training...\n",
      "\n",
      "i_episode:  1\n",
      "episode_rewards:  [0.25, -1.5, -3.25, -6.0, -6.75, -12.5, -11.25, -13.0, -15.75, -16.5, -17.25, -18.0, -19.75, -21.5, -23.25, -25.0, -27.75, -28.5, -32.25, -36.0, -35.75, -37.5, -39.25, -42.0, -43.75, -42.5, -44.25, -47.0, -49.75, -50.5, -52.25, -55.0, -54.75, -59.5]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -989.25\n",
      "n_steps_episode 34\n",
      "log_probs:  [tensor(-1.6996, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6997, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6997, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5628, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5879, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5629, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6368, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5643, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6365, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5683, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5634, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5686, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5881, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6964, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5629, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6961, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6367, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5625, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6367, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5900, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6380, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6928, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5908, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5675, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6943, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5647, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6946, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5901, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5663, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6389, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5884, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5896, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6374, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6372, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-989.247796152658, -989.4978951024475, -987.9979939022469, -984.7480923770561, -978.748190251875, -971.9982874517037, -959.498383401542, -948.2484782263898, -935.2485717512469, -919.4986637011133, -902.9987540009886, -885.7488425758728, -867.7489293507657, -847.9990141506671, -826.4990968005767, -803.2491771254944, -778.2492549504199, -750.4993300003529, -721.999402200293, -689.7494711752402, -653.7495365501937, -617.9995983501535, -580.4996564001191, -541.2497105250901, -499.2497604500661, -455.4998060000467, -412.99984730003143, -368.7498841750198, -321.74991635001146, -271.9999435500058, -221.49996570000235, -169.2499826250006, -114.24999405, -59.5], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0728, -1.0736, -1.0685, -1.0573, -1.0367, -1.0135, -0.9705, -0.9318,\n",
      "        -0.8871, -0.8329, -0.7762, -0.7169, -0.6550, -0.5871, -0.5131, -0.4332,\n",
      "        -0.3472, -0.2518, -0.1538, -0.0429,  0.0809,  0.2039,  0.3328,  0.4678,\n",
      "         0.6122,  0.7627,  0.9088,  1.0610,  1.2226,  1.3937,  1.5674,  1.7470,\n",
      "         1.9362,  2.1245], dtype=torch.float64)\n",
      "policy_loss:  -0.030010201972405337\n",
      "=========================== rl_policy._test_counter_i :  1 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  2\n",
      "episode_rewards:  [-1.75, -1.5, -3.25, -5.0, -6.75, -7.5, -10.25, -12.0, -12.75, -14.5, -17.25, -19.0, -20.75, -22.5, -23.25, -25.0, -26.75, -31.5, -30.25, -37.0, -35.75, -37.5, -37.25, -39.0, -43.75]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -521.75\n",
      "n_steps_episode 25\n",
      "log_probs:  [tensor(-1.5796, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6257, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6254, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5711, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5682, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6257, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5789, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6246, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6245, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7132, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5718, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5690, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5711, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7112, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5785, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7111, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5710, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5786, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6262, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7095, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7069, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5808, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6236, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7067, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5808, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-521.7491431007479, -519.9991951006674, -518.4992469505921, -515.249298475522, -510.24934950045696, -503.4993998503969, -495.99944945034184, -485.74949802529164, -473.7495454002462, -460.9995915002053, -446.4996361501689, -429.2496790751368, -410.2497201001088, -389.49975905008466, -366.9997957500642, -343.7498301250472, -318.7498620000334, -291.9998912000225, -260.4999172500142, -230.24994027500824, -193.2499596000042, -157.4999753500017, -119.99998735000044, -82.749995625, -43.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0411, -1.0296, -1.0197, -0.9983, -0.9654, -0.9209, -0.8715, -0.8041,\n",
      "        -0.7251, -0.6411, -0.5456, -0.4321, -0.3070, -0.1703, -0.0222,  0.1309,\n",
      "         0.2955,  0.4716,  0.6790,  0.8782,  1.1218,  1.3572,  1.6041,  1.8494,\n",
      "         2.1061], dtype=torch.float64)\n",
      "policy_loss:  0.3306082606843419\n",
      "=========================== rl_policy._test_counter_i :  2 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  3\n",
      "episode_rewards:  [1.25, -4.5, -2.25, -7.0, -5.75, -7.5, -10.25, -11.0, -15.75, -14.5, -19.25, -18.0, -19.75, -21.5, -24.25, -26.0, -27.75, -29.5, -34.25, -32.0, -34.75, -36.5, -40.25, -42.0]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -483.0\n",
      "n_steps_episode 24\n",
      "log_probs:  [tensor(-1.5776, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5779, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6994, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5766, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5710, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6976, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6272, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5804, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5765, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5710, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5805, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6273, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6271, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6269, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5704, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6990, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6991, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5703, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5770, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5799, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5703, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5783, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5775, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5700, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-482.99923950063544, -484.2492879255642, -479.7493359004978, -477.49938365043613, -470.49943070037915, -464.74947717532683, -457.2495229002791, -446.9995676002358, -435.9996112001969, -420.2496532251622, -405.7496938001316, -386.49973245010483, -368.49976930008177, -348.74980417506214, -327.2498369000458, -302.9998672000325, -276.99989490002196, -249.24991982501393, -219.7499418000081, -185.4999603500041, -153.49997570000167, -118.74998757500042, -82.2499958, -42.0], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0422, -1.0512, -1.0190, -1.0030, -0.9530, -0.9120, -0.8584, -0.7852,\n",
      "        -0.7067, -0.5943, -0.4908, -0.3534, -0.2249, -0.0839,  0.0696,  0.2427,\n",
      "         0.4283,  0.6264,  0.8370,  1.0815,  1.3099,  1.5580,  1.8186,  2.1059],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.24701001481981022\n",
      "=========================== rl_policy._test_counter_i :  3 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  4\n",
      "episode_rewards:  [1.25, -1.5, -3.25, -5.0, -9.75, -8.5, -9.25, -12.0, -14.75, -14.5, -16.25, -18.0, -21.75, -21.5, -23.25, -25.0, -27.75, -29.5, -31.25, -33.0, -33.75, -35.5, -38.25, -39.0, -43.75]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -514.75\n",
      "n_steps_episode 25\n",
      "log_probs:  [tensor(-1.5760, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5593, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7007, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7005, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5598, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7056, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6294, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6292, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5582, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5587, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6289, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5590, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5743, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6290, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5840, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5603, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6271, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5836, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5600, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7041, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6279, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7040, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5781, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6281, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6285, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-514.749156200735, -515.9992078006558, -514.4992592505816, -511.2493103755127, -506.2493610004488, -496.49941065038985, -487.9994594503358, -478.7495073252865, -466.7495540002419, -451.9995992002018, -437.4996429501661, -421.2496850751346, -403.24972540010714, -381.49976355008346, -359.9997995500634, -336.7498332250467, -311.7498644000331, -283.9998928000224, -254.49991825001422, -223.24994057500825, -190.2499596000042, -156.4999752500017, -120.99998735000044, -82.749995625, -43.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0442, -1.0525, -1.0425, -1.0208, -0.9875, -0.9225, -0.8659, -0.8042,\n",
      "        -0.7242, -0.6259, -0.5292, -0.4209, -0.3009, -0.1559, -0.0126,  0.1424,\n",
      "         0.3090,  0.4940,  0.6907,  0.8990,  1.1190,  1.3439,  1.5806,  1.8356,\n",
      "         2.0955], dtype=torch.float64)\n",
      "policy_loss:  0.12338021939167909\n",
      "=========================== rl_policy._test_counter_i :  4 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  5\n",
      "episode_rewards:  [1.25, -3.5, -5.25, -4.0, -9.75, -9.5, -10.25, -12.0, -13.75, -16.5, -18.25, -18.0, -21.75, -21.5, -23.25, -26.0, -26.75, -29.5, -31.25, -32.0, -33.75, -36.5, -37.25, -41.0, -47.75, -42.5, -45.25, -46.0, -48.75, -50.5, -51.25, -53.0, -55.75, -57.5, -59.25, -60.0, -63.75, -63.5, -68.25]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1293.0\n",
      "n_steps_episode 39\n",
      "log_probs:  [tensor(-1.5905, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5771, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7030, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6272, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5557, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5771, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5768, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5764, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6274, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6285, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7049, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5919, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5556, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6278, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6276, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5893, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5554, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5891, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5774, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5767, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6271, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5893, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5552, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5768, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5552, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7035, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5552, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6273, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6270, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7030, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5560, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6272, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6271, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7038, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7037, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5558, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5559, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5913, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6248, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-1292.9966872046205, -1294.246816629302, -1290.7469457039965, -1285.4970742537039, -1281.4972024034241, -1271.747329578157, -1262.2474558029026, -1251.9975810026606, -1239.997705002431, -1226.2478276272138, -1209.7479486020086, -1191.4980677518154, -1173.498185101634, -1151.7483002764639, -1130.2484133013052, -1106.9985240011574, -1080.9986321010206, -1054.2487375258943, -1024.7488400007783, -993.498939350672, -961.4990355005755, -927.7491282754883, -891.24921740041, -853.9993028003402, -812.9993841002786, -765.2494606252246, -722.7495329001779, -677.499600650138, -631.4996638001043, -582.7497220750764, -532.2497753000539, -480.9998234000363, -427.9998662000229, -372.2499034250132, -314.7499349000067, -255.49996045000267, -195.49998000000068, -131.749993175, -68.25], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0797, -1.0830, -1.0738, -1.0600, -1.0494, -1.0238, -0.9988, -0.9718,\n",
      "        -0.9403, -0.9041, -0.8607, -0.8127, -0.7653, -0.7081, -0.6515, -0.5904,\n",
      "        -0.5220, -0.4516, -0.3740, -0.2918, -0.2076, -0.1188, -0.0228,  0.0752,\n",
      "         0.1831,  0.3087,  0.4205,  0.5395,  0.6606,  0.7888,  0.9217,  1.0565,\n",
      "         1.1959,  1.3426,  1.4939,  1.6497,  1.8076,  1.9753,  2.1423],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  0.23420942919968546\n",
      "=========================== rl_policy._test_counter_i :  5 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  6\n",
      "episode_rewards:  [1.25, -0.5, -2.25, -5.0, -5.75, -9.5, -9.25, -11.0, -14.75, -16.5, -18.25, -19.0, -20.75, -25.5, -24.25, -27.0, -27.75, -30.5, -30.25, -33.0, -34.75, -36.5, -39.25, -39.0, -44.75, -43.5, -44.25, -46.0, -49.75, -51.5, -51.25, -54.0, -54.75, -56.5, -62.25, -60.0, -61.75, -63.5, -67.25, -67.0, -70.75, -73.5, -73.25, -77.0, -77.75, -78.5, -80.25, -82.0, -83.75, -86.5, -88.25, -88.0, -90.75, -91.5, -94.25, -95.0, -96.75, -98.5, -103.25]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -2986.5\n",
      "n_steps_episode 59\n",
      "log_probs:  [tensor(-1.7028, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7009, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7009, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5537, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5969, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5961, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5848, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5525, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6219, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7023, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7024, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5821, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5960, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5865, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5531, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5948, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6221, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6222, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5835, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7005, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5525, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5851, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6230, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5945, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5948, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5528, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5851, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6974, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5530, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5531, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5960, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6966, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5532, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5845, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6975, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6229, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6227, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6227, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5842, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5510, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5814, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6244, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6231, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6235, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5813, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5812, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6237, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5505, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6223, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5943, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6211, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5854, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6223, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5931, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5939, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6218, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6231, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-2986.4883419249886, -2987.7386406988526, -2987.238939422746, -2984.98923792167, -2979.9895359206234, -2974.2398333446067, -2964.7401298186196, -2955.490425367662, -2944.490719816734, -2929.741012790835, -2913.2413041149653, -2894.9915936141247, -2875.991881213313, -2855.2421667375293, -2829.742449711774, -2805.4927302610467, -2778.4930081103475, -2750.743283184676, -2720.2435552090315, -2689.9938242084136, -2656.9940899078224, -2622.2443521322575, -2585.7446107067185, -2546.494865356205, -2507.4951161057165, -2462.7453623802526, -2419.245604304813, -2374.995841804397, -2328.9960747040045, -2279.2463026286346, -2227.746525403287, -2176.496743052961, -2122.4969553026563, -2067.747162077372, -2011.247363202108, -1948.997558101864, -1888.9977470016386, -1827.2479297264315, -1763.748106101242, -1696.4982757510695, -1629.4984387009133, -1558.7485945757726, -1485.2487431006468, -1411.998884300535, -1334.9990178004368, -1257.2491435253512, -1178.7492614002772, -1098.4993712502142, -1016.4994729001615, -932.7495661751182, -846.2496508000831, -757.9997266000557, -669.9997936000351, -579.2498515250202, -487.7499003000102, -393.49993965000414, -298.49996950000104, -201.749989675, -103.25], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0836, -1.0850, -1.0845, -1.0819, -1.0763, -1.0698, -1.0591, -1.0486,\n",
      "        -1.0362, -1.0196, -1.0009, -0.9803, -0.9589, -0.9355, -0.9067, -0.8793,\n",
      "        -0.8488, -0.8175, -0.7831, -0.7489, -0.7117, -0.6724, -0.6312, -0.5869,\n",
      "        -0.5429, -0.4924, -0.4433, -0.3933, -0.3414, -0.2852, -0.2271, -0.1693,\n",
      "        -0.1083, -0.0465,  0.0173,  0.0876,  0.1553,  0.2250,  0.2967,  0.3726,\n",
      "         0.4482,  0.5281,  0.6111,  0.6938,  0.7807,  0.8685,  0.9571,  1.0477,\n",
      "         1.1402,  1.2348,  1.3324,  1.4321,  1.5314,  1.6338,  1.7371,  1.8435,\n",
      "         1.9508,  2.0600,  2.1712], dtype=torch.float64)\n",
      "policy_loss:  -0.22308488559813755\n",
      "=========================== rl_policy._test_counter_i :  6 ===================\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  7\n",
      "episode_rewards:  [0.25, -1.5, -2.25, -5.0, -5.75, -8.5, -9.25, -16.0, -13.75, -14.5, -16.25, -18.0, -19.75, -24.5, -27.25, -26.0, -27.75, -29.5, -30.25, -32.0, -35.75, -38.5]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -401.75\n",
      "n_steps_episode 22\n",
      "log_probs:  [tensor(-1.6159, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6148, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5974, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6141, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5484, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5967, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5480, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5483, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5509, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5846, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7038, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5984, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6153, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6151, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5858, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6108, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5919, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5912, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5513, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5912, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5521, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-401.74941965044013, -401.9994598503861, -400.49949990033605, -398.24953972529, -393.24957905024786, -387.4996178002096, -378.99965570017514, -369.7496926751444, -353.7497280501172, -339.9997620500934, -325.49979460007285, -309.24982552505537, -291.24985465004085, -271.499881800029, -246.99990650001968, -219.74992847501252, -193.7499478500073, -165.99996445000372, -136.49997810000153, -106.24998872500039, -74.24999615, -38.5], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0201, -1.0222, -1.0095, -0.9903, -0.9477, -0.8987, -0.8262, -0.7474,\n",
      "        -0.6110, -0.4938, -0.3703, -0.2318, -0.0784,  0.0900,  0.2988,  0.5310,\n",
      "         0.7526,  0.9891,  1.2405,  1.4984,  1.7711,  2.0758],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.05765065406651004\n",
      "=========================== rl_policy._test_counter_i :  7 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  8\n",
      "episode_rewards:  [1.25, -0.5, -5.25, -4.0, -9.75, -8.5, -10.25, -12.0, -12.75, -14.5, -17.25, -21.0, -19.75, -24.5]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -158.75\n",
      "n_steps_episode 14\n",
      "log_probs:  [tensor(-1.5907, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5508, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6055, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5916, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5916, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7046, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6036, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7056, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5860, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5857, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5855, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6035, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6035, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5853, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-158.74985585006598, -159.99987185005315, -159.49988780004193, -154.24990322503226, -150.24991825002408, -140.4999323000173, -131.99994550001185, -121.74995767500761, -109.74996865000446, -96.9999783500023, -82.49998660000095, -65.24999312500026, -44.24999755, -24.5], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.9797, -1.0072, -0.9962, -0.8805, -0.7923, -0.5774, -0.3901, -0.1641,\n",
      "         0.1004,  0.3814,  0.7010,  1.0812,  1.5441,  1.9794],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.048765891998930666\n",
      "=========================== rl_policy._test_counter_i :  8 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  9\n",
      "episode_rewards:  [-3.75, -1.5, -3.25, -6.0, -6.75, -8.5, -9.25, -13.0, -13.75, -15.5, -17.25, -19.0, -21.75, -22.5, -25.25, -26.0, -27.75, -30.5, -32.25, -33.0, -34.75, -36.5, -38.25, -40.0, -41.75, -43.5, -44.25, -50.0, -48.75, -49.5, -53.25, -53.0, -54.75, -56.5, -59.25, -60.0, -63.75, -63.5, -69.25, -68.0, -68.75, -70.5, -72.25, -76.0, -75.75, -77.5, -79.25, -82.0, -84.75, -86.5, -86.25, -88.0, -89.75, -91.5, -93.25, -98.0]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -2685.0\n",
      "n_steps_episode 56\n",
      "log_probs:  [tensor(-1.6042, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5466, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5467, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5850, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7109, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7105, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6068, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6026, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5888, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5892, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5889, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7133, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5890, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6039, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5893, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6038, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6012, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6010, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7105, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5501, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5501, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6049, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7104, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5505, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5870, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7084, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6013, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7086, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6069, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6038, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6036, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5871, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5478, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6073, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5477, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6033, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5872, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7094, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5868, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5498, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5862, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7086, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6063, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5857, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6037, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5501, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5856, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6046, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7101, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6044, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6044, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7102, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7104, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7100, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5844, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-2684.9900835201415, -2681.2403516441764, -2679.7406196182383, -2676.490887267327, -2670.491154316442, -2663.741420690584, -2655.2416862147525, -2645.9919508139474, -2632.9922141131688, -2619.242476037416, -2603.7427364116897, -2586.492995060989, -2567.493251810314, -2545.7435063846647, -2523.2437587090403, -2497.994008508441, -2471.994255707866, -2444.2445001323163, -2413.7447415067904, -2381.4949796562883, -2348.4952145058096, -2313.745445880354, -2277.245673604921, -2238.995897504511, -2198.9961174041227, -2157.246333128756, -2113.7465445034104, -2069.4967514530854, -2019.496953402781, -1970.7471504774958, -1921.24734260223, -1867.9975294019828, -1814.9977109017539, -1760.2478869265424, -1703.7480573013481, -1644.4982217511701, -1584.498380201008, -1520.7485322758612, -1457.248678000729, -1387.9988168006105, -1319.9989488005053, -1251.2490739254126, -1180.7491920003317, -1108.499302850262, -1032.4994061002026, -956.7495017751527, -879.2495897001116, -799.9996697000786, -717.9997415000527, -633.2498048250332, -546.749859500019, -460.49990555000966, -372.4999428000039, -282.749971075001, -191.2499902, -98.0], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0914, -1.0867, -1.0848, -1.0807, -1.0731, -1.0646, -1.0539, -1.0422,\n",
      "        -1.0259, -1.0085, -0.9890, -0.9673, -0.9433, -0.9159, -0.8876, -0.8558,\n",
      "        -0.8230, -0.7881, -0.7496, -0.7090, -0.6674, -0.6236, -0.5777, -0.5295,\n",
      "        -0.4791, -0.4265, -0.3717, -0.3159, -0.2529, -0.1915, -0.1291, -0.0621,\n",
      "         0.0047,  0.0737,  0.1449,  0.2195,  0.2951,  0.3754,  0.4554,  0.5427,\n",
      "         0.6284,  0.7150,  0.8038,  0.8948,  0.9906,  1.0860,  1.1837,  1.2835,\n",
      "         1.3868,  1.4936,  1.6026,  1.7113,  1.8221,  1.9352,  2.0505,  2.1680],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  0.4718847850548613\n",
      "=========================== rl_policy._test_counter_i :  9 ===================\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  10\n",
      "episode_rewards:  [1.25, -0.5, -2.25, -4.0, -5.75, -12.5, -10.25, -12.0, -14.75, -14.5, -18.25, -22.0]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -115.5\n",
      "n_steps_episode 12\n",
      "log_probs:  [tensor(-1.6048, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6020, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7130, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6019, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5484, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5875, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6015, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6014, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6055, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5504, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6008, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6047, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-115.49990790003551, -116.74991957502746, -116.24993120002057, -113.99994260001482, -109.99995360001017, -104.24996402500658, -91.7499732000039, -81.49998135000203, -69.49998830000085, -54.74999377500023, -40.2499978, -22.0], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.8850, -0.9230, -0.9078, -0.8394, -0.7179, -0.5432, -0.1633,  0.1481,\n",
      "         0.5128,  0.9610,  1.4016,  1.9561], dtype=torch.float64)\n",
      "policy_loss:  -0.1008921236501461\n",
      "=========================== rl_policy._test_counter_i :  10 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  11\n",
      "episode_rewards:  [0.25, -1.5, -2.25, -5.0, -6.75, -7.5, -10.25, -12.0, -13.75, -15.5, -19.25, -18.0, -21.75, -23.5, -23.25, -25.0, -28.75, -28.5, -30.25, -32.0, -33.75, -37.5, -37.25, -43.0, -41.75, -43.5, -45.25, -47.0, -47.75, -49.5, -52.25, -53.0, -57.75, -59.5, -60.25, -60.0, -61.75, -65.5, -68.25, -67.0, -68.75, -70.5, -73.25, -74.0, -75.75, -77.5, -79.25, -85.0, -83.75, -84.5, -86.25, -88.0, -92.75, -92.5, -96.25, -96.0, -99.75, -102.5, -103.25, -102.0, -106.75, -106.5, -108.25, -110.0, -111.75, -113.5, -115.25, -116.0, -117.75, -119.5, -121.25, -127.0, -124.75, -126.5, -128.25, -131.0, -131.75, -133.5, -135.25, -141.0, -139.75, -142.5, -144.25, -144.0, -145.75, -148.5, -150.25, -151.0, -153.75, -154.5, -156.25, -159.0, -161.75, -162.5, -163.25, -166.0, -169.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -8125.75\n",
      "n_steps_episode 97\n",
      "log_probs:  [tensor(-1.6039, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6000, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6027, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6027, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5497, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7130, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5494, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5892, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5498, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6026, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6024, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5911, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5499, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5906, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5503, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7110, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5506, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6019, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5901, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7096, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7112, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6017, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6022, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7107, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7096, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5891, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6027, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5892, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5488, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6036, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6040, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6036, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7100, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6029, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5983, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5503, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5939, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5492, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7134, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5495, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5976, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5502, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7119, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5909, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6030, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5506, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5945, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5931, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7087, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6030, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5919, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5990, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5926, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7081, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5982, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6012, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5528, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5975, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7072, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5917, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5970, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5917, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5971, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6047, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7066, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7063, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6055, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5531, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5987, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7067, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7081, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6041, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5916, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6040, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7091, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5983, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6028, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6013, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7103, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5507, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7101, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5510, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6013, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5531, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5529, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7081, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5925, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5542, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5987, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5509, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5972, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-8125.697684086743, -8125.948496681592, -8124.449309126522, -8122.200121346534, -8117.200933066627, -8110.4517441118005, -8102.952554407056, -8092.703363677391, -8080.704171747808, -8066.954978443305, -8051.455783588883, -8032.206586809541, -8014.207388230279, -7992.458187476098, -7968.958984371996, -7945.709778942974, -7920.71057101403, -7891.961360210166, -7863.4621465563805, -7833.212929877673, -7801.213709999043, -7767.464486745492, -7729.965259742017, -7692.71602901362, -7649.716793985299, -7607.967554782054, -7564.468311228885, -7519.219063150791, -7472.2198103727715, -7424.4705528198265, -7374.971290316955, -7322.722022589157, -7269.722749561432, -7211.973470758779, -7152.474186006197, -7092.224895228686, -7032.225598451246, -6970.476295498875, -6904.976985996573, -6836.727669669339, -6769.728346642173, -6700.979016740074, -6630.479679788042, -6557.230335511075, -6483.2309838341735, -6407.481624582336, -6329.982257580561, -6250.732882653849, -6165.733499227198, -6081.984107425608, -5997.484707174079, -5911.235298297608, -5823.235880621196, -5730.4864536698415, -5637.987017468543, -5541.737571642299, -5445.738116216111, -5345.988650814976, -5243.489175163893, -5140.239689187862, -5038.24019301188, -4931.490686160949, -4824.991168660065, -4716.7416403342295, -4606.742101008439, -4494.9925505076935, -4381.492988656993, -4266.243415281334, -4150.243830305717, -4032.4942335551405, -3912.9946248546025, -3791.745004029103, -3664.7453705036396, -3539.995724503212, -3413.4960658528184, -3285.246394377458, -3154.2467098021284, -3022.4970120518296, -2888.9973009515597, -2753.7475763263174, -2612.747837601101, -2472.9980849009094, -2330.4983179507412, -2186.2485365755947, -2042.2487408004688, -1896.4989304503617, -1747.9991052502721, -1597.7492650251986, -1446.7494097001395, -1292.9995390000934, -1138.4996528500585, -982.2497510750336, -823.2498334000169, -661.4998995500068, -498.99994945000174, -335.749983025, -169.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0970, -1.0971, -1.0965, -1.0956, -1.0935, -1.0907, -1.0876, -1.0833,\n",
      "        -1.0784, -1.0727, -1.0663, -1.0583, -1.0508, -1.0418, -1.0321, -1.0225,\n",
      "        -1.0121, -1.0002, -0.9884, -0.9759, -0.9626, -0.9486, -0.9331, -0.9177,\n",
      "        -0.8999, -0.8826, -0.8646, -0.8458, -0.8263, -0.8066, -0.7861, -0.7644,\n",
      "        -0.7425, -0.7186, -0.6939, -0.6690, -0.6441, -0.6185, -0.5914, -0.5631,\n",
      "        -0.5354, -0.5069, -0.4777, -0.4474, -0.4167, -0.3853, -0.3532, -0.3204,\n",
      "        -0.2852, -0.2505, -0.2155, -0.1798, -0.1433, -0.1049, -0.0666, -0.0267,\n",
      "         0.0130,  0.0543,  0.0968,  0.1396,  0.1818,  0.2260,  0.2701,  0.3150,\n",
      "         0.3605,  0.4068,  0.4538,  0.5016,  0.5496,  0.5984,  0.6479,  0.6981,\n",
      "         0.7507,  0.8024,  0.8548,  0.9079,  0.9621,  1.0167,  1.0720,  1.1280,\n",
      "         1.1864,  1.2443,  1.3033,  1.3631,  1.4227,  1.4831,  1.5446,  1.6068,\n",
      "         1.6694,  1.7331,  1.7971,  1.8618,  1.9276,  1.9946,  2.0619,  2.1296,\n",
      "         2.1983], dtype=torch.float64)\n",
      "policy_loss:  0.04882031876167314\n",
      "=========================== rl_policy._test_counter_i :  11 ===================\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  12\n",
      "episode_rewards:  [-1.75, -0.5, -2.25, -4.0, -6.75, -7.5, -10.25, -12.0, -13.75, -15.5, -20.25, -21.0, -20.75, -22.5, -24.25, -26.0, -27.75, -29.5, -34.25, -33.0, -34.75, -36.5, -40.25]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -445.0\n",
      "n_steps_episode 23\n",
      "log_probs:  [tensor(-1.7115, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5523, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7113, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5931, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6040, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5513, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6044, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7121, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5514, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5979, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5903, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7134, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5897, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5980, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7131, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6032, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5920, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7118, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5519, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5527, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-444.9993283005343, -443.2493726254715, -442.74941690041317, -440.49946095035926, -436.4995046003097, -429.74954757526444, -422.2495898002234, -411.99963100018647, -399.99967100015357, -386.2497096251245, -370.7497467000992, -350.4997817500774, -329.49981470005883, -308.74984557504337, -286.2498742000308, -261.9999004000208, -235.99992400001315, -208.24994482500762, -178.7499627000039, -144.4999771500016, -111.49998830000041, -76.749995975, -40.25], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0255, -1.0121, -1.0082, -0.9909, -0.9602, -0.9084, -0.8509, -0.7722,\n",
      "        -0.6801, -0.5746, -0.4556, -0.3002, -0.1390,  0.0203,  0.1930,  0.3791,\n",
      "         0.5787,  0.7916,  1.0181,  1.2809,  1.5342,  1.8009,  2.0811],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.041818475534370236\n",
      "=========================== rl_policy._test_counter_i :  12 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  13\n",
      "episode_rewards:  [1.25, -0.5, -2.25, -4.0, -6.75, -12.5, -9.25, -11.0, -12.75, -14.5, -18.25, -18.0, -19.75, -21.5, -23.25, -27.0, -26.75, -28.5, -30.25, -34.0, -33.75, -35.5, -38.25, -45.0]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -472.0\n",
      "n_steps_episode 24\n",
      "log_probs:  [tensor(-1.5901, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5527, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5985, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7104, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5531, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6036, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5890, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5981, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5984, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5908, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5518, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6037, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5933, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5960, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5953, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5947, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5944, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7109, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5940, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5932, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5515, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5933, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5945, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-471.99925010062833, -473.24929742555804, -472.7493447004925, -470.49939175043164, -466.4994384003755, -459.7494843753239, -447.2495291002768, -437.99957290023406, -426.99961560019557, -414.24965702516124, -399.74969700013094, -381.4997351501044, -363.4997715000815, -343.74980587506207, -322.24983810004585, -298.9998680000326, -271.99989520002214, -245.24991972501414, -216.74994140000825, -186.49996005000426, -152.49997530000178, -118.74998717500047, -83.24999550000001, -45.0], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0228, -1.0320, -1.0283, -1.0119, -0.9826, -0.9332, -0.8418, -0.7742,\n",
      "        -0.6938, -0.6005, -0.4945, -0.3611, -0.2294, -0.0850,  0.0722,  0.2422,\n",
      "         0.4397,  0.6353,  0.8437,  1.0649,  1.3135,  1.5603,  1.8199,  2.0996],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.056159511393273576\n",
      "=========================== rl_policy._test_counter_i :  13 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  14\n",
      "episode_rewards:  [0.25, -0.5, -3.25, -5.0, -5.75, -8.5, -10.25, -11.0, -13.75, -15.5, -17.25, -20.0, -19.75, -21.5, -24.25, -26.0, -29.75]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -231.75\n",
      "n_steps_episode 17\n",
      "log_probs:  [tensor(-1.5512, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5514, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5938, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6063, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5907, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5857, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7119, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6000, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5522, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5510, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7131, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5864, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5856, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7109, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5529, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6004, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5528, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-231.7497410001484, -231.9997642001248, -231.4997873501035, -228.24981017508452, -223.24983250006775, -217.49985425005318, -208.99987515004068, -198.74989502503018, -187.74991380002155, -173.99993120001466, -158.49994705000935, -141.24996117500547, -121.24997330000281, -101.49998345000115, -79.9999914500003, -55.749997025, -29.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.9860, -0.9897, -0.9822, -0.9335, -0.8586, -0.7723, -0.6449, -0.4912,\n",
      "        -0.3263, -0.1202,  0.1122,  0.3709,  0.6707,  0.9668,  1.2892,  1.6527,\n",
      "         2.0425], dtype=torch.float64)\n",
      "policy_loss:  0.01351476709331667\n",
      "=========================== rl_policy._test_counter_i :  14 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  15\n",
      "episode_rewards:  [1.25, -3.5, -2.25, -4.0, -9.75, -7.5, -9.25, -11.0, -14.75, -15.5, -17.25, -20.0, -22.75, -24.5, -24.25, -28.0]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -213.0\n",
      "n_steps_episode 16\n",
      "log_probs:  [tensor(-1.7121, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5516, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6013, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6001, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7131, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6087, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7072, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5525, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5943, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5522, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7075, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6095, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5937, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7085, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5946, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5940, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-212.99977700011937, -214.2497984250992, -210.74981950008112, -208.49984035006514, -204.4998608000512, -194.74988027503923, -187.24989900002913, -177.9999168000208, -166.99993350001415, -152.249948725009, -136.74996240000524, -119.49997435000266, -99.49998430000109, -76.74999197500028, -52.2499972, -28.0], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.9872, -1.0076, -0.9503, -0.9135, -0.8480, -0.6884, -0.5657, -0.4143,\n",
      "        -0.2343,  0.0072,  0.2609,  0.5432,  0.8705,  1.2429,  1.6439,  2.0408],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.06349205320476159\n",
      "=========================== rl_policy._test_counter_i :  15 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  16\n",
      "episode_rewards:  [-3.75, -1.5, -3.25, -5.0, -9.75, -8.5, -10.25, -11.0, -16.75, -14.5, -19.25, -18.0, -22.75, -24.5, -23.25, -26.0, -26.75, -32.5, -32.25, -34.0, -33.75, -35.5, -37.25, -41.0, -42.75, -44.5, -46.25, -50.0, -47.75, -50.5, -54.25, -54.0, -55.75, -59.5, -58.25, -61.0, -61.75, -64.5, -66.25, -68.0, -69.75, -70.5, -75.25, -74.0, -78.75, -77.5, -80.25, -81.0, -82.75, -84.5, -86.25, -88.0, -93.75]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -2418.25\n",
      "n_steps_episode 53\n",
      "log_probs:  [tensor(-1.6104, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7114, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7114, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7111, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5989, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6077, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7111, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5841, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5977, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7079, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5949, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5549, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6100, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7079, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5950, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5931, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5892, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6088, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5935, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7078, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5901, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5838, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5834, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5827, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6080, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5532, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7116, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5520, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7139, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6003, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6000, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6075, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5979, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6084, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5505, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5523, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6100, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5992, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5857, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5853, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6093, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6005, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5845, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6014, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5867, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5839, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6014, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7111, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6073, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7120, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7126, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7124, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-2418.2415707161567, -2414.4918121653377, -2412.992053464543, -2409.742294438772, -2404.742534913026, -2394.9927744123033, -2386.4930130616044, -2376.2432506859295, -2365.243487210278, -2348.4937220596503, -2333.9939554590455, -2314.7441869334643, -2296.744416607906, -2273.99464400737, -2249.494868956857, -2226.245091581366, -2200.2453116058973, -2173.49552895545, -2140.9957430550244, -2108.7459539296196, -2074.746161404236, -2040.9963655038725, -2005.496566053529, -1968.2467628782051, -1927.2469556029005, -1884.4971440526149, -1839.9973280523475, -1793.747507427098, -1743.747681801866, -1695.9978514016511, -1645.4980159514525, -1591.24817507627, -1537.2483288011028, -1481.4984769509504, -1421.9986191508121, -1363.7487555256876, -1302.748885800576, -1240.999009900477, -1176.4991275503896, -1110.2492385753135, -1042.2493428002476, -972.4994400501914, -901.9995302501444, -826.7496129251057, -752.7496882000745, -673.99975560005, -596.4998152500315, -516.2498668750181, -435.24991040000924, -352.4999456500038, -267.999972450001, -181.749990625, -93.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0944, -1.0892, -1.0871, -1.0825, -1.0755, -1.0619, -1.0500, -1.0356,\n",
      "        -1.0202, -0.9968, -0.9765, -0.9496, -0.9244, -0.8926, -0.8583, -0.8258,\n",
      "        -0.7894, -0.7520, -0.7065, -0.6614, -0.6138, -0.5666, -0.5169, -0.4648,\n",
      "        -0.4074, -0.3476, -0.2853, -0.2206, -0.1507, -0.0839, -0.0132,  0.0627,\n",
      "         0.1383,  0.2163,  0.2995,  0.3810,  0.4664,  0.5528,  0.6430,  0.7357,\n",
      "         0.8308,  0.9284,  1.0271,  1.1324,  1.2359,  1.3461,  1.4545,  1.5668,\n",
      "         1.6801,  1.7959,  1.9141,  2.0348,  2.1579], dtype=torch.float64)\n",
      "policy_loss:  0.09169316791277327\n",
      "=========================== rl_policy._test_counter_i :  16 ===================\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  17\n",
      "episode_rewards:  [1.25, -0.5, -6.25, -4.0, -5.75, -7.5, -9.25, -12.0, -15.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -59.75\n",
      "n_steps_episode 9\n",
      "log_probs:  [tensor(-1.5848, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5532, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5829, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5839, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5530, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5825, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7115, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6095, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5754, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-59.749964900009616, -60.99997100000671, -60.49997705000441, -54.24998247500265, -50.2499875000014, -44.4999919500006, -36.99999565000016, -27.749998425, -15.75], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.8835, -0.9618, -0.9305, -0.5392, -0.2887,  0.0713,  0.5409,  1.1201,\n",
      "         1.8714], dtype=torch.float64)\n",
      "policy_loss:  0.12010329253127328\n",
      "=========================== rl_policy._test_counter_i :  17 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  18\n",
      "episode_rewards:  [0.25, -1.5, -7.25, -4.0, -5.75, -7.5, -11.25, -13.0, -14.75, -15.5, -17.25, -18.0, -20.75, -21.5, -25.25, -25.0, -28.75, -28.5, -35.25, -35.0, -36.75, -35.5, -40.25, -39.0, -41.75, -42.5, -45.25, -46.0, -47.75, -49.5, -51.25, -54.0, -56.75, -58.5, -61.25]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -1041.5\n",
      "n_steps_episode 35\n",
      "log_probs:  [tensor(-1.6029, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5533, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5776, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6148, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7071, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7069, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5829, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5819, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7056, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5789, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5533, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6024, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5773, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6029, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6021, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6005, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5806, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6001, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7131, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5536, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7100, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5830, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5534, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5821, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5805, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6152, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5948, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5541, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5548, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5547, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6159, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5917, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5548, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7070, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5548, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-1041.497606502978, -1041.747710677749, -1040.2478147025304, -1032.9979180023222, -1028.9980209021242, -1023.2481232269364, -1015.7482248017589, -1004.4983252515913, -991.4984244014337, -976.7485220762858, -961.2486182011476, -943.9987126010187, -925.9988052008991, -905.2488957257887, -883.748984100687, -858.4990699505939, -833.4991533005092, -804.7492337754326, -776.2493114003637, -740.9993855003022, -705.9994561002478, -669.2495230252001, -633.7495864001587, -593.4996457501232, -554.4997012000933, -512.7497524750686, -470.24979950004854, -424.99984200003274, -378.9998799000207, -331.24991302501195, -281.74994120000605, -230.49996425000245, -176.49998190000062, -119.749993875, -61.25], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-1.0703, -1.0711, -1.0662, -1.0425, -1.0295, -1.0107, -0.9862, -0.9495,\n",
      "        -0.9071, -0.8589, -0.8083, -0.7520, -0.6933, -0.6255, -0.5554, -0.4729,\n",
      "        -0.3913, -0.2975, -0.2045, -0.0894,  0.0249,  0.1448,  0.2607,  0.3921,\n",
      "         0.5194,  0.6557,  0.7944,  0.9421,  1.0923,  1.2481,  1.4097,  1.5770,\n",
      "         1.7533,  1.9385,  2.1295], dtype=torch.float64)\n",
      "policy_loss:  -0.30063062730776524\n",
      "=========================== rl_policy._test_counter_i :  18 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  19\n",
      "episode_rewards:  [1.25, -0.5, -2.25, -5.0, -5.75, -8.5, -9.25, -11.0, -13.75, -15.5, -17.25, -19.0, -20.75, -24.5]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -151.75\n",
      "n_steps_episode 14\n",
      "log_probs:  [tensor(-1.5925, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7070, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5925, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5923, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5885, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5909, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5884, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7088, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5943, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5934, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5927, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5866, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5929, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.5922, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "returns:  deque([-151.74985835006564, -152.999873650053, -152.49988890004187, -150.24990392503224, -145.24991845002407, -139.49993240001731, -130.99994550001185, -121.7499576750076, -110.74996875000446, -96.9999784500023, -81.49998660000095, -64.24999302500025, -45.24999755, -24.5], maxlen=1000)\n",
      "Post standard\n",
      "returns:  tensor([-0.9203, -0.9493, -0.9377, -0.8856, -0.7697, -0.6366, -0.4397, -0.2254,\n",
      "         0.0294,  0.3479,  0.7069,  1.1064,  1.5465,  2.0271],\n",
      "       dtype=torch.float64)\n",
      "policy_loss:  -0.13487785264695562\n",
      "=========================== rl_policy._test_counter_i :  19 ===================\n",
      "  Training episode took: 0:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 212\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training episode took: \u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(training_time))\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Calculate the average loss over all of the batches.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# avg_train_score = np.mean(cumulative_scores_episode)     \u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# training_scores.append(avg_train_score)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# print(\"\")\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# print(\"  Average training cumulative score: {0:.4f}\".format(avg_train_score))\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Measure how long this episode took.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m training_time \u001b[38;5;241m=\u001b[39m format_time(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1261\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "N_training_episodes = 100000\n",
    "N_val_episodes = 1000\n",
    "gamma = 0.9999999\n",
    "\n",
    "optimizer = torch.optim.Adam(rl_policy.parameters(), lr=1e-3, betas=(0.9, 0.998), eps=1e-9, weight_decay=1e-4)\n",
    "\n",
    "start_epoch = 0\n",
    "FROM_CHECKPOINT = not True\n",
    "if FROM_CHECKPOINT:\n",
    "    \n",
    "    checkpoint = torch.load(output_dir+'best_gait_model.tar')\n",
    "    g = checkpoint['model_state_dict']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Lowest Loss: {loss}')\n",
    "    save_best_model = SaveBestModel(output_dir+best_model_name, loss)\n",
    "    # print(g.keys())\n",
    "    model.load_state_dict(g)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'From epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "\n",
    "training_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# For now, generate the initial grid once \n",
    "# generate initial grid\n",
    "initial_grid_N = N_grid * N_grid * 4\n",
    "print('Generating initial grid')\n",
    "initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "print(initial_grid)\n",
    "\n",
    "\n",
    "# generate preset sandgrain locations\n",
    "preset_sandgrain_locs = np.random.randint(0, N_grid, size=(max_nmoves_per_episode, 2))\n",
    "print('preset_sandgrain_locs', preset_sandgrain_locs)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print('Training...')\n",
    "# Measure how long the training epoch takes.\n",
    "t0 = time.time()\n",
    "rl_policy.train()\n",
    "for i_episode in range(1, N_training_episodes+1):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print()\n",
    "    t_episode_start = time.time()\n",
    "    print('i_episode: ', i_episode)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Progress update every 40 batches.\n",
    "    if i_episode % 100 == 0 and not i_episode == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Episode {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_episode, N_training_episodes, elapsed))\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "    \n",
    "\n",
    "    # # generate initial grid\n",
    "    # initial_grid_N = N_grid * N_grid * 4\n",
    "    # # print('Generating initial grid')\n",
    "    # initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print(initial_grid)\n",
    "\n",
    "\n",
    "    rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    agents = [rl_policy_agent]\n",
    "\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    \n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode, grain_loc_order=preset_sandgrain_locs)\n",
    "\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    # rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "    rl_policy_agent.move_agent_to_point(N_grid//2, N_grid//2)\n",
    "\n",
    "    pos = rl_policy_agent.get_agent_pos()\n",
    "    # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    agent_moves = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    # sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print('Step i: ', i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "        # print(sandpile_grid)\n",
    "        # sandpile.print_grid_and_agent_pos(rl_policy_agent)\n",
    "        # print(agent_rewards)\n",
    "        # print(game_is_running)\n",
    "        pos = rl_policy_agent.get_agent_pos()\n",
    "        # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        # print('action: ', action)\n",
    "        # print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "\n",
    "        # subtract expected value from just staying in the center\n",
    "        reward = reward - 1.75 * i\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        agent_moves.append(list(Directions)[action])\n",
    "\n",
    "        # input()\n",
    "\n",
    "    print('episode_rewards: ', episode_rewards)\n",
    "    print('agent_moves: ', agent_moves)\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    training_scores.append(cumulative_score_episode)\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # print('episode_rewards', episode_rewards)\n",
    "    print('cumulative_score_episode', cumulative_score_episode)\n",
    "\n",
    "    returns = deque(maxlen=max_nmoves_per_episode)\n",
    "    n_steps_episode = len(episode_rewards)\n",
    "\n",
    "    print('n_steps_episode', n_steps_episode)\n",
    "\n",
    "    #TODO: replace with reverse numpy cumsum?\n",
    "    for t in range(n_steps_episode)[::-1]:\n",
    "        discounted_return_t = returns[0] if len(returns) > 0 else 0\n",
    "        returns.appendleft(gamma * discounted_return_t + episode_rewards[t])\n",
    "\n",
    "    # print('Pre standard')\n",
    "    print('log_probs: ', log_probs)\n",
    "    print('returns: ', returns)\n",
    "\n",
    "\n",
    "    #standardize\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    else:\n",
    "        returns = (returns - returns.mean())\n",
    "\n",
    "    # compute loss\n",
    "    print('Post standard')\n",
    "    print('returns: ', returns)\n",
    "\n",
    "    # policy_loss = []\n",
    "    policy_loss = 0\n",
    "\n",
    "    for log_prob, disc_return in zip(log_probs, returns):\n",
    "        # print('log_prob ', log_prob)\n",
    "        # print('disc_return ', disc_return)\n",
    "        policy_loss += -log_prob * disc_return\n",
    "        \n",
    "        # policy_loss.append(-log_prob * disc_return)\n",
    "        # print(policy_loss)\n",
    "    print('policy_loss: ', policy_loss.item())\n",
    "    # policy_loss = torch.tensor(policy_loss).sum()\n",
    "\n",
    "    optimizer.zero_grad()   \n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # TEST COUNTER\n",
    "    rl_policy._test_counter()\n",
    "    print('=========================== rl_policy._test_counter_i : ', rl_policy._test_counter_i, '===================')\n",
    "\n",
    "    training_time = format_time(time.time() - t_episode_start)\n",
    "    print(\"  Training episode took: {:}\".format(training_time))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    # avg_train_score = np.mean(cumulative_scores_episode)     \n",
    "    # training_scores.append(avg_train_score)\n",
    "    # print(\"\")\n",
    "    # print(\"  Average training cumulative score: {0:.4f}\".format(avg_train_score))\n",
    "\n",
    "    # input()\n",
    "\n",
    "# Measure how long this episode took.\n",
    "training_time = format_time(time.time() - t0)\n",
    "print(\"  Training took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "rl_policy.eval()\n",
    "\n",
    "for episode_i in range(N_val_episodes):\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "\n",
    "\n",
    "    # generate initial grid\n",
    "    # run the sandpile 1000 times\n",
    "    # initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    # initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print('initial grid')\n",
    "    # print(initial_grid)\n",
    "\n",
    "    rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    agents = [rl_policy_agent]\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode, grain_loc_order=preset_sandgrain_locs)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    # rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    rl_policy_agent.move_agent_to_point(N_grid//2, N_grid//2)\n",
    "\n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print(i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        print('action: ', action)\n",
    "        print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    validation_scores.append(cumulative_score_episode)\n",
    "\n",
    "    #save best model\n",
    "    # save_best_model(\n",
    "    #     cumulative_score_episode, episode_i+1, rl_policy, optimizer\n",
    "    # )\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # avg_val_score = np.mean(cumulative_scores_episode)\n",
    "\n",
    "\n",
    "    # validation_scores.append(avg_val_score)\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "\n",
    "# print(\"  Validation Score: {0:.4f}\".format(avg_val_score))\n",
    "print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "\n",
    "#print training vals\n",
    "# print('Validation scores')\n",
    "# print(validation_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "#save model params\n",
    "# model_name = 'rl_policy_params.pt'\n",
    "# torch.save(rl_policy.state_dict(), output_dir+model_name)\n",
    "\n",
    "# model_name = 'rl_policy_full.pt'\n",
    "# torch.save(rl_policy, output_dir+model_name)\n",
    "\n",
    "#save model params\n",
    "model_name = 'rl_policy_params.pt'\n",
    "torch.save(rl_policy.state_dict(), model_name)\n",
    "\n",
    "model_name = 'rl_policy_full.pt'\n",
    "torch.save(rl_policy, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KFrle1f3fxJP",
    "outputId": "65ca3ae9-d33d-4184-bd58-99b6038128f3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(training_scores)\n\u001b[1;32m      2\u001b[0m validation_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(validation_scores)\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "training_scores = np.array(training_scores)\n",
    "validation_scores = np.array(validation_scores)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(training_scores,'-',label='Train')\n",
    "axs.set_ylabel('Scores')\n",
    "axs.plot(validation_scores,'-',label='Val')\n",
    "axs.set_xlabel('Episode')\n",
    "axs.legend()\n",
    "print(np.min(validation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn0gYmJ3dSt8",
    "outputId": "4d16be89-fe4d-456f-a13f-f1129cf94f86"
   },
   "outputs": [],
   "source": [
    "params = list(best_model.named_parameters())\n",
    "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "for p in params:\n",
    "    # print('p')\n",
    "    # print(p[0])\n",
    "    # print(p[1].data)\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7PxtSeLVlak"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../staging_area/gait-model ../full_models/\n",
    "!zip -r ../full_models/gait-model.zip ../full_models/gait-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7184MNpqVlal"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../full_models/gait-model/ gs://ml_gait_estimation/full_models/\n",
    "!gsutil cp ../full_models/gait-model.zip gs://ml_gait_estimation/full_models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
