{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoK10It3TUN2",
    "outputId": "115d4998-5725-4fe6-d9c4-7f5597f5eeff"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_built_with_cuda()\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from save_best_model import SaveBestModel\n",
    "from sandpile import Sandpile, run_sandpile_alone\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import datetime\n",
    "from rl_agents import Policy\n",
    "from agents import RLPolicyAgent\n",
    "from util import Directions\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "#RUN THIS ON COLAB\n",
    "ON_COLAB = False\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/Phase ML Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4YR0pTPWV3Dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /staging_area/reinforce-agent/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "model_nickname = 'reinforce-agent'\n",
    "\n",
    "output_dir = f'/staging_area/{model_nickname}/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# checkpoint_dir = 'checkpoints/'\n",
    "# if not os.path.exists(output_dir+checkpoint_dir):\n",
    "#     os.makedirs(output_dir+checkpoint_dir)\n",
    "\n",
    "\n",
    "best_model_name = 'best_agent.tar'\n",
    "save_best_model = SaveBestModel(output_dir+best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "Total Trainable Params: 10309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=25, out_features=64, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def enum_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(rl_policy)\n",
    "\n",
    "\n",
    "# SET UP POLICY AGENT\n",
    "N_grid = 10\n",
    "num_hidden_layers = 2\n",
    "hidden_dim = 64\n",
    "input_dim = N_grid**2# The number of input variables. \n",
    "output_dim = len(Directions) # The number of output variables. \n",
    "\n",
    "rl_policy = Policy(\n",
    "    input_dim=input_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    device=device\n",
    ")\n",
    "enum_parameters(rl_policy)\n",
    "rl_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(-1.5809, device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXIMUM_GRAINS = 4\n",
    "max_nmoves_per_episode = 10000\n",
    "\n",
    "rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "agents = [rl_policy_agent]\n",
    "\n",
    "# start new sandpile with initial grid\n",
    "sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=10)\n",
    "rl_policy.select_action(sandpile=sandpile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xJFUThsu9tzX",
    "outputId": "c4a12851-65aa-495b-95ca-c2d18abfe757",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "i_episode:  1\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  2\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  3\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  4\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  5\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  6\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -56.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  7\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -98.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  8\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -98.0000\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  9\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -96.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  10\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  11\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  12\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  13\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -95.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  14\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  15\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  16\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  17\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  18\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  19\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  20\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -91.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  21\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  22\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  23\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  24\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -96.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  25\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  26\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -72.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  27\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -83.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  28\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  29\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  30\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -96.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  31\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -89.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  32\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  33\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  34\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -97.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  35\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -87.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  36\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  37\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -98.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  38\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -76.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  39\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  40\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  41\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  42\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  43\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  44\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -98.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  45\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -97.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  46\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  47\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  48\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  49\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  50\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  51\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -81.0000\n",
      "  Training episode took: 0:00:02\n",
      "\n",
      "i_episode:  52\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 62.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  53\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  54\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  55\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -98.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  56\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  57\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:03\n",
      "\n",
      "i_episode:  58\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 146.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  59\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  60\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  61\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  62\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  63\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  64\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  65\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  66\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  67\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  68\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  69\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:04\n",
      "\n",
      "i_episode:  70\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 195.0000\n",
      "  Training episode took: 0:00:05\n",
      "\n",
      "i_episode:  71\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 51.0000\n",
      "  Training episode took: 0:00:05\n",
      "\n",
      "i_episode:  72\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:05\n",
      "\n",
      "i_episode:  73\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:05\n",
      "\n",
      "i_episode:  74\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -79.0000\n",
      "  Training episode took: 0:00:05\n",
      "\n",
      "i_episode:  75\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 47.0000\n",
      "  Training episode took: 0:00:06\n",
      "\n",
      "i_episode:  76\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:06\n",
      "\n",
      "i_episode:  77\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 106.0000\n",
      "  Training episode took: 0:00:06\n",
      "\n",
      "i_episode:  78\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 5.0000\n",
      "  Training episode took: 0:00:07\n",
      "\n",
      "i_episode:  79\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:07\n",
      "\n",
      "i_episode:  80\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 72.0000\n",
      "  Training episode took: 0:00:07\n",
      "\n",
      "i_episode:  81\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -19.0000\n",
      "  Training episode took: 0:00:08\n",
      "\n",
      "i_episode:  82\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 84.0000\n",
      "  Training episode took: 0:00:08\n",
      "\n",
      "i_episode:  83\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:08\n",
      "\n",
      "i_episode:  84\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -4.0000\n",
      "  Training episode took: 0:00:08\n",
      "\n",
      "i_episode:  85\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:08\n",
      "\n",
      "i_episode:  86\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 10.0000\n",
      "  Training episode took: 0:00:09\n",
      "\n",
      "i_episode:  87\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -24.0000\n",
      "  Training episode took: 0:00:09\n",
      "\n",
      "i_episode:  88\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 55.0000\n",
      "  Training episode took: 0:00:09\n",
      "\n",
      "i_episode:  89\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  90\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -27.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  91\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  92\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -81.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  93\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 51.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  94\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 11.0000\n",
      "  Training episode took: 0:00:10\n",
      "\n",
      "i_episode:  95\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 27.0000\n",
      "  Training episode took: 0:00:11\n",
      "\n",
      "i_episode:  96\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -100.0000\n",
      "  Training episode took: 0:00:11\n",
      "\n",
      "i_episode:  97\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -99.0000\n",
      "  Training episode took: 0:00:11\n",
      "\n",
      "i_episode:  98\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 118.0000\n",
      "  Training episode took: 0:00:11\n",
      "\n",
      "i_episode:  99\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: 68.0000\n",
      "  Training episode took: 0:00:12\n",
      "\n",
      "i_episode:  100\n",
      "  Episode   100  of    100.    Elapsed: 0:00:12.\n",
      "Ending game\n",
      "\n",
      "  Average training cumulative score: -82.0000\n",
      "  Training episode took: 0:00:12\n",
      "\n",
      "Running Validation...\n",
      "initial grid\n",
      "[[1. 3. 2. 1. 2.]\n",
      " [1. 1. 3. 1. 0.]\n",
      " [0. 3. 3. 2. 1.]\n",
      " [3. 1. 2. 3. 1.]\n",
      " [3. 3. 1. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 0. 2. 2.]\n",
      " [1. 1. 2. 2. 3.]\n",
      " [2. 2. 3. 1. 1.]\n",
      " [2. 2. 1. 2. 3.]\n",
      " [3. 2. 0. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 3. 3. 3.]\n",
      " [1. 2. 3. 3. 3.]\n",
      " [0. 3. 1. 3. 1.]\n",
      " [2. 3. 2. 0. 3.]\n",
      " [0. 1. 2. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 2. 1.]\n",
      " [3. 3. 0. 3. 0.]\n",
      " [0. 1. 3. 3. 2.]\n",
      " [3. 2. 1. 3. 3.]\n",
      " [1. 0. 3. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 0. 3.]\n",
      " [0. 3. 2. 1. 1.]\n",
      " [3. 3. 0. 3. 2.]\n",
      " [0. 3. 3. 3. 1.]\n",
      " [1. 3. 1. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 2. 2.]\n",
      " [2. 2. 2. 3. 3.]\n",
      " [3. 2. 1. 0. 3.]\n",
      " [3. 3. 1. 3. 0.]\n",
      " [0. 3. 2. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 1. 2. 1. 2.]\n",
      " [3. 3. 2. 1. 0.]\n",
      " [2. 1. 3. 2. 3.]\n",
      " [1. 3. 2. 3. 2.]\n",
      " [3. 1. 2. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 3. 0.]\n",
      " [0. 2. 1. 1. 3.]\n",
      " [1. 2. 3. 3. 3.]\n",
      " [1. 2. 3. 2. 0.]\n",
      " [3. 3. 1. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 2.]\n",
      " [2. 1. 3. 3. 1.]\n",
      " [2. 3. 0. 3. 0.]\n",
      " [1. 1. 3. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 2. 2.]\n",
      " [2. 3. 3. 0. 2.]\n",
      " [3. 2. 1. 3. 3.]\n",
      " [0. 1. 3. 3. 3.]\n",
      " [2. 1. 2. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 2. 3. 2.]\n",
      " [3. 3. 2. 1. 2.]\n",
      " [0. 1. 3. 3. 1.]\n",
      " [3. 1. 3. 1. 2.]\n",
      " [2. 2. 1. 0. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 2. 1.]\n",
      " [1. 3. 2. 3. 2.]\n",
      " [1. 2. 2. 3. 2.]\n",
      " [2. 1. 0. 2. 0.]\n",
      " [3. 3. 3. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 1. 3. 1. 2.]\n",
      " [3. 1. 2. 3. 2.]\n",
      " [1. 3. 0. 1. 1.]\n",
      " [1. 1. 3. 1. 3.]\n",
      " [2. 2. 2. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 3. 1.]\n",
      " [3. 1. 2. 3. 3.]\n",
      " [2. 1. 3. 3. 0.]\n",
      " [2. 2. 3. 0. 3.]\n",
      " [3. 2. 0. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 3. 2. 3. 2.]\n",
      " [1. 2. 3. 3. 1.]\n",
      " [3. 3. 1. 1. 2.]\n",
      " [1. 2. 3. 1. 2.]\n",
      " [2. 2. 3. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 2. 3. 3.]\n",
      " [3. 3. 1. 2. 1.]\n",
      " [2. 2. 2. 3. 2.]\n",
      " [2. 1. 2. 3. 0.]\n",
      " [0. 3. 2. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 2. 2. 2.]\n",
      " [3. 3. 2. 1. 0.]\n",
      " [2. 2. 3. 3. 1.]\n",
      " [2. 1. 0. 2. 3.]\n",
      " [2. 2. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 2. 3. 1.]\n",
      " [2. 2. 2. 1. 2.]\n",
      " [0. 2. 3. 1. 3.]\n",
      " [3. 2. 3. 2. 1.]\n",
      " [1. 3. 1. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 3. 2. 0.]\n",
      " [2. 2. 0. 2. 2.]\n",
      " [3. 1. 1. 3. 3.]\n",
      " [1. 3. 2. 3. 0.]\n",
      " [2. 1. 3. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 2. 1. 2.]\n",
      " [2. 1. 2. 3. 3.]\n",
      " [2. 2. 3. 3. 3.]\n",
      " [1. 1. 3. 0. 3.]\n",
      " [2. 1. 2. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 2. 2. 3.]\n",
      " [3. 2. 3. 3. 1.]\n",
      " [3. 2. 0. 3. 1.]\n",
      " [3. 0. 2. 2. 2.]\n",
      " [2. 3. 2. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 1. 1.]\n",
      " [2. 3. 2. 0. 3.]\n",
      " [1. 2. 2. 1. 2.]\n",
      " [3. 0. 3. 3. 3.]\n",
      " [2. 3. 0. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 0. 1. 1.]\n",
      " [2. 2. 2. 2. 1.]\n",
      " [3. 0. 2. 3. 3.]\n",
      " [3. 1. 3. 0. 2.]\n",
      " [3. 3. 1. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 2. 3. 1.]\n",
      " [1. 2. 2. 3. 1.]\n",
      " [3. 0. 2. 0. 3.]\n",
      " [2. 2. 1. 2. 1.]\n",
      " [0. 3. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 3. 3. 0. 3.]\n",
      " [2. 3. 3. 3. 1.]\n",
      " [0. 3. 2. 0. 2.]\n",
      " [3. 3. 2. 3. 2.]\n",
      " [2. 1. 1. 2. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 2. 2. 1.]\n",
      " [2. 1. 1. 0. 3.]\n",
      " [2. 3. 1. 2. 1.]\n",
      " [3. 1. 3. 2. 2.]\n",
      " [0. 3. 1. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 1. 3. 2.]\n",
      " [3. 1. 2. 3. 3.]\n",
      " [2. 3. 1. 1. 2.]\n",
      " [3. 3. 1. 3. 2.]\n",
      " [2. 2. 1. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 3. 3. 0. 1.]\n",
      " [2. 3. 3. 2. 3.]\n",
      " [2. 1. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3.]\n",
      " [2. 3. 2. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 2. 3. 2.]\n",
      " [1. 3. 3. 3. 3.]\n",
      " [3. 1. 3. 1. 1.]\n",
      " [1. 3. 0. 3. 2.]\n",
      " [1. 1. 3. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 2. 2. 3. 1.]\n",
      " [2. 2. 2. 2. 0.]\n",
      " [0. 1. 2. 2. 2.]\n",
      " [1. 3. 3. 3. 2.]\n",
      " [2. 2. 2. 1. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 1. 2. 2. 2.]\n",
      " [2. 3. 3. 2. 2.]\n",
      " [2. 2. 1. 1. 1.]\n",
      " [1. 0. 3. 0. 3.]\n",
      " [2. 1. 3. 2. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 1. 1. 0.]\n",
      " [2. 2. 3. 3. 2.]\n",
      " [2. 3. 1. 3. 0.]\n",
      " [0. 2. 3. 2. 3.]\n",
      " [3. 3. 0. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 2. 0. 2. 3.]\n",
      " [3. 2. 3. 0. 1.]\n",
      " [2. 2. 2. 3. 2.]\n",
      " [2. 3. 1. 0. 2.]\n",
      " [1. 0. 3. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 0. 3. 2. 3.]\n",
      " [1. 2. 3. 0. 3.]\n",
      " [0. 3. 1. 2. 2.]\n",
      " [2. 3. 3. 3. 0.]\n",
      " [3. 1. 3. 0. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 2. 2. 1.]\n",
      " [0. 3. 1. 2. 3.]\n",
      " [3. 3. 0. 2. 3.]\n",
      " [0. 2. 3. 3. 3.]\n",
      " [1. 2. 2. 1. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 0. 2. 2. 1.]\n",
      " [3. 1. 3. 3. 1.]\n",
      " [1. 3. 2. 0. 3.]\n",
      " [3. 1. 2. 1. 2.]\n",
      " [1. 3. 3. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 3. 2. 2.]\n",
      " [3. 1. 0. 3. 2.]\n",
      " [3. 3. 3. 1. 3.]\n",
      " [1. 1. 1. 2. 2.]\n",
      " [1. 2. 2. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 2. 3.]\n",
      " [2. 3. 3. 0. 2.]\n",
      " [0. 2. 3. 1. 2.]\n",
      " [2. 2. 1. 3. 3.]\n",
      " [0. 1. 3. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 3. 3.]\n",
      " [3. 1. 1. 2. 2.]\n",
      " [0. 3. 3. 3. 2.]\n",
      " [1. 2. 0. 2. 0.]\n",
      " [3. 1. 3. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 2. 2.]\n",
      " [1. 3. 2. 3. 1.]\n",
      " [1. 3. 2. 3. 1.]\n",
      " [0. 2. 0. 2. 2.]\n",
      " [2. 3. 2. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 1. 1.]\n",
      " [3. 1. 1. 2. 3.]\n",
      " [3. 3. 1. 1. 2.]\n",
      " [3. 2. 3. 2. 0.]\n",
      " [1. 1. 0. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 2. 1.]\n",
      " [3. 2. 2. 3. 3.]\n",
      " [3. 3. 2. 2. 2.]\n",
      " [0. 2. 1. 1. 2.]\n",
      " [2. 0. 3. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 2. 3. 2.]\n",
      " [1. 2. 2. 2. 2.]\n",
      " [3. 1. 2. 0. 1.]\n",
      " [1. 2. 0. 3. 1.]\n",
      " [3. 3. 2. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 0. 3. 3.]\n",
      " [1. 3. 3. 0. 3.]\n",
      " [0. 2. 2. 1. 2.]\n",
      " [2. 2. 3. 2. 3.]\n",
      " [3. 2. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 3. 2. 0.]\n",
      " [1. 2. 3. 1. 3.]\n",
      " [2. 3. 2. 3. 3.]\n",
      " [2. 1. 1. 2. 0.]\n",
      " [2. 3. 2. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 2. 1. 1. 0.]\n",
      " [3. 3. 1. 3. 2.]\n",
      " [1. 2. 2. 0. 2.]\n",
      " [1. 3. 2. 3. 3.]\n",
      " [2. 3. 2. 0. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 2. 3. 0.]\n",
      " [3. 3. 0. 2. 3.]\n",
      " [3. 1. 3. 1. 2.]\n",
      " [2. 1. 2. 3. 2.]\n",
      " [1. 2. 2. 1. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 1. 3. 2.]\n",
      " [1. 3. 3. 2. 1.]\n",
      " [0. 2. 2. 0. 2.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 1. 0. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 3. 2. 2. 3.]\n",
      " [1. 0. 1. 3. 3.]\n",
      " [2. 3. 2. 2. 3.]\n",
      " [1. 3. 2. 3. 3.]\n",
      " [2. 3. 0. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 2. 0. 3.]\n",
      " [3. 2. 3. 3. 2.]\n",
      " [3. 2. 1. 3. 2.]\n",
      " [1. 0. 3. 3. 2.]\n",
      " [3. 3. 0. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 1. 2. 1.]\n",
      " [2. 3. 3. 0. 2.]\n",
      " [3. 3. 1. 1. 3.]\n",
      " [2. 2. 2. 3. 1.]\n",
      " [3. 2. 2. 0. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 0. 2. 2. 3.]\n",
      " [0. 2. 3. 1. 3.]\n",
      " [3. 3. 2. 2. 3.]\n",
      " [0. 2. 1. 2. 2.]\n",
      " [1. 3. 0. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 2. 3.]\n",
      " [1. 3. 1. 3. 1.]\n",
      " [3. 0. 1. 2. 1.]\n",
      " [2. 3. 2. 1. 3.]\n",
      " [2. 3. 1. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 2. 3. 3. 3.]\n",
      " [3. 1. 3. 2. 1.]\n",
      " [2. 3. 0. 2. 3.]\n",
      " [2. 3. 3. 3. 1.]\n",
      " [1. 3. 1. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 1. 3. 0.]\n",
      " [1. 3. 1. 2. 3.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [1. 1. 1. 2. 3.]\n",
      " [3. 2. 1. 0. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 3. 3. 1.]\n",
      " [2. 0. 2. 3. 2.]\n",
      " [3. 2. 3. 3. 2.]\n",
      " [1. 0. 3. 3. 0.]\n",
      " [2. 1. 2. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 1. 1. 3.]\n",
      " [3. 3. 2. 1. 3.]\n",
      " [2. 1. 3. 1. 2.]\n",
      " [0. 3. 3. 2. 1.]\n",
      " [3. 0. 3. 1. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 2. 1.]\n",
      " [0. 3. 3. 1. 0.]\n",
      " [2. 2. 2. 3. 2.]\n",
      " [3. 0. 2. 3. 2.]\n",
      " [3. 3. 3. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 2. 0. 1.]\n",
      " [2. 2. 3. 3. 2.]\n",
      " [1. 1. 2. 2. 1.]\n",
      " [2. 3. 3. 1. 2.]\n",
      " [3. 1. 2. 0. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 2. 3. 2.]\n",
      " [3. 3. 0. 2. 3.]\n",
      " [3. 2. 1. 1. 2.]\n",
      " [1. 2. 2. 2. 3.]\n",
      " [3. 3. 0. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 0. 2. 2.]\n",
      " [2. 3. 2. 3. 1.]\n",
      " [1. 2. 2. 1. 1.]\n",
      " [0. 2. 3. 3. 0.]\n",
      " [1. 1. 3. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 3. 2. 2. 0.]\n",
      " [1. 2. 1. 3. 2.]\n",
      " [3. 3. 0. 2. 3.]\n",
      " [3. 2. 3. 0. 3.]\n",
      " [3. 0. 3. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 3. 2. 1. 3.]\n",
      " [2. 2. 3. 2. 3.]\n",
      " [3. 3. 0. 1. 1.]\n",
      " [2. 1. 2. 2. 2.]\n",
      " [2. 3. 0. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 1. 2. 1.]\n",
      " [1. 3. 1. 3. 3.]\n",
      " [2. 1. 2. 2. 2.]\n",
      " [3. 2. 3. 3. 3.]\n",
      " [1. 2. 1. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 3. 2. 2.]\n",
      " [0. 2. 3. 2. 3.]\n",
      " [2. 3. 0. 1. 2.]\n",
      " [1. 2. 3. 2. 2.]\n",
      " [3. 2. 1. 0. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 0. 3.]\n",
      " [2. 2. 1. 3. 2.]\n",
      " [2. 2. 1. 2. 2.]\n",
      " [3. 3. 3. 3. 2.]\n",
      " [1. 0. 3. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 3. 2.]\n",
      " [2. 3. 2. 3. 0.]\n",
      " [3. 0. 3. 3. 3.]\n",
      " [1. 3. 3. 3. 3.]\n",
      " [0. 2. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 2. 3. 1.]\n",
      " [2. 0. 3. 3. 1.]\n",
      " [2. 3. 1. 3. 0.]\n",
      " [2. 2. 3. 3. 3.]\n",
      " [2. 2. 2. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 2. 3. 1.]\n",
      " [0. 3. 3. 1. 1.]\n",
      " [3. 1. 3. 0. 3.]\n",
      " [2. 1. 2. 3. 2.]\n",
      " [1. 0. 3. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 2. 3. 1.]\n",
      " [3. 1. 3. 2. 1.]\n",
      " [2. 2. 2. 1. 3.]\n",
      " [3. 2. 1. 0. 1.]\n",
      " [0. 2. 3. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 2. 3.]\n",
      " [1. 3. 2. 2. 0.]\n",
      " [1. 3. 3. 1. 3.]\n",
      " [2. 1. 1. 3. 3.]\n",
      " [3. 1. 3. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 2. 3. 3.]\n",
      " [1. 0. 2. 2. 0.]\n",
      " [3. 2. 1. 3. 1.]\n",
      " [0. 3. 2. 2. 1.]\n",
      " [3. 1. 3. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 3. 3. 3.]\n",
      " [2. 2. 3. 1. 3.]\n",
      " [3. 1. 3. 3. 3.]\n",
      " [0. 1. 1. 3. 0.]\n",
      " [2. 2. 2. 0. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 3. 0. 2. 2.]\n",
      " [3. 2. 3. 2. 3.]\n",
      " [1. 3. 1. 3. 3.]\n",
      " [2. 2. 2. 2. 1.]\n",
      " [3. 0. 1. 2. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 2. 0. 2.]\n",
      " [2. 1. 1. 2. 2.]\n",
      " [3. 0. 3. 2. 0.]\n",
      " [1. 3. 2. 3. 3.]\n",
      " [1. 3. 3. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 3. 2.]\n",
      " [2. 2. 3. 2. 1.]\n",
      " [3. 2. 0. 3. 3.]\n",
      " [3. 2. 1. 1. 2.]\n",
      " [2. 2. 3. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 2. 2. 0.]\n",
      " [3. 3. 3. 3. 1.]\n",
      " [1. 1. 3. 2. 2.]\n",
      " [2. 0. 1. 2. 2.]\n",
      " [3. 2. 2. 1. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 2. 2.]\n",
      " [3. 3. 1. 3. 0.]\n",
      " [3. 3. 1. 3. 3.]\n",
      " [3. 1. 3. 1. 3.]\n",
      " [0. 2. 2. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 2. 2. 1. 3.]\n",
      " [2. 3. 1. 2. 3.]\n",
      " [0. 2. 1. 3. 1.]\n",
      " [3. 2. 2. 2. 1.]\n",
      " [2. 3. 1. 0. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 3. 2. 2.]\n",
      " [2. 2. 2. 1. 3.]\n",
      " [3. 0. 1. 3. 2.]\n",
      " [0. 2. 3. 3. 0.]\n",
      " [3. 3. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 1. 0. 3. 2.]\n",
      " [2. 3. 3. 0. 2.]\n",
      " [0. 1. 3. 1. 1.]\n",
      " [3. 1. 3. 3. 2.]\n",
      " [3. 1. 1. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 2. 1. 2.]\n",
      " [0. 2. 0. 3. 1.]\n",
      " [3. 2. 3. 3. 3.]\n",
      " [1. 1. 3. 3. 2.]\n",
      " [2. 2. 0. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 3. 2. 2.]\n",
      " [3. 1. 3. 1. 3.]\n",
      " [1. 1. 3. 2. 0.]\n",
      " [3. 2. 1. 1. 1.]\n",
      " [1. 1. 3. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 1. 3. 3. 1.]\n",
      " [2. 2. 3. 2. 3.]\n",
      " [2. 3. 3. 3. 3.]\n",
      " [0. 3. 1. 3. 2.]\n",
      " [3. 3. 1. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 0. 3. 3.]\n",
      " [1. 3. 3. 3. 2.]\n",
      " [1. 2. 2. 2. 0.]\n",
      " [3. 2. 2. 2. 3.]\n",
      " [2. 3. 3. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 0. 2. 3.]\n",
      " [3. 1. 1. 3. 0.]\n",
      " [2. 3. 3. 2. 2.]\n",
      " [2. 1. 0. 3. 3.]\n",
      " [1. 2. 3. 1. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 3. 1. 2. 2.]\n",
      " [1. 2. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 1.]\n",
      " [2. 3. 2. 1. 2.]\n",
      " [3. 3. 2. 3. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 1. 3.]\n",
      " [1. 2. 3. 1. 3.]\n",
      " [3. 3. 1. 0. 1.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [0. 1. 1. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 2. 0.]\n",
      " [0. 3. 1. 3. 1.]\n",
      " [3. 1. 1. 2. 2.]\n",
      " [3. 0. 2. 3. 3.]\n",
      " [1. 2. 2. 1. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 3. 1. 3.]\n",
      " [2. 1. 0. 3. 3.]\n",
      " [3. 2. 2. 3. 1.]\n",
      " [2. 3. 1. 2. 0.]\n",
      " [3. 3. 2. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 3. 2.]\n",
      " [3. 3. 3. 1. 3.]\n",
      " [2. 2. 0. 3. 1.]\n",
      " [3. 2. 2. 1. 0.]\n",
      " [1. 2. 3. 3. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[3. 2. 0. 3. 2.]\n",
      " [2. 3. 2. 1. 2.]\n",
      " [3. 1. 3. 2. 3.]\n",
      " [2. 3. 3. 0. 3.]\n",
      " [2. 2. 0. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 1. 1. 3. 2.]\n",
      " [0. 1. 2. 3. 0.]\n",
      " [1. 3. 3. 0. 3.]\n",
      " [2. 2. 1. 3. 0.]\n",
      " [2. 2. 1. 2. 3.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3.]\n",
      " [2. 1. 3. 0. 2.]\n",
      " [2. 3. 1. 2. 2.]\n",
      " [1. 2. 2. 2. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 2. 3. 1. 0.]\n",
      " [0. 3. 1. 2. 3.]\n",
      " [2. 3. 3. 2. 1.]\n",
      " [3. 3. 2. 3. 2.]\n",
      " [0. 3. 1. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 0. 2. 2. 3.]\n",
      " [0. 2. 1. 2. 3.]\n",
      " [3. 1. 2. 2. 1.]\n",
      " [1. 1. 3. 2. 2.]\n",
      " [1. 2. 3. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[2. 3. 0. 3. 1.]\n",
      " [3. 1. 2. 2. 0.]\n",
      " [2. 3. 3. 3. 2.]\n",
      " [3. 1. 2. 1. 3.]\n",
      " [3. 3. 3. 3. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 0. 1. 2.]\n",
      " [3. 2. 1. 2. 3.]\n",
      " [0. 3. 3. 2. 2.]\n",
      " [1. 1. 2. 1. 1.]\n",
      " [1. 3. 2. 3. 1.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[0. 2. 3. 2. 3.]\n",
      " [2. 3. 0. 3. 3.]\n",
      " [2. 1. 3. 2. 1.]\n",
      " [3. 3. 0. 3. 1.]\n",
      " [2. 0. 3. 2. 0.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "initial grid\n",
      "[[1. 3. 3. 1. 2.]\n",
      " [2. 2. 2. 1. 1.]\n",
      " [3. 1. 3. 1. 3.]\n",
      " [0. 3. 2. 2. 1.]\n",
      " [1. 2. 3. 2. 2.]]\n",
      "Ending game\n",
      "action:  0\n",
      "log_prob:  tensor(-1.1921e-07, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "  Validation Score: -100.0000\n",
      "  Validation took: 0:00:01\n",
      "Validation scores\n",
      "[-100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0, -100.0]\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:13 (h:mm:ss)\n",
      "Saving model to /staging_area/reinforce-agent/\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /staging_area/reinforce-agent does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 271\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m#save model params\u001b[39;00m\n\u001b[1;32m    270\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrl_policy_params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 271\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrl_policy_full.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    274\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(rl_policy, output_dir\u001b[38;5;241m+\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /staging_area/reinforce-agent does not exist."
     ]
    }
   ],
   "source": [
    "N_training_episodes = 1000\n",
    "N_val_episodes = 100\n",
    "gamma = 0.99\n",
    "\n",
    "optimizer = torch.optim.Adam(rl_policy.parameters(), lr=0.001, betas=(0.9, 0.998), eps=1e-9, weight_decay=1e-4)\n",
    "\n",
    "start_epoch = 0\n",
    "FROM_CHECKPOINT = not True\n",
    "if FROM_CHECKPOINT:\n",
    "    \n",
    "    checkpoint = torch.load(output_dir+'best_gait_model.tar')\n",
    "    g = checkpoint['model_state_dict']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Lowest Loss: {loss}')\n",
    "    save_best_model = SaveBestModel(output_dir+best_model_name, loss)\n",
    "    # print(g.keys())\n",
    "    model.load_state_dict(g)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'From epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "SAVE_EVERY_EPOCH_N = 1\n",
    "\n",
    "training_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "print(\"\")\n",
    "print('Training...')\n",
    "# Measure how long the training epoch takes.\n",
    "t0 = time.time()\n",
    "rl_policy.train()\n",
    "for i_episode in range(1, N_training_episodes+1):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print()\n",
    "    print('i_episode: ', i_episode)\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "\n",
    "    # Progress update every 40 batches.\n",
    "    if i_episode % 100 == 0 and not i_episode == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Episode {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_episode, N_training_episodes, elapsed))\n",
    "\n",
    "    cumulative_scores_episode = []\n",
    "    \n",
    "\n",
    "    # generate initial grid\n",
    "    # run the sandpile 1000 times\n",
    "    initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    # initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print('generated initial grid')\n",
    "    # print(initial_grid)\n",
    "\n",
    "\n",
    "    rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    agents = [rl_policy_agent]\n",
    "\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    pos = rl_policy_agent.get_agent_pos()\n",
    "    # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    # sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print('Step i: ', i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "        # print(sandpile_grid)\n",
    "        # sandpile.print_grid_and_agent_pos(rl_policy_agent)\n",
    "        # print(agent_rewards)\n",
    "        # print(game_is_running)\n",
    "        pos = rl_policy_agent.get_agent_pos()\n",
    "        # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        # print('action: ', action)\n",
    "        # print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # input()\n",
    "\n",
    "    cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # print('episode_rewards', episode_rewards)\n",
    "    # print('cumulative_scores_episode', cumulative_scores_episode)\n",
    "\n",
    "    returns = deque(maxlen=max_nmoves_per_episode)\n",
    "    n_steps_episode = len(episode_rewards)\n",
    "\n",
    "    print('n_steps_episode', n_steps_episode)\n",
    "\n",
    "    #TODO: replace with reverse numpy cumsum?\n",
    "    for t in range(n_steps_episode)[::-1]:\n",
    "        discounted_return_t = returns[0] if len(returns) > 0 else 0\n",
    "        returns.appendleft(gamma * discounted_return_t + episode_rewards[t])\n",
    "\n",
    "    # print('Pre standard')\n",
    "    # print('log_probs: ', log_probs)\n",
    "    # print('returns: ', returns)\n",
    "\n",
    "\n",
    "    #standardize\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    else:\n",
    "        returns = (returns - returns.mean())\n",
    "\n",
    "    # compute loss\n",
    "    # print('Post standard')\n",
    "    # print('returns: ', returns)\n",
    "\n",
    "    # policy_loss = []\n",
    "    policy_loss = 0\n",
    "\n",
    "    for log_prob, disc_return in zip(log_probs, returns):\n",
    "        # print('log_prob ', log_prob)\n",
    "        # print('disc_return ', disc_return)\n",
    "        policy_loss += -log_prob * disc_return\n",
    "        # policy_loss.append(-log_prob * disc_return)\n",
    "    # policy_loss = torch.\n",
    "    # cat(policy_loss).sum()\n",
    "\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_score = np.mean(cumulative_scores_episode)     \n",
    "    training_scores.append(avg_train_score)\n",
    "    # Measure how long this episode took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training cumulative score: {0:.4f}\".format(avg_train_score))\n",
    "    print(\"  Training episode took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "rl_policy.eval()\n",
    "\n",
    "for episode_i in range(N_val_episodes):\n",
    "\n",
    "    cumulative_scores_episode = []\n",
    "\n",
    "\n",
    "    # generate initial grid\n",
    "    # run the sandpile 1000 times\n",
    "    initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    print('initial grid')\n",
    "    print(initial_grid)\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print(i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        print('action: ', action)\n",
    "        print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "    cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    avg_val_score = np.mean(cumulative_scores_episode)\n",
    "\n",
    "\n",
    "    validation_scores.append(avg_val_score)\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"  Validation Score: {0:.4f}\".format(avg_val_score))\n",
    "print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "#save best model\n",
    "# save_best_model(\n",
    "#     avg_val_score, episode_i+1, rl_policy, optimizer\n",
    "# )\n",
    "\n",
    "#print training vals\n",
    "print('Validation scores')\n",
    "print(validation_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "#save model params\n",
    "model_name = 'rl_policy_params.pt'\n",
    "torch.save(rl_policy.state_dict(), output_dir+model_name)\n",
    "\n",
    "model_name = 'rl_policy_full.pt'\n",
    "torch.save(rl_policy, output_dir+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KFrle1f3fxJP",
    "outputId": "65ca3ae9-d33d-4184-bd58-99b6038128f3"
   },
   "outputs": [],
   "source": [
    "training_scores = np.array(training_scores)\n",
    "validation_scores = np.array(validation_scores)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(training_scores,'-',label='Train')\n",
    "axs.set_ylabel('Scores')\n",
    "axs.plot(validation_scores,'-',label='Val')\n",
    "axs.set_xlabel('Episode')\n",
    "axs.legend()\n",
    "print(np.min(validation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn0gYmJ3dSt8",
    "outputId": "4d16be89-fe4d-456f-a13f-f1129cf94f86"
   },
   "outputs": [],
   "source": [
    "params = list(best_model.named_parameters())\n",
    "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "for p in params:\n",
    "    # print('p')\n",
    "    # print(p[0])\n",
    "    # print(p[1].data)\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7PxtSeLVlak"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../staging_area/gait-model ../full_models/\n",
    "!zip -r ../full_models/gait-model.zip ../full_models/gait-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7184MNpqVlal"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../full_models/gait-model/ gs://ml_gait_estimation/full_models/\n",
    "!gsutil cp ../full_models/gait-model.zip gs://ml_gait_estimation/full_models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
