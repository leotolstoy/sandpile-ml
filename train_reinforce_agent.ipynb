{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoK10It3TUN2",
    "outputId": "115d4998-5725-4fe6-d9c4-7f5597f5eeff"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_built_with_cuda()\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from save_best_model import SaveBestModel\n",
    "from sandpile import Sandpile, run_sandpile_alone\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import datetime\n",
    "from rl_agents import Policy\n",
    "from agents import RLPolicyAgent\n",
    "from util import Directions\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "#RUN THIS ON COLAB\n",
    "ON_COLAB = False\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/Phase ML Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4YR0pTPWV3Dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /staging_area/reinforce-agent/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "model_nickname = 'reinforce-agent'\n",
    "\n",
    "output_dir = f'/staging_area/{model_nickname}/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# checkpoint_dir = 'checkpoints/'\n",
    "# if not os.path.exists(output_dir+checkpoint_dir):\n",
    "#     os.makedirs(output_dir+checkpoint_dir)\n",
    "\n",
    "\n",
    "best_model_name = 'best_agent.tar'\n",
    "save_best_model = SaveBestModel(output_dir+best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "Total Trainable Params: 18757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=27, out_features=64, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.0, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (10): GELU(approximate='none')\n",
       "    (11): Dropout(p=0.0, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (13): GELU(approximate='none')\n",
       "    (14): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def enum_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(rl_policy)\n",
    "\n",
    "\n",
    "# SET UP POLICY AGENT\n",
    "N_grid = 10\n",
    "num_hidden_layers = 4\n",
    "hidden_dim = 64\n",
    "input_dim = N_grid**2 + 2# The number of input variables. \n",
    "output_dim = len(Directions) # The number of output variables. \n",
    "\n",
    "rl_policy = Policy(\n",
    "    input_dim=input_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    device=device\n",
    ")\n",
    "enum_parameters(rl_policy)\n",
    "rl_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(-1.6536, device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXIMUM_GRAINS = 4\n",
    "max_nmoves_per_episode = 1000\n",
    "\n",
    "rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "agents = [rl_policy_agent]\n",
    "\n",
    "# start new sandpile with initial grid\n",
    "sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=10)\n",
    "rl_policy.select_action(sandpile, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xJFUThsu9tzX",
    "outputId": "c4a12851-65aa-495b-95ca-c2d18abfe757",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "i_episode:  1\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(0.1217, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  2\n",
      "episode_rewards:  [3.0, 1.0, -2, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 14\n",
      "tensor(-0.0524, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  3\n",
      "episode_rewards:  [2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -965.0\n",
      "n_steps_episode 16\n",
      "tensor(0.1177, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  4\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(-0.0550, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  5\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -989.0\n",
      "n_steps_episode 6\n",
      "tensor(0.0976, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  6\n",
      "episode_rewards:  [3.0, 3.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -993.0\n",
      "n_steps_episode 4\n",
      "tensor(-0.0369, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  7\n",
      "episode_rewards:  [0.0, 0.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 2.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 13\n",
      "tensor(0.1586, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  8\n",
      "episode_rewards:  [0.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -1000.0\n",
      "n_steps_episode 2\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  9\n",
      "episode_rewards:  [3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -985.0\n",
      "n_steps_episode 8\n",
      "tensor(0.0962, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  10\n",
      "episode_rewards:  [3.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -996.0\n",
      "n_steps_episode 3\n",
      "tensor(-0.0465, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  11\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  12\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -978.0\n",
      "n_steps_episode 9\n",
      "tensor(-0.1384, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  13\n",
      "episode_rewards:  [2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -995.0\n",
      "n_steps_episode 3\n",
      "tensor(0.0026, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  14\n",
      "episode_rewards:  [3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -967.0\n",
      "n_steps_episode 15\n",
      "tensor(0.0031, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  15\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  16\n",
      "episode_rewards:  [2.0, 3.0, 1.0, 3.0, 2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 12\n",
      "tensor(-0.1601, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  17\n",
      "episode_rewards:  [2.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -998.0\n",
      "n_steps_episode 2\n",
      "tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  18\n",
      "episode_rewards:  [-1, 2.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -999.0\n",
      "n_steps_episode 3\n",
      "tensor(-0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  19\n",
      "episode_rewards:  [3.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.1254, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  20\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 1.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -970.0\n",
      "n_steps_episode 12\n",
      "tensor(0.1515, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  21\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  22\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -990.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.0648, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  23\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 2.0, -1, 2.0, 0.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 1.0, -1, 2.0, 0.0, 3.0, 0.0, 2.0, -1, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 0.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -908.0\n",
      "n_steps_episode 48\n",
      "tensor(-0.4802, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  24\n",
      "episode_rewards:  [2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 3.0, 3.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -971.0\n",
      "n_steps_episode 16\n",
      "tensor(-0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  25\n",
      "episode_rewards:  [1.0, 3.0, -2, 2.0, 0.0, 2.0, 0.0, 2.0, 3.0, 3.0, 2.0, -1, 2.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 15\n",
      "tensor(-0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  26\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -999.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.0513, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  27\n",
      "episode_rewards:  [2.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -998.0\n",
      "n_steps_episode 3\n",
      "tensor(0.0027, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  28\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.UP: 3>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  29\n",
      "episode_rewards:  [2.0, 2.0, 1.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -983.0\n",
      "n_steps_episode 8\n",
      "tensor(-0.0955, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  30\n",
      "episode_rewards:  [2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -996.0\n",
      "n_steps_episode 3\n",
      "tensor(0.1084, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  31\n",
      "episode_rewards:  [3.0, 0.0, 2.0, 0.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -986.0\n",
      "n_steps_episode 13\n",
      "tensor(-0.2664, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  32\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 1.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -988.0\n",
      "n_steps_episode 6\n",
      "tensor(-0.0118, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  33\n",
      "episode_rewards:  [1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 2.0, 2.0, -1, 1.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -952.0\n",
      "n_steps_episode 25\n",
      "tensor(-0.2543, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  34\n",
      "episode_rewards:  [2.0, 3.0, 2.0, 3.0, 2.0, 3.0, -1, -1, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -985.0\n",
      "n_steps_episode 10\n",
      "tensor(0.2898, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  35\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 11\n",
      "tensor(0.0020, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  36\n",
      "episode_rewards:  [3.0, 0.0, 2.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.1271, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  37\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  38\n",
      "episode_rewards:  [1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 3.0, 3.0, 3.0, 0.0, 3.0, 3.0, -1, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -953.0\n",
      "n_steps_episode 25\n",
      "tensor(0.0945, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  39\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 2.0, 1.0, -1, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 7\n",
      "tensor(-0.1918, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  40\n",
      "episode_rewards:  [1.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.4245, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  41\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, -2, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, -3, 2.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -930.0\n",
      "n_steps_episode 33\n",
      "tensor(-0.3478, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  42\n",
      "episode_rewards:  [3.0, 1.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 5\n",
      "tensor(0.0103, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  43\n",
      "episode_rewards:  [1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 10\n",
      "tensor(0.4881, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  44\n",
      "episode_rewards:  [-1, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 1.0, 1.0, 3.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 13\n",
      "tensor(0.1548, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  45\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(-0.2587, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  46\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  47\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  48\n",
      "episode_rewards:  [3.0, 1.0, 1.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0, 3.0, 3.0, 1.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, -2, 3.0, -1, 3.0, 3.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 3.0, 0.0, 3.0, 3.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -931.0\n",
      "n_steps_episode 38\n",
      "tensor(-0.8434, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  49\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(0.0324, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  50\n",
      "episode_rewards:  [3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 10\n",
      "tensor(0.2652, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  51\n",
      "episode_rewards:  [0.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000.0\n",
      "n_steps_episode 2\n",
      "tensor(-0.2262, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  52\n",
      "episode_rewards:  [3.0, 3.0, 1.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -993.0\n",
      "n_steps_episode 5\n",
      "tensor(0.3190, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  53\n",
      "episode_rewards:  [3.0, -2, -1, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -1000.0\n",
      "n_steps_episode 4\n",
      "tensor(-0.4537, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  54\n",
      "episode_rewards:  [3.0, 1.0, 3.0, 0.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 6\n",
      "tensor(-0.0264, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  55\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 7\n",
      "tensor(-0.3003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  56\n",
      "episode_rewards:  [0.0, 0.0, 1.0, -1, 0.0, 1.0, 2.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -995.0\n",
      "n_steps_episode 10\n",
      "tensor(-0.6933, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  57\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  58\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 1.0, 3.0, -1, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -978.0\n",
      "n_steps_episode 11\n",
      "tensor(-0.9018, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  59\n",
      "episode_rewards:  [3.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 3.0, 0.0, 3.0, 2.0, 1.0, 3.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -968.0\n",
      "n_steps_episode 16\n",
      "tensor(-0.3381, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  60\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  61\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 2.0, 2.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -988.0\n",
      "n_steps_episode 7\n",
      "tensor(0.4012, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  62\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -989.0\n",
      "n_steps_episode 6\n",
      "tensor(-0.0709, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  63\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  64\n",
      "episode_rewards:  [0.0, 3.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 4\n",
      "tensor(0.2301, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  65\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(0.2635, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  66\n",
      "episode_rewards:  [3.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -992.0\n",
      "n_steps_episode 8\n",
      "tensor(-0.0668, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  67\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  68\n",
      "episode_rewards:  [3.0, 2.0, 2.0, 0.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 6\n",
      "tensor(-0.2716, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  69\n",
      "episode_rewards:  [2.0, 3.0, 2.0, 3.0, -2, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -951.0\n",
      "n_steps_episode 25\n",
      "tensor(-0.8404, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  70\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  71\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(-1.0421, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  72\n",
      "episode_rewards:  [2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 13\n",
      "tensor(-0.1865, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  73\n",
      "episode_rewards:  [1.0, 3.0, 3.0, 3.0, 1.0, 3.0, 3.0, -1, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -964.0\n",
      "n_steps_episode 17\n",
      "tensor(-0.3797, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  74\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -988.0\n",
      "n_steps_episode 8\n",
      "tensor(0.6359, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  75\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(-1.9100, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  76\n",
      "episode_rewards:  [2.0, 3.0, 2.0, 0.0, 0.0, 0.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.UP: 3>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -993.0\n",
      "n_steps_episode 8\n",
      "tensor(3.2138, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  77\n",
      "episode_rewards:  [1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -998.0\n",
      "n_steps_episode 3\n",
      "tensor(-0.4937, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  78\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  79\n",
      "episode_rewards:  [3.0, 3.0, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -992.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.8362, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  80\n",
      "episode_rewards:  [1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -998.0\n",
      "n_steps_episode 3\n",
      "tensor(-1.3812, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  81\n",
      "episode_rewards:  [1.0, 2.0, 1.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -956.0\n",
      "n_steps_episode 19\n",
      "tensor(2.4171, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  82\n",
      "episode_rewards:  [2.0, 1.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -988.0\n",
      "n_steps_episode 7\n",
      "tensor(0.0906, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  83\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 0.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -998.0\n",
      "n_steps_episode 5\n",
      "tensor(-0.5615, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  84\n",
      "episode_rewards:  [3.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -993.0\n",
      "n_steps_episode 4\n",
      "tensor(0.3326, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  85\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(-0.8541, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  86\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, -1, 1.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -972.0\n",
      "n_steps_episode 17\n",
      "tensor(0.7047, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  87\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  88\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 9\n",
      "tensor(-2.8246, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  89\n",
      "episode_rewards:  [0.0, 2.0, 3.0, -1, 2.0, 0.0, 2.0, 2.0, 3.0, 2.0, 0.0, 0.0, 0.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.RIGHT: 2>]\n",
      "cumulative_score_episode -972.0\n",
      "n_steps_episode 20\n",
      "tensor(2.2701, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  90\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, -1000]\n",
      "agent_moves:  [<Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -919.0\n",
      "n_steps_episode 43\n",
      "tensor(-4.3794, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  91\n",
      "episode_rewards:  [1.0, 1.0, 1.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 2.0, 3.0, 2.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>, <Directions.RIGHT: 2>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -938.0\n",
      "n_steps_episode 37\n",
      "tensor(-6.1449, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  92\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  93\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  94\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.UP: 3>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  95\n",
      "episode_rewards:  [3.0, -1, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -968.0\n",
      "n_steps_episode 23\n",
      "tensor(-3.2289, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  96\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, -1, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.DOWN: 4>, <Directions.LEFT: 1>]\n",
      "cumulative_score_episode -954.0\n",
      "n_steps_episode 36\n",
      "tensor(-14.7584, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  97\n",
      "episode_rewards:  [2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.UP: 3>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 15\n",
      "tensor(-5.1250, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  98\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(0.0026, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  99\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(0.0107, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  100\n",
      "  Episode   100  of  10,000.    Elapsed: 0:00:04.\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.RIGHT: 2>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -858.0\n",
      "n_steps_episode 60\n",
      "tensor(0.2510, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  101\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.LEFT: 1>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -428.0\n",
      "n_steps_episode 274\n",
      "tensor(2.4114, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  102\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -972.0\n",
      "n_steps_episode 25\n",
      "tensor(-0.0092, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  103\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -937.0\n",
      "n_steps_episode 31\n",
      "tensor(-0.0131, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  104\n",
      "episode_rewards:  [-1, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -992.0\n",
      "n_steps_episode 5\n",
      "tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  105\n",
      "episode_rewards:  [-1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -902.0\n",
      "n_steps_episode 41\n",
      "tensor(0.0023, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  106\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -918.0\n",
      "n_steps_episode 43\n",
      "tensor(-0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  107\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -903.0\n",
      "n_steps_episode 48\n",
      "tensor(0.0040, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  108\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -898.0\n",
      "n_steps_episode 51\n",
      "tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  109\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -785.0\n",
      "n_steps_episode 99\n",
      "tensor(0.0022, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  110\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -970.0\n",
      "n_steps_episode 14\n",
      "tensor(-2.3508e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  111\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -947.0\n",
      "n_steps_episode 23\n",
      "tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  112\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -894.0\n",
      "n_steps_episode 53\n",
      "tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  113\n",
      "episode_rewards:  [2.0, 2.0, -1, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -775.0\n",
      "n_steps_episode 98\n",
      "tensor(-0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  114\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -2, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -2, 3.0, -1, 3.0, -1, 3.0, -2, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -876.0\n",
      "n_steps_episode 65\n",
      "tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  115\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -968.0\n",
      "n_steps_episode 16\n",
      "tensor(-4.0035e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  116\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -700.0\n",
      "n_steps_episode 144\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  117\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -984.0\n",
      "n_steps_episode 8\n",
      "tensor(4.8909e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  118\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -938.0\n",
      "n_steps_episode 35\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  119\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -970.0\n",
      "n_steps_episode 12\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  120\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -710.0\n",
      "n_steps_episode 140\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  121\n",
      "episode_rewards:  [3.0, 3.0, -3, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 1.0, 1.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -871.0\n",
      "n_steps_episode 59\n",
      "tensor(-0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  122\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -959.0\n",
      "n_steps_episode 22\n",
      "tensor(2.4340e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  123\n",
      "episode_rewards:  [-1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -922.0\n",
      "n_steps_episode 40\n",
      "tensor(4.9075e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  124\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -3, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -854.0\n",
      "n_steps_episode 75\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  125\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -955.0\n",
      "n_steps_episode 21\n",
      "tensor(3.0609e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  126\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -956.0\n",
      "n_steps_episode 30\n",
      "tensor(-1.0003e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  127\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -984.0\n",
      "n_steps_episode 9\n",
      "tensor(7.6625e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  128\n",
      "episode_rewards:  [-1, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -907.0\n",
      "n_steps_episode 43\n",
      "tensor(-1.3283e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  129\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(4.9770e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  130\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -3, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -424.0\n",
      "n_steps_episode 286\n",
      "tensor(1.6537e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  131\n",
      "episode_rewards:  [-1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -954.0\n",
      "n_steps_episode 29\n",
      "tensor(-2.4419e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  132\n",
      "episode_rewards:  [2.0, 2.0, -1, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -460.0\n",
      "n_steps_episode 267\n",
      "tensor(-3.0645e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  133\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -909.0\n",
      "n_steps_episode 42\n",
      "tensor(-1.2212e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  134\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -3, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -531.0\n",
      "n_steps_episode 213\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  135\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -887.0\n",
      "n_steps_episode 52\n",
      "tensor(6.7526e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  136\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -902.0\n",
      "n_steps_episode 47\n",
      "tensor(5.1150e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  137\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 16\n",
      "tensor(-1.0564e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  138\n",
      "episode_rewards:  [3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -925.0\n",
      "n_steps_episode 35\n",
      "tensor(1.4359e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  139\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -958.0\n",
      "n_steps_episode 22\n",
      "tensor(2.8873e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  140\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -940.0\n",
      "n_steps_episode 36\n",
      "tensor(8.2641e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  141\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -932.0\n",
      "n_steps_episode 38\n",
      "tensor(1.1600e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  142\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(4.2147e-08, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  143\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -858.0\n",
      "n_steps_episode 73\n",
      "tensor(8.8549e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  144\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 7\n",
      "tensor(5.2447e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  145\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -948.0\n",
      "n_steps_episode 28\n",
      "tensor(-3.8155e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  146\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -778.0\n",
      "n_steps_episode 97\n",
      "tensor(-0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  147\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -989.0\n",
      "n_steps_episode 6\n",
      "tensor(2.6610e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  148\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -942.0\n",
      "n_steps_episode 26\n",
      "tensor(-1.3576e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  149\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -839.0\n",
      "n_steps_episode 78\n",
      "tensor(8.5852e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  150\n",
      "episode_rewards:  [-2, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 6\n",
      "tensor(-1.2692e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  151\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -984.0\n",
      "n_steps_episode 7\n",
      "tensor(5.9861e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  152\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -586.0\n",
      "n_steps_episode 206\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  153\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, -1, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -861.0\n",
      "n_steps_episode 85\n",
      "tensor(2.0368e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  154\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -946.0\n",
      "n_steps_episode 19\n",
      "tensor(1.8226e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  155\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.STAY: 0>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  156\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -939.0\n",
      "n_steps_episode 24\n",
      "tensor(8.1459e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  157\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -974.0\n",
      "n_steps_episode 10\n",
      "tensor(1.8404e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  158\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(2.1073e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  159\n",
      "episode_rewards:  [2.0, 2.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -919.0\n",
      "n_steps_episode 41\n",
      "tensor(3.1938e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  160\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.STAY: 0>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  161\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -891.0\n",
      "n_steps_episode 58\n",
      "tensor(-2.1807e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  162\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -847.0\n",
      "n_steps_episode 80\n",
      "tensor(5.8122e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  163\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -915.0\n",
      "n_steps_episode 42\n",
      "tensor(6.1631e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  164\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -979.0\n",
      "n_steps_episode 8\n",
      "tensor(3.1401e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  165\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -966.0\n",
      "n_steps_episode 17\n",
      "tensor(8.7445e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  166\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -970.0\n",
      "n_steps_episode 12\n",
      "tensor(1.0825e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  167\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(-4.2147e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  168\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -941.0\n",
      "n_steps_episode 37\n",
      "tensor(-5.3215e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  169\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -949.0\n",
      "n_steps_episode 20\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  170\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -708.0\n",
      "n_steps_episode 138\n",
      "tensor(-1.0979e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  171\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -953.0\n",
      "n_steps_episode 21\n",
      "tensor(1.0123e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  172\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -944.0\n",
      "n_steps_episode 21\n",
      "tensor(4.0604e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  173\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -756.0\n",
      "n_steps_episode 117\n",
      "tensor(-3.3834e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  174\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, -1, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -986.0\n",
      "n_steps_episode 12\n",
      "tensor(-5.3467e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  175\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -937.0\n",
      "n_steps_episode 28\n",
      "tensor(-1.3396e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  176\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 9\n",
      "tensor(-1.0442e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  177\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -738.0\n",
      "n_steps_episode 127\n",
      "tensor(-0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  178\n",
      "episode_rewards:  [-1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -923.0\n",
      "n_steps_episode 36\n",
      "tensor(3.6889e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  179\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, -1, -1, 1.0, 1.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -904.0\n",
      "n_steps_episode 59\n",
      "tensor(4.7483e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  180\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -838.0\n",
      "n_steps_episode 74\n",
      "tensor(-3.6386e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  181\n",
      "episode_rewards:  [1.0, 1.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -910.0\n",
      "n_steps_episode 38\n",
      "tensor(-6.4316e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  182\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -961.0\n",
      "n_steps_episode 14\n",
      "tensor(3.3594e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  183\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -914.0\n",
      "n_steps_episode 38\n",
      "tensor(5.0294e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  184\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -763.0\n",
      "n_steps_episode 99\n",
      "tensor(6.3873e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  185\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -929.0\n",
      "n_steps_episode 40\n",
      "tensor(2.6242e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  186\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -796.0\n",
      "n_steps_episode 93\n",
      "tensor(8.7048e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  187\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -843.0\n",
      "n_steps_episode 82\n",
      "tensor(9.9489e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  188\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -966.0\n",
      "n_steps_episode 19\n",
      "tensor(-5.9137e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  189\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -973.0\n",
      "n_steps_episode 23\n",
      "tensor(-2.5817e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  190\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -923.0\n",
      "n_steps_episode 31\n",
      "tensor(-7.3739e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  191\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -955.0\n",
      "n_steps_episode 16\n",
      "tensor(1.5673e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  192\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 12\n",
      "tensor(-3.6989e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  193\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -956.0\n",
      "n_steps_episode 26\n",
      "tensor(-9.1575e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  194\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 26\n",
      "tensor(-5.5032e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  195\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(3.7932e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  196\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -969.0\n",
      "n_steps_episode 15\n",
      "tensor(3.2022e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  197\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -787.0\n",
      "n_steps_episode 113\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  198\n",
      "episode_rewards:  [3.0, 3.0, -4, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -887.0\n",
      "n_steps_episode 48\n",
      "tensor(3.2281e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  199\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -967.0\n",
      "n_steps_episode 17\n",
      "tensor(-5.2722e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  200\n",
      "  Episode   200  of  10,000.    Elapsed: 0:00:27.\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 7\n",
      "tensor(1.2155e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  201\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -979.0\n",
      "n_steps_episode 16\n",
      "tensor(2.2680e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  202\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -973.0\n",
      "n_steps_episode 14\n",
      "tensor(4.5546e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  203\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -983.0\n",
      "n_steps_episode 9\n",
      "tensor(1.1649e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  204\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -867.0\n",
      "n_steps_episode 61\n",
      "tensor(5.2238e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  205\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -887.0\n",
      "n_steps_episode 56\n",
      "tensor(1.4720e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  206\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(9.4459e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  207\n",
      "episode_rewards:  [3.0, 3.0, -1, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -927.0\n",
      "n_steps_episode 42\n",
      "tensor(1.2605e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  208\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -928.0\n",
      "n_steps_episode 32\n",
      "tensor(5.5489e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  209\n",
      "episode_rewards:  [3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -961.0\n",
      "n_steps_episode 16\n",
      "tensor(1.7197e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  210\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -928.0\n",
      "n_steps_episode 29\n",
      "tensor(-2.3491e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  211\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -935.0\n",
      "n_steps_episode 32\n",
      "tensor(5.3186e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  212\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -936.0\n",
      "n_steps_episode 26\n",
      "tensor(1.8566e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  213\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -827.0\n",
      "n_steps_episode 96\n",
      "tensor(-3.5101e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  214\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(3.7932e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  215\n",
      "episode_rewards:  [1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -945.0\n",
      "n_steps_episode 32\n",
      "tensor(7.7925e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  216\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -950.0\n",
      "n_steps_episode 26\n",
      "tensor(-3.0519e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  217\n",
      "episode_rewards:  [-1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -986.0\n",
      "n_steps_episode 7\n",
      "tensor(2.1642e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  218\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1, -1, 2.0, 2.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -874.0\n",
      "n_steps_episode 58\n",
      "tensor(-1.9721e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  219\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -937.0\n",
      "n_steps_episode 25\n",
      "tensor(1.4522e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  220\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -973.0\n",
      "n_steps_episode 10\n",
      "tensor(2.9181e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  221\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -657.0\n",
      "n_steps_episode 168\n",
      "tensor(-4.9473e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  222\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -904.0\n",
      "n_steps_episode 42\n",
      "tensor(-6.9824e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  223\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -742.0\n",
      "n_steps_episode 118\n",
      "tensor(7.7943e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  224\n",
      "episode_rewards:  [2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -592.0\n",
      "n_steps_episode 179\n",
      "tensor(2.3822e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  225\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -966.0\n",
      "n_steps_episode 15\n",
      "tensor(1.2746e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  226\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -985.0\n",
      "n_steps_episode 6\n",
      "tensor(-2.2636e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  227\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 1.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -993.0\n",
      "n_steps_episode 7\n",
      "tensor(-3.6245e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  228\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -979.0\n",
      "n_steps_episode 8\n",
      "tensor(-4.3500e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  229\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -935.0\n",
      "n_steps_episode 29\n",
      "tensor(3.1965e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  230\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -954.0\n",
      "n_steps_episode 20\n",
      "tensor(-9.3863e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  231\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 15\n",
      "tensor(-3.3782e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  232\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -940.0\n",
      "n_steps_episode 23\n",
      "tensor(1.2393e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  233\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -973.0\n",
      "n_steps_episode 10\n",
      "tensor(-5.2635e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  234\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -893.0\n",
      "n_steps_episode 42\n",
      "tensor(5.8323e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  235\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -3, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -829.0\n",
      "n_steps_episode 88\n",
      "tensor(-7.8544e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  236\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 3.0, -1, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -826.0\n",
      "n_steps_episode 77\n",
      "tensor(-2.3547e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  237\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -946.0\n",
      "n_steps_episode 19\n",
      "tensor(1.6211e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  238\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -913.0\n",
      "n_steps_episode 33\n",
      "tensor(-5.0345e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  239\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -852.0\n",
      "n_steps_episode 62\n",
      "tensor(5.6320e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  240\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -736.0\n",
      "n_steps_episode 122\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  241\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -973.0\n",
      "n_steps_episode 17\n",
      "tensor(5.1705e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  242\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -643.0\n",
      "n_steps_episode 173\n",
      "tensor(4.6511e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  243\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -987.0\n",
      "n_steps_episode 10\n",
      "tensor(-3.3464e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  244\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -915.0\n",
      "n_steps_episode 37\n",
      "tensor(2.0481e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  245\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -2, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -518.0\n",
      "n_steps_episode 230\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  246\n",
      "episode_rewards:  [2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -949.0\n",
      "n_steps_episode 23\n",
      "tensor(-1.7715e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  247\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -908.0\n",
      "n_steps_episode 46\n",
      "tensor(-9.4239e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  248\n",
      "episode_rewards:  [0.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -955.0\n",
      "n_steps_episode 17\n",
      "tensor(2.2327e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  249\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -949.0\n",
      "n_steps_episode 21\n",
      "tensor(3.4302e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  250\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -876.0\n",
      "n_steps_episode 61\n",
      "tensor(5.9623e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  251\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -930.0\n",
      "n_steps_episode 37\n",
      "tensor(9.0021e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  252\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -816.0\n",
      "n_steps_episode 94\n",
      "tensor(4.8275e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  253\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -915.0\n",
      "n_steps_episode 38\n",
      "tensor(-5.4534e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  254\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -991.0\n",
      "n_steps_episode 4\n",
      "tensor(1.4326e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  255\n",
      "episode_rewards:  [1.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -987.0\n",
      "n_steps_episode 8\n",
      "tensor(-2.8458e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  256\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 1.0, 1.0, 1.0, 1.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -841.0\n",
      "n_steps_episode 80\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  257\n",
      "episode_rewards:  [0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -906.0\n",
      "n_steps_episode 56\n",
      "tensor(2.5008e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  258\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1, 3.0, -2, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -4, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -658.0\n",
      "n_steps_episode 187\n",
      "tensor(4.1874e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  259\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -869.0\n",
      "n_steps_episode 60\n",
      "tensor(1.5180e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  260\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 16\n",
      "tensor(-6.5777e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  261\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -876.0\n",
      "n_steps_episode 63\n",
      "tensor(2.4913e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  262\n",
      "episode_rewards:  [-1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -865.0\n",
      "n_steps_episode 58\n",
      "tensor(4.3844e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  263\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -913.0\n",
      "n_steps_episode 42\n",
      "tensor(1.3412e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  264\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -952.0\n",
      "n_steps_episode 17\n",
      "tensor(3.5464e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  265\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(4.2147e-08, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  266\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -929.0\n",
      "n_steps_episode 35\n",
      "tensor(-1.4607e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  267\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -903.0\n",
      "n_steps_episode 36\n",
      "tensor(2.2155e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  268\n",
      "episode_rewards:  [1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -927.0\n",
      "n_steps_episode 43\n",
      "tensor(2.2399e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  269\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 2.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -861.0\n",
      "n_steps_episode 86\n",
      "tensor(3.7371e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  270\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -931.0\n",
      "n_steps_episode 27\n",
      "tensor(1.7117e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  271\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -825.0\n",
      "n_steps_episode 76\n",
      "tensor(3.7075e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  272\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -959.0\n",
      "n_steps_episode 20\n",
      "tensor(-1.0016e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  273\n",
      "episode_rewards:  [3.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -678.0\n",
      "n_steps_episode 139\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  274\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -751.0\n",
      "n_steps_episode 109\n",
      "tensor(4.5027e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  275\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -977.0\n",
      "n_steps_episode 10\n",
      "tensor(2.2152e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  276\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -961.0\n",
      "n_steps_episode 22\n",
      "tensor(5.6487e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  277\n",
      "episode_rewards:  [3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -770.0\n",
      "n_steps_episode 112\n",
      "tensor(8.7882e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  278\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -914.0\n",
      "n_steps_episode 38\n",
      "tensor(6.8509e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  279\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -879.0\n",
      "n_steps_episode 61\n",
      "tensor(4.4322e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  280\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, 3.0, -2, 3.0, 3.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -946.0\n",
      "n_steps_episode 26\n",
      "tensor(4.1386e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  281\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -978.0\n",
      "n_steps_episode 14\n",
      "tensor(-3.2995e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  282\n",
      "episode_rewards:  [1.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -797.0\n",
      "n_steps_episode 89\n",
      "tensor(-6.5289e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  283\n",
      "episode_rewards:  [3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -681.0\n",
      "n_steps_episode 138\n",
      "tensor(-1.2263e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  284\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, -2, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -871.0\n",
      "n_steps_episode 58\n",
      "tensor(-9.1063e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  285\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, -4, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -3, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -529.0\n",
      "n_steps_episode 236\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  286\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -949.0\n",
      "n_steps_episode 21\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  287\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -922.0\n",
      "n_steps_episode 42\n",
      "tensor(3.1792e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  288\n",
      "episode_rewards:  [3.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -881.0\n",
      "n_steps_episode 67\n",
      "tensor(1.5911e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  289\n",
      "episode_rewards:  [3.0, 3.0, -1, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, -1, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -686.0\n",
      "n_steps_episode 146\n",
      "tensor(-1.5659e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  290\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -949.0\n",
      "n_steps_episode 27\n",
      "tensor(1.1586e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  291\n",
      "episode_rewards:  [3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -900.0\n",
      "n_steps_episode 44\n",
      "tensor(-1.3274e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  292\n",
      "episode_rewards:  [-1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -746.0\n",
      "n_steps_episode 107\n",
      "tensor(6.7996e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  293\n",
      "episode_rewards:  [-1, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -954.0\n",
      "n_steps_episode 25\n",
      "tensor(-8.2920e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  294\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -891.0\n",
      "n_steps_episode 51\n",
      "tensor(2.8397e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  295\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 19\n",
      "tensor(-1.6025e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  296\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -932.0\n",
      "n_steps_episode 31\n",
      "tensor(4.0981e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  297\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, -1, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -637.0\n",
      "n_steps_episode 169\n",
      "tensor(5.5402e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  298\n",
      "episode_rewards:  [1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 10\n",
      "tensor(1.0546e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  299\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -924.0\n",
      "n_steps_episode 28\n",
      "tensor(-7.2723e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  300\n",
      "  Episode   300  of  10,000.    Elapsed: 0:00:51.\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -2, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -2, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -746.0\n",
      "n_steps_episode 129\n",
      "tensor(3.5380e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  301\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -958.0\n",
      "n_steps_episode 21\n",
      "tensor(7.2460e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  302\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(5.8532e-08, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  303\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -849.0\n",
      "n_steps_episode 67\n",
      "tensor(-4.6543e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  304\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -958.0\n",
      "n_steps_episode 20\n",
      "tensor(-3.8347e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  305\n",
      "episode_rewards:  [1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -834.0\n",
      "n_steps_episode 75\n",
      "tensor(-5.7934e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  306\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -832.0\n",
      "n_steps_episode 86\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  307\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -2, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -990.0\n",
      "n_steps_episode 6\n",
      "tensor(1.6912e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  308\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -927.0\n",
      "n_steps_episode 35\n",
      "tensor(4.8915e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  309\n",
      "episode_rewards:  [3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -994.0\n",
      "n_steps_episode 3\n",
      "tensor(3.4534e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  310\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -975.0\n",
      "n_steps_episode 12\n",
      "tensor(-2.0369e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  311\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -805.0\n",
      "n_steps_episode 84\n",
      "tensor(1.8785e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  312\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, -1, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -841.0\n",
      "n_steps_episode 73\n",
      "tensor(3.0897e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  313\n",
      "episode_rewards:  [3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -984.0\n",
      "n_steps_episode 8\n",
      "tensor(1.7696e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  314\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -786.0\n",
      "n_steps_episode 105\n",
      "tensor(1.4915e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  315\n",
      "episode_rewards:  [-1, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -918.0\n",
      "n_steps_episode 39\n",
      "tensor(-1.0511e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  316\n",
      "episode_rewards:  [1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -687.0\n",
      "n_steps_episode 157\n",
      "tensor(-2.7719e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  317\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, -1, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -926.0\n",
      "n_steps_episode 30\n",
      "tensor(2.6851e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  318\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -969.0\n",
      "n_steps_episode 25\n",
      "tensor(-4.0573e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  319\n",
      "episode_rewards:  [3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -969.0\n",
      "n_steps_episode 16\n",
      "tensor(9.9531e-08, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  320\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -981.0\n",
      "n_steps_episode 9\n",
      "tensor(-1.9263e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  321\n",
      "episode_rewards:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -889.0\n",
      "n_steps_episode 64\n",
      "tensor(-3.9103e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  322\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -879.0\n",
      "n_steps_episode 55\n",
      "tensor(2.4912e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  323\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -962.0\n",
      "n_steps_episode 19\n",
      "tensor(3.9772e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  324\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -968.0\n",
      "n_steps_episode 13\n",
      "tensor(3.6902e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  325\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -824.0\n",
      "n_steps_episode 77\n",
      "tensor(0.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  326\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -4, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -775.0\n",
      "n_steps_episode 102\n",
      "tensor(-3.8856e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  327\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 7\n",
      "tensor(1.6854e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  328\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -700.0\n",
      "n_steps_episode 142\n",
      "tensor(3.4135e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  329\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -969.0\n",
      "n_steps_episode 17\n",
      "tensor(-4.9024e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  330\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -962.0\n",
      "n_steps_episode 15\n",
      "tensor(7.2668e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  331\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -692.0\n",
      "n_steps_episode 152\n",
      "tensor(-0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  332\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -965.0\n",
      "n_steps_episode 16\n",
      "tensor(-4.8948e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  333\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -2, 1.0, 1.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -979.0\n",
      "n_steps_episode 13\n",
      "tensor(-1.2399e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  334\n",
      "episode_rewards:  [-1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -914.0\n",
      "n_steps_episode 44\n",
      "tensor(3.9511e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  335\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1, 2.0, 2.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -839.0\n",
      "n_steps_episode 64\n",
      "tensor(8.4640e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  336\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -1, 3.0, -1, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -885.0\n",
      "n_steps_episode 56\n",
      "tensor(5.7658e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  337\n",
      "episode_rewards:  [2.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -4, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -763.0\n",
      "n_steps_episode 110\n",
      "tensor(-0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  338\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -974.0\n",
      "n_steps_episode 14\n",
      "tensor(1.7582e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  339\n",
      "episode_rewards:  [1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -761.0\n",
      "n_steps_episode 129\n",
      "tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  340\n",
      "episode_rewards:  [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -798.0\n",
      "n_steps_episode 87\n",
      "tensor(4.4749e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  341\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -934.0\n",
      "n_steps_episode 28\n",
      "tensor(4.6917e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  342\n",
      "episode_rewards:  [-1000]\n",
      "agent_moves:  [<Directions.STAY: 0>]\n",
      "cumulative_score_episode -1000\n",
      "n_steps_episode 1\n",
      "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  343\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -972.0\n",
      "n_steps_episode 16\n",
      "tensor(-6.3073e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  344\n",
      "episode_rewards:  [0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -891.0\n",
      "n_steps_episode 47\n",
      "tensor(8.0128e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  345\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -954.0\n",
      "n_steps_episode 17\n",
      "tensor(3.2809e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  346\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 3.0, -2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -956.0\n",
      "n_steps_episode 24\n",
      "tensor(4.3647e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  347\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -976.0\n",
      "n_steps_episode 9\n",
      "tensor(8.1987e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  348\n",
      "episode_rewards:  [3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -791.0\n",
      "n_steps_episode 105\n",
      "tensor(6.4370e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  349\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -889.0\n",
      "n_steps_episode 50\n",
      "tensor(-3.0908e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  350\n",
      "episode_rewards:  [3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, -2, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -695.0\n",
      "n_steps_episode 134\n",
      "tensor(5.9811e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  351\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1, 1.0, 1.0, 1.0, 1.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -803.0\n",
      "n_steps_episode 108\n",
      "tensor(2.7247e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  352\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -883.0\n",
      "n_steps_episode 45\n",
      "tensor(1.7911e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  353\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 2.0, 2.0, 2.0, -1, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, -3, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -829.0\n",
      "n_steps_episode 91\n",
      "tensor(5.5158e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  354\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -850.0\n",
      "n_steps_episode 62\n",
      "tensor(4.2526e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  355\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -926.0\n",
      "n_steps_episode 32\n",
      "tensor(-1.9803e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  356\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -903.0\n",
      "n_steps_episode 43\n",
      "tensor(4.6556e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  357\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -982.0\n",
      "n_steps_episode 7\n",
      "tensor(-2.5511e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  358\n",
      "episode_rewards:  [3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -997.0\n",
      "n_steps_episode 2\n",
      "tensor(4.2147e-08, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  359\n",
      "episode_rewards:  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, -2, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -902.0\n",
      "n_steps_episode 47\n",
      "tensor(2.2029e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  360\n",
      "episode_rewards:  [3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -910.0\n",
      "n_steps_episode 49\n",
      "tensor(2.0268e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  361\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -986.0\n",
      "n_steps_episode 6\n",
      "tensor(-3.5192e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  362\n",
      "episode_rewards:  [2.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -983.0\n",
      "n_steps_episode 7\n",
      "tensor(-8.6035e-07, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  363\n",
      "episode_rewards:  [2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -2, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, -1, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1, 3.0, 3.0, 3.0, -1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -704.0\n",
      "n_steps_episode 139\n",
      "tensor(4.2019e-06, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:01\n",
      "\n",
      "i_episode:  364\n",
      "episode_rewards:  [0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1000]\n",
      "agent_moves:  [<Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>, <Directions.STAY: 0>]\n",
      "cumulative_score_episode -962.0\n",
      "n_steps_episode 22\n",
      "tensor(6.5476e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m game_is_running:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# print('Step i: ', i)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 97\u001b[0m     sandpile_grid, agent_rewards, game_is_running \u001b[38;5;241m=\u001b[39m \u001b[43msandpile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# print(sandpile_grid)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# sandpile.print_grid_and_agent_pos(rl_policy_agent)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# print(agent_rewards)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# print(game_is_running)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     pos \u001b[38;5;241m=\u001b[39m rl_policy_agent\u001b[38;5;241m.\u001b[39mget_agent_pos()\n",
      "File \u001b[0;32m~/sandpile-ml/sandpile.py:68\u001b[0m, in \u001b[0;36mSandpile.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mis_in_game():\n\u001b[1;32m     64\u001b[0m         \n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# print('agent_pos (i,j) (Y, X): ', agent.get_agent_pos())\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;66;03m# print('Moving agent')\u001b[39;00m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;66;03m# have the agent choose a direction to move in\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m         direction \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_move\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# print('move: ', direction)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m         agent\u001b[38;5;241m.\u001b[39mmove_agent_in_direction(direction)\n",
      "File \u001b[0;32m~/sandpile-ml/agents.py:220\u001b[0m, in \u001b[0;36mRLPolicyAgent.choose_move\u001b[0;34m(self, sandpile)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_move\u001b[39m(\u001b[38;5;28mself\u001b[39m, sandpile):\n\u001b[0;32m--> 220\u001b[0m     action_idx, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43msandpile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_idx \u001b[38;5;241m=\u001b[39m action_idx\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prob \u001b[38;5;241m=\u001b[39m log_prob\n",
      "File \u001b[0;32m~/sandpile-ml/rl_agents.py:43\u001b[0m, in \u001b[0;36mPolicy.select_action\u001b[0;34m(self, sandpile, x_pos, y_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print('m: ', m)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print('action: ', action)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_training_episodes = 10000\n",
    "N_val_episodes = 1000\n",
    "gamma = 0.9\n",
    "\n",
    "optimizer = torch.optim.Adam(rl_policy.parameters(), lr=0.001, betas=(0.9, 0.998), eps=1e-9, weight_decay=1e-4)\n",
    "\n",
    "start_epoch = 0\n",
    "FROM_CHECKPOINT = not True\n",
    "if FROM_CHECKPOINT:\n",
    "    \n",
    "    checkpoint = torch.load(output_dir+'best_gait_model.tar')\n",
    "    g = checkpoint['model_state_dict']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Lowest Loss: {loss}')\n",
    "    save_best_model = SaveBestModel(output_dir+best_model_name, loss)\n",
    "    # print(g.keys())\n",
    "    model.load_state_dict(g)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'From epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "\n",
    "training_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "print(\"\")\n",
    "print('Training...')\n",
    "# Measure how long the training epoch takes.\n",
    "t0 = time.time()\n",
    "rl_policy.train()\n",
    "for i_episode in range(1, N_training_episodes+1):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print()\n",
    "    t_episode_start = time.time()\n",
    "    print('i_episode: ', i_episode)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Progress update every 40 batches.\n",
    "    if i_episode % 100 == 0 and not i_episode == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Episode {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_episode, N_training_episodes, elapsed))\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "    \n",
    "\n",
    "    # generate initial grid\n",
    "    initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print(initial_grid)\n",
    "\n",
    "\n",
    "    rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    agents = [rl_policy_agent]\n",
    "\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    pos = rl_policy_agent.get_agent_pos()\n",
    "    # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    agent_moves = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    # sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print('Step i: ', i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "        # print(sandpile_grid)\n",
    "        # sandpile.print_grid_and_agent_pos(rl_policy_agent)\n",
    "        # print(agent_rewards)\n",
    "        # print(game_is_running)\n",
    "        pos = rl_policy_agent.get_agent_pos()\n",
    "        # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        # print('action: ', action)\n",
    "        # print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "\n",
    "        # subtract expected value from just staying in the center\n",
    "        reward = reward - 1.75 * i\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        agent_moves.append(list(Directions)[action])\n",
    "\n",
    "        # input()\n",
    "\n",
    "    print('episode_rewards: ', episode_rewards)\n",
    "    print('agent_moves: ', agent_moves)\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    training_scores.append(cumulative_score_episode)\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # print('episode_rewards', episode_rewards)\n",
    "    print('cumulative_score_episode', cumulative_score_episode)\n",
    "\n",
    "    returns = deque(maxlen=max_nmoves_per_episode)\n",
    "    n_steps_episode = len(episode_rewards)\n",
    "\n",
    "    print('n_steps_episode', n_steps_episode)\n",
    "\n",
    "    #TODO: replace with reverse numpy cumsum?\n",
    "    for t in range(n_steps_episode)[::-1]:\n",
    "        discounted_return_t = returns[0] if len(returns) > 0 else 0\n",
    "        returns.appendleft(gamma * discounted_return_t + episode_rewards[t])\n",
    "\n",
    "    # print('Pre standard')\n",
    "    # print('log_probs: ', log_probs)\n",
    "    # print('returns: ', returns)\n",
    "\n",
    "\n",
    "    #standardize\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    else:\n",
    "        returns = (returns - returns.mean())\n",
    "\n",
    "    # compute loss\n",
    "    # print('Post standard')\n",
    "    # print('returns: ', returns)\n",
    "\n",
    "    # policy_loss = []\n",
    "    policy_loss = 0\n",
    "\n",
    "    for log_prob, disc_return in zip(log_probs, returns):\n",
    "        # print('log_prob ', log_prob)\n",
    "        # print('disc_return ', disc_return)\n",
    "        policy_loss += -log_prob * disc_return\n",
    "        \n",
    "        # policy_loss.append(-log_prob * disc_return)\n",
    "        # print(policy_loss)\n",
    "    print(policy_loss)\n",
    "    # policy_loss = torch.tensor(policy_loss).sum()\n",
    "\n",
    "    optimizer.zero_grad()   \n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    training_time = format_time(time.time() - t_episode_start)\n",
    "    print(\"  Training episode took: {:}\".format(training_time))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    # avg_train_score = np.mean(cumulative_scores_episode)     \n",
    "    # training_scores.append(avg_train_score)\n",
    "    # print(\"\")\n",
    "    # print(\"  Average training cumulative score: {0:.4f}\".format(avg_train_score))\n",
    "\n",
    "# Measure how long this episode took.\n",
    "training_time = format_time(time.time() - t0)\n",
    "print(\"  Training took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "rl_policy.eval()\n",
    "\n",
    "for episode_i in range(N_val_episodes):\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "\n",
    "\n",
    "    # generate initial grid\n",
    "    # run the sandpile 1000 times\n",
    "    initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print('initial grid')\n",
    "    # print(initial_grid)\n",
    "\n",
    "    rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    agents = [rl_policy_agent]\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print(i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        print('action: ', action)\n",
    "        print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    validation_scores.append(cumulative_score_episode)\n",
    "\n",
    "    #save best model\n",
    "    # save_best_model(\n",
    "    #     cumulative_score_episode, episode_i+1, rl_policy, optimizer\n",
    "    # )\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # avg_val_score = np.mean(cumulative_scores_episode)\n",
    "\n",
    "\n",
    "    # validation_scores.append(avg_val_score)\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "\n",
    "# print(\"  Validation Score: {0:.4f}\".format(avg_val_score))\n",
    "print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "\n",
    "#print training vals\n",
    "# print('Validation scores')\n",
    "# print(validation_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "#save model params\n",
    "# model_name = 'rl_policy_params.pt'\n",
    "# torch.save(rl_policy.state_dict(), output_dir+model_name)\n",
    "\n",
    "# model_name = 'rl_policy_full.pt'\n",
    "# torch.save(rl_policy, output_dir+model_name)\n",
    "\n",
    "#save model params\n",
    "model_name = 'rl_policy_params.pt'\n",
    "torch.save(rl_policy.state_dict(), model_name)\n",
    "\n",
    "model_name = 'rl_policy_full.pt'\n",
    "torch.save(rl_policy, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KFrle1f3fxJP",
    "outputId": "65ca3ae9-d33d-4184-bd58-99b6038128f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABg5klEQVR4nO3deXwM9/8H8NcmkQtJBEmo+6irKFLE0VIqjiqt/tqqOsq3etAv1daXb1vVaktVVatKL1Srrm/R1h1niBBCHEEIISEXct/Hfn5/pFlZ2SS7m9md2Z3X8/HIg5357Mx7Zndn3vP5fOYzGiGEABEREZGKOcgdABEREZHcmBARERGR6jEhIiIiItVjQkRERESqx4SIiIiIVI8JEREREakeEyIiIiJSPSe5A7AFWq0W8fHxqF27NjQajdzhEBERkRGEEMjMzETDhg3h4FB5HRATIiPEx8ejcePGcodBREREZoiLi0OjRo0qLcOEyAi1a9cGULJDPTw8ZI6GiIiIjJGRkYHGjRvrzuOVYUJkhNJmMg8PDyZERERENsaY7i7sVE1ERESqx4SIiIiIVI8JEREREake+xBJqLi4GIWFhXKHYbNq1KgBR0dHucMgIiIVYkIkASEEEhMTkZaWJncoNs/Lywt+fn4c74mIiKyKCZEESpMhHx8fuLu782RuBiEEcnJykJycDABo0KCBzBEREZGaMCGqpuLiYl0yVLduXbnDsWlubm4AgOTkZPj4+LD5jIiIrIadqquptM+Qu7u7zJHYh9L9yL5YRERkTUyIJMJmMmlwPxIRkRyYEBEREZHqMSEiIiIi1WNCRJJq1qwZlixZIncYREREJmFCpFIajabSv7lz55q13BMnTmDy5MnSBktERAblFRZDCCF3GHaBt92rVEJCgu7/GzZswJw5cxAVFaWbVqtWLd3/hRAoLi6Gk1PVX5f69etLGygRERmUkl2ArvOC0LOFN9ZPDpA7HJvHGiILEEIgp6BIlj9jrxT8/Px0f56entBoNLrXly5dQu3atbFz505069YNLi4uOHLkCK5evYoRI0bA19cXtWrVwiOPPIK9e/fqLff+JjONRoOffvoJTz/9NNzd3dG6dWv89ddfUu5uIqsoKtaioEhr9vtvZ+bjTla+hBGR2u2OTAQAHLuWInMkJYIv38bcvyKRX1QsdyhmYQ2RBeQWFqP9nN2yrPvCx4Fwd5bmY501axYWLVqEFi1aoE6dOoiLi8PQoUPx6aefwsXFBWvWrMHw4cMRFRWFJk2aVLicjz76CAsXLsQXX3yBpUuXYsyYMbhx4wa8vb0liZPI0oQQ6PP5AWQXFOHUB0+ghqNp15L5RcV45NOSi4crnw4x+f1EtmDcyjAAQEMvV0x+tKXM0ZiOv0qq0Mcff4wnnngCLVu2hLe3Nzp37oxXX30VDz30EFq3bo158+ahZcuWVdb4TJgwAaNHj0arVq3w2WefISsrC2FhYVbaCrIHWq3AvotJuJ0pTw1LQbEWiRl5yMwrws3UXJPfn55zb6DRnHzbvHomMtYtM34jSsAaIgtwq+GICx8HyrZuqfj7++u9zsrKwty5c7F9+3YkJCSgqKgIubm5iI2NrXQ5nTp10v2/Zs2a8PDw0D2zjNQh9Opd+Hi4oGX9WlUXNmDDyTjM3nwOHq5OODtXnt8WEdk3JkQWoNFoJGu2klPNmjX1Xr/zzjsICgrCokWL0KpVK7i5ueHZZ59FQUFBpcupUaOG3muNRgOt1vy+GGRbopMzMfrHYwCA6wuGmfz+34/H4vNdlwAAGXlFksYmhfTcQjy9LARDOvrh3cC2codDRGZikxkZLSQkBBMmTMDTTz+Njh07ws/PD9evX5c7LElptQLztl3AjnMJVRcmo0QlZpn93mPX7uK/W84hPVe5z7Zbc/Q6rt3JxrIDV+UOhahaEtPzZGuWVgImRGS01q1bY/PmzYiIiMCZM2fw4osv2l1Nz47zCfj5SAzeWHtK0uVeTMjAf/53Fgnpttm2Lpcbd7PlDqFKxRwDhuxATkERes7fh0c+3avacY2YEJHRFi9ejDp16qBXr14YPnw4AgMD0bVr1wrLFxWXJEtZecq9ur9fcoZlro6GfH0YG07GYdq6CIssX6l2nU/ElN+lTS7VICoxE3siExF7N0fuUKqUU1CEFYeu4tpt82sCSX4J6Xm6/2vVmQ+xDxGV3AU2YcIE3et+/foZvEJo1qwZ9u/frzdtypQpeq/LNqElZ+bjTFxqueWkpaVVK15bdikxQ+4QzBISfQe+Hq5o5WNap+jXfgu3UET2LXBJsO7/IbMexwNeblZbd1Z+EWo6O0Kj0RhV/ss9l/HzkRgs2HnJrD5iRErBGiKymGK1XmbYEGOqxi8lZmDMT8cxcPEhCCEQdCEJt9L0m/4KirTIK+Tt5JYQEZtmtXVdScrEQx/uxqu/Gp/InrxR/qKHKpdXWIztZxP0hmMg+TEhsmN5hcVIzsxjYkIGzd58FoO+Cq4ykbmUkKn7/87ziXhlzUn0XnCvplAIgUc+3YuOc3dXayRnkt+a0BsAgD0Xkqy63sy8QlX1W/l81yVM+f0Uxq48LncoVAYTIjt2OSkTiel5SMrIq7owqc66sDhcSc4y6eQXevVuuWkFxVqk5xaisFiw0ziZ7Gj0HXScuwfvbz0vdyhVCrqQhOe+D0VcSvX6dv0ZEQ8AOHszvVrLUVEOaRVMiFQgp4BNGVQxOa/M03IKJO+Me/V2FpsiKhB7N0dxz1P7MugyAGDt8coHeFWCV9acRFhMCmZtPit3KBXKKShCVGJm1QWpHCZERCSbhz8OwuNfHsL1O6bfXp9fVIzvDkbjYsK9jupXb2dhwJeH0PnjPVKGaRduZ+bj0S8OwP+TvVUXpkqlZis34R685DAClwQj+PJtuUOxOUyIiEh25nTM/eHQNSzcFYUhXx/WTTt2rXyTHgDsOp+A97acQ2GxdH2cbK3Py+Uk5dUaJKTnIrzMZz/mp2P4NfR6tZe7/OBVLD+ozoEyY/9pztt2Nl7mSGwPEyIiksWp2OrdnXTulvH9L1777RTWHo/FxpNx1VpnqWUHotFz/r5yd9tVRM7UKTu/CH9G3EKmzOOBGUogP/wzUu91SPRdfHDfNFOl5xTi812X8PmuS7Jvc0VSsit/3BHJg+MQEZEsnvnuqNXXaepjCYQQ+HxXFHw9XPSmfbE7CgDw5e4oNKnrbloQxg3vo5OaU72T58w/zmL72QQ4Opi4Ygn1XrAft9JycfCdfmhW794zEtMs8EiWgjK1gCOWhWDNxO5oVMfEz4hUiTVEdsoaz37q168fpk+fbvH1kHnyCovxZ8Qt3FVYJ1pbcu5WOlYcuoqP/r5gcL41an5Mufvq9+OxeP77UL3f//azJc/ls9jwG1U0HR67dldXk/bJdsP70VKu3c62ibvXSBmYENmpqp4BNXz4cAwePNjgvMOHD0Oj0eDsWeXeSWGrrNl08uWeKExbH4HnfzhmxbUax1bGK8rMK5I7BJP8d8s5HI9JwYpDyuk/czP1XrOiHN2ulPxgYFIWJkQqNWnSJAQFBeHmzZvl5q1atQr+/v7o1KmTDJGZJ/jybYxYFmLS7aZXb2dh+vrTiE6+9551Ycq/9ddYO84lAgCik5X3jKnpG07LHYJdy8m3rUTOGmytE7ySRManq+KuNSZEKvXkk0+ifv36WL16td70rKwsbNq0CSNHjsTo0aPxwAMPwN3dHR07dsS6devkCdYAIYTeAW7cyjCciUvD5F9P6pW7ejsLS/ZeRoaBzpUv/ngMWyPi8UKZGpQrVk4eIuLSMGJZCE5eTzH5vflFxVi67wrOm9C5WClKkzUiqf1y9Hq5abkFxei/6CDe3njG+gHZgWHfHMG4lWGIMWN4DFvChMgShAAKsuX5M/IqyMnJCePGjcPq1av1EotNmzahuLgYL730Erp164bt27fj/PnzmDx5MsaOHYuwsDBL7TWTTFx9AsO/PVKuX0TqfXdvDPjyEJbsvYKPDfQBSfrnyfZ3suS74+O5FaE4E5eGZ1eEmvzenw7H4Mugy3hy6RELRGa/hBD4/tBVg6Nuk207cT0F3x6ILjd9z4VEXL+bgz9Ola8RVwOpasdiqzlCt9LxLjNLKMwBPmsoz7r/Gw8416y6HICJEyfiiy++wKFDh9CvXz8AJc1lo0aNQtOmTfHOO+/oyr755pvYvXs3Nm7ciO7du0saslYrIACT7oI5EFVSfXspMQMdGnpWWf50NW/xtpSCaoyLc6HMgIRkvD0XkjB/5yUAqPLp7PLdl0XmiK9gGAQ1t5ZNWXsKUUmZ2PHvvnB2Yh1IZbh3VKxt27bo1asXVq5cCQCIjo7G4cOHMWnSJBQXF2PevHno2LEjvL29UatWLezevRuxsdL2sRFC4Hx8OiLj06G976iVllNQ5Z0xGp6yyESxd+37KrcsqwzOp+FvUC7CiNs0tp9LQHRyFo5evWOFiGwba4gsoYZ7SU2NXOs2waRJk/Dmm29i2bJlWLVqFVq2bInHHnsMn3/+Ob7++mssWbIEHTt2RM2aNTF9+nQUFEjbvFQ23ykqU1sSl5KNJ745hs6NvfDnlN6SrlMJQq/exfoT9tOB25Cq7nQky5v6u7I7r7Ojs/VomLhWiQmRJWg0RjdbSS0lOx8pJjxn57nnnsO0adPw+++/Y82aNXj99deh0WgQEhKCESNG4KWXXgIAaLVaXL58Ge3bt7dU6Hr2XkwGAJyJS7PK+qxt9I/KuxVean+fsf9HB/AUg2q1R524rsymbKoeW02+2GRmZ26m5iKnwPhbbmvVqoXnn38es2fPRkJCAiZMmAAAaN26NYKCgnD06FFcvHgRr776KpKSkiwUNZFCGTiu2+rBnogqx4RIBaoaBG/SpElITU1FYGAgGjYs6Qz+/vvvo2vXrggMDES/fv3g5+eHkSNHWiFay7ibXYARy0Lw+3H7bqYiy9ts4p1KxVqBk6wJAQDsu5SMi7wZQDLsQyktNpmpQJG28oQoICCgXFu+t7c3tm7dWun7Dh48WM3IKib1zzwtpxBpOWk4E5eGF3s0kXjp9ik5Mw/1a7lUXVBllu4vf1t3ZVYcuqp79hkBT38XgkvzhsgdhupYs7+WrfYNYw0RSa6gqLjceED0Dxs5Tmw/m4Dun+7D7M3nZI3DRo+retaEXpc7BLNsPBlX7WVk5Rch/IZ+7VheoW08toXUhwkRSe5SYibiUnOq/ZRuks+Xe0pqNNafqP5JUU7xabn4fNcls99viSYJU/r4yWnm/+49y9DcB8OOXBZiV4/DIfvGhIjImmy8yd+U/sRKqN0ZtzIMyw8q50GnABAwf7/kT54vKNLibla+pMssdSkxAx3n7sZ3B01rKgSU+Rw9W7YnMhE/Hb4mdxh2i32ISJF4I495tFqB2ZvPoXNjL8mWacufhaETshK2J7ewGLVcpDv8Bi4JtshzpoQQ+GDreeQUFGPhrii80a+V5OuwtMy8ImTZycNuJ/8aDgB4pJm3pL9xKsGESCK22olMae7tRwWctWQUnZwJJwcHNKtn2nhWQReTsOFkHDacjMMDXm4Wis56lJC82AJLPXTzue9DbX6soOjkLLy/9bzcYUjqdqbptYFVnaHKnsJKjsMaFBVrkWJG14fjMaY/rFoJmBBVU40aNQAAOTk5cHOz3glICGGX46Hk5JQ8VqFAa3/bZqys/CIMXBwMALj22VA4mPCMt/Rc4wflNFZVuf7984u1AiaErGhV/sTsZDsrYkwypMZLQbVs8/M/HCvXKd4YlxIzodUKk45dSsCEqJocHR3h5eWF5OSSkZXd3d0tnqhk5xfiVmoefDxc4OXurDdPFBnO5vPy8iwaU1UxVLT+Yq0oKS8EUlMKkXL3Dry8vKC9pd6+B2Wv/oqFgIOVzrr5RcVYui8a/drUr8YytHh04QE0q+eOns3rShgdWYMdXmPZNWOeZVYd5iRDpWwxaWRCJAE/Pz8A0CVFliCEgFaUPBE+Pi0XWgEkxQON6ujXSiWnGn7as3Ou9WqvDMVQ0fq1QiA5LQ+AgIOHK+rV9S7Zn5H3OnBqtQJh11PQroEHPN1qWCps1RBCYM6fkeWmrwq5jm8PROPbA9FoYUJTXdmTaPiNVNxKy8WttFzFJkRqaN02dxPVsG/IsLK/Y3tsfTAGEyIJaDQaNGjQAD4+PigslL7JAgDG/nwc8Wm5+GVid3z6Vzjyi4oBAPve7qdX7l+bDxp8//3lLMlQDBWtP7egCJO3HIFWABte740GdWuXK/O/8JuY+cdZNKvrjoPv9pc4WvU5ezMdvx67UW66FHcEyX0YVeLIvfaeZChvj5tHqxVIysxDA0/b73tH5mFCJCFHR0c4OjpaZNkn4kpOVgeupCEhqxi5hSUJkaurq165W5nFBt9/fzlLMhRDRevXOhTpyjs43Nt3ZS9Q/vrnIaHX7+ZIGKV6ZVdjHBxLndyVelJValwkvSm/n8LO84n4fmw3BHbwkzucait3cWDnibkUOA6RStjLbaf27trtLHy6/aLutb3XLkitqj4V958keHfoPe9vPWew5lAtdp5PBAD8EMxxftSKCRGZrbCYQ/BLbcS3Idh7MUnuMFTjv1vs63bs6vjtGEeUNpYt1BxausO1PWJCRGb5Zt8VtH5vJ07HWmaMEmMPODb3o68i3EzW5BnNnH6fAkLvfXysBBGVYkJElcopKELw5dsoKNKvDVocdBkAMPfvC3KERSb67dgN5BUa7l9miLXTTHPWp4TWLiV24iYi8zAhokpNWXsK41aGVesBmSS/97ee1yWx1RGbwo7tZdlcDaUFcA/YBn5Xq8aEiCp1IOo2gJIaBmsydxiMjDz7anKS8iB25Mqdai8jKjHT6LI8/MqH9VZEpuNt9ySbb/dfwabwm3KHYRcmrzmJJzs3lDsMMoCJIZFtkLWGaP78+XjkkUdQu3Zt+Pj4YOTIkYiKitIrk5eXhylTpqBu3bqoVasWRo0ahaQk/btwYmNjMWzYMLi7u8PHxwfvvvsuior0awoOHjyIrl27wsXFBa1atcLq1astvXkWsfn0Ld0YREohhMD1O9nQak079C/acxk3OLaQJPZcSMK/152WOwy7O/mrdcReIjWSNSE6dOgQpkyZgmPHjiEoKAiFhYUYNGgQsrPvPbn5rbfewt9//41Nmzbh0KFDiI+PxzPPPKObX1xcjGHDhqGgoABHjx7FL7/8gtWrV2POnDm6MjExMRg2bBj69++PiIgITJ8+Hf/617+we/duq26vFC4mZMgdQjk/Hr6GfosOYu7f5R8HYS6eiIiIKmfKjQVlbwD4M+IW+i86iCtJxjeBq4GsCdGuXbswYcIEdOjQAZ07d8bq1asRGxuL8PBwAEB6ejp+/vlnLF68GI8//ji6deuGVatW4ejRozh27BgAYM+ePbhw4QJ+++03PPzwwxgyZAjmzZuHZcuWoaCg5CGjK1asQPPmzfHll1+iXbt2mDp1Kp599ll89dVXsm27rckv0iL48m2DtUALd5XU6q0JrV4/IyXcNUREtic5Mw9HrtzhMaQSZfsjTlsfgZg72Zix8YyMESmPojpVp6enAwC8vb0BAOHh4SgsLMTAgQN1Zdq2bYsmTZogNDQUABAaGoqOHTvC19dXVyYwMBAZGRmIjIzUlSm7jNIypcu4X35+PjIyMvT+CBi3MgxbTt+SOwyqhNz1anKv31ZwhGxp9Zq/Hy/9fBxBF0wf1PR2Zj7m/iVd7bYtMWUoDjVQTEKk1Woxffp09O7dGw899BAAIDExEc7OzvDy8tIr6+vri8TERF2ZsslQ6fzSeZWVycjIQG5u+Sezz58/H56enrq/xo0bS7KN9mDfJcuNosxWsvJ43pRXVd9JNY5DVFSsxYGoZKTnlDzIWgl7oOifmuvgy7dNfu+MjRFYffS67rUStofkoZiEaMqUKTh//jzWr18vdyiYPXs20tPTdX9xcXFyh0QVUOMJSU5MWunHwzF4edUJPLviKAB5OtIfu3YXf0ZIU1sdGc8WAFMVFGlx42521QVtjCJuu586dSq2bduG4OBgNGrUSDfdz88PBQUFSEtL06slSkpKgp+fn65MWFiY3vJK70IrW+b+O9OSkpLg4eEBNze3cvG4uLjAxcVFkm0jKsvSJw9WKJGllSYiV5KzZIvhhR9K+pC28auNtn4essWhVs//EIrTsWlY/fIj6NfGR+5wJCNrDZEQAlOnTsWWLVuwf/9+NG/eXG9+t27dUKNGDezbt083LSoqCrGxsQgICAAABAQE4Ny5c0hOTtaVCQoKgoeHB9q3b68rU3YZpWVKl0FkCiEExq8Mw6u/npQ7FKuQs1bIlCbD6OQsnI5Ns1gsUuJdlNKITyvf5YEMk7L5vfR3tvGkfbWeyFpDNGXKFPz+++/4888/Ubt2bV2fH09PT7i5ucHT0xOTJk3CjBkz4O3tDQ8PD7z55psICAhAz549AQCDBg1C+/btMXbsWCxcuBCJiYl4//33MWXKFF0tz2uvvYZvv/0WM2fOxMSJE7F//35s3LgR27dvl23bpZRbUAw3Z0d5Vq7CTi43U3Nx6J++CjkFRXB3VkRFq+oNXHxI8mXaatqivl+l7RNC4OrtLLSoVwsODpV/80o/X+bV0pK1hmj58uVIT09Hv3790KBBA93fhg0bdGW++uorPPnkkxg1ahQeffRR+Pn5YfPmzbr5jo6O2LZtGxwdHREQEICXXnoJ48aNw8cff6wr07x5c2zfvh1BQUHo3LkzvvzyS/z0008IDAy06vZawod/nke7ObsQEZcmdyhkBEmPXzzrkQxYu2UZyw5EY+DiYMz567zcoUjCFu+klPXS1pgd5urqimXLlmHZsmUVlmnatCl27NhR6XL69euH06flH8lXar/8M/bPV0GX8cvE7jJHI52yx1xbOf7ezszHuVtp6PegT5VXeERkuyo7c129nYXxK8MwtX8rvNC9idHLXLSn5OHLvx2LxScjO1ZalkcXy1DMXWZEtm7g4kOYuPokNoVbqV1dgUfFsicKG7xAJKq22ZvP4WZqLmZtPid3KGQiJkREEknPLRmXZd/F5CpK2paqEhslJD6WGmDOVmonrYm7pHKFxVq5QzBICb9TpWNCRIrE8YWsT4o9LtdB968z8bKs11oJU24BRxSm8pjkSIsJEdmclOwCuUOwiBd/PFZumi0c8CyZFBi7bFvswGmK5Mw8uUPQuf8zUdKuV1AoNqfs52jvv6eKMCEim9N1XpDu/8KODoFHr96t9jKuyjhYXnXYW31gXEoOZv1xFtE2+nmoyf3fPWOS8KQM5SSoSmWLdyMyISJSsPuPKZcSM/DT4WsoKNLiZmr5Qem+D75mpcgMs8FjYBXM26CJq09g/Yk4PLviqKJqUKi8+z8eYz6vtBzL1lLnFBQBADLzCrHzXIJZfeSM6XYg52jjSsQR5VTC1s5TZU+s9nRCqe6mDF5yGACgFQJ3MvOrH1AZUuxmpX/PrNU3rfREk5ZTiHq17OsxQEpKeu8/Nlg7tMy8Qvx2LBZPdmqAxt7uki337M109GxRF6//dgpHou/gOf9GWPhs52ot0xKHUXs6NgOsIbIbSjpIkeV9tuOSye/hd0R6x67dxZUk273KPnczHT0+24vNp26avQw1f6/m/nUBn++6hKHfHLbI8o9E3wEAbDxp/udjC5buu4I1odflDoM1RGohVSJvb1cEliDlLuL+VrabqbkGmy5txdR1p5CUkY8ZG8/IHYpZ5E7Gjl0r6feXmVckbyAykWL/x6Xk4MugkkEpxwU0q/4Cq4EJESmSii86Fa2qz0WqwejyCovx6q/hSEiXNtlQcid8IUSFHVEtlRgXFZu+YA6JYZsdhqsi1yZlFygnmWSTmZ1gTYL6ZOYVmlTelO9IRbfdWutr9sepmzh0+TYuV9IcZcvnpPv378nrKXjk0334W6bxlMj+KDn5VyomRERV2HEuAc99Hyp5bUV1ZchQTW+txDsnX10DEU5cfQJ3svLx5jrDz1u05eTPlv18JAbf7LtiVNl3N51B+I1UC0dkPrWOLWQKJkRkEmsdmJV0Anhj7SmExaTgwz8jjSqvoNDJRhRrpT1ZWfI7qOSBGaVUVKzFvG0XsDjoMuLTqr4Y2hR+E6OWH7VCZLbh7M00hMWkyB2GSZgQERkpLde0Jiop2HK1tymx31/SmITYkififReTMOuPs8gvkqKmynY/QzUr+6nlWuhZedam1Qr8dNj8scpMqWV6+rujeO77UJt6sgA7VROR0ZRUcye1G3ez8VdEPMb1aoZJv5wEANzJss7BPJvPKjObLZ1wpWZKqq3RaLD9XAI+2X7RYvEYcicrH941na26TnMxIbIT1jpRWfKqvOyyeScLWduT3xxBZn4RLiVl6qYlZiir35ixLFknpbRfZtlH+Zjj/u2JSszE4Su30bNF3WotV4mq+yiZiu+CrPgOSVvCJjMyyc7ziTh85bZs67eFjoHKj9A+SL2fM/NLOqmX7feglMTcBr72FnE0+g5WhcQY/bs3Zzfd/57M/CKM/TkM1+9km7E05RJCWOT4eTszHwHz92PhLtMHi1Ua1hCRycb+HGbV9ZW98HjGxjst2kJCpwRVXW0mZ+Qh6EKSlaKRnlKups36Plox9hd/Og4AeNC3Nnq3qlduviV/Ttfv5lhu4UY4fytd8mVKvbuEAFYcuorEjDx8d/CqxEu3PtYQkWKUPc5WdMw9HZtW7fUsOxCN07HKvT22LCXkTwo5d+uxh4OvKZT4GVjTzVR5kxNrK9YKPLn0iOTL1VrggKKEY5RUWENEqvPF7ijJlqWUJhV7U9Vevb9mg58D2ZNTFhjPSMC+khdLYEJEZKduGTF2ClXO3mtmlNJ0Zy4lhq/AkAAAr/4ajs6NvSRdphL3f3WwyYyoGkwdJ8hWTkC2EieR0ii5EuZMXJrcISgaEyI7IkeHXbl//EpsKlFeRLaH+Zg+pTR1bDwZJ3cIimDpY218ep5Fl0+GMSGyE7EpOej2yV65wyCJWfM8WDYHsbW74Wx5RG9bMvevSCb8VrD3ou3eQWnL2IfITly7bV9jZhhLiSdC5UVkWZbInZRy0lVKHKZKzSmU/PloZD3Gfu9s9fupVKwhIjISDz5kDjkq2/4+E49nV0g/ZpeNVRwapTq/a0vVpJrTZDzzf2ekD0RlmBCR4h2+ckfuEFRNjpOgPXXqvivBs7bM2R3GjNll6gldQKimf1fZfXP/JislLywbx8aTN2WLw14wISJFsqcTor2r6KMy5Vyrtk/b1vpolVJ62Gr5Hv15Jl6W79CBS8l2faca+xARVWDympOoX9tF7jAkY0s5pmJiVUwgZIgUOYHCczyD/j4Tj0HtfSstY4k7cFcfvY7VR6/rXkuy/xX0ATAhUgke1k23xwLPyjK5iUJJRws7YYu1j0r5GggYaj5SSHCwXnKjhG/QqdhUtKhfS7Lllf2OSbkflfLdNQabzKhCSRnyjYWhhAMOlWepzyUqKRPxZUbWttR6lDhulU2R6eRmSydVa+E+kR4TIjLou4PR6PHZPrnDUD1brM0wxw/B19BrwX7dax7rlamkU7U6vpNlt7Oi76Oc+6JIq5Vt3faKTWZk0MJd0j0A1dpyCorw0k/H8XhbH7lDsSlqTULOxKUhsYLaUHs/9dtKcnMxIcPgdGuFv3jPZSwb09Xk94XFpMC/aR04OEgf6G/HYmGBxZrERr4+RmMNkcKtPBKDp749IncYVledH9r6sDicik3Doj2XpQuIKnQz1fiHyGqNHCywyqfdG71GA++9r61hxLIQvPpruOTrkZK9nXhM9UvoDb0mVWvbfi7BrPc9930o/jhludvhlTD2praKtruqvrtK+m4zIVK4j7ddwNmb6dVejgJ+N1aTX2Q/Vcm20Kl67l+RRpcd+s1ho5MiUh4h5Ks1u5KcJdOaq+evM/Emv8eW+rqVvevM1jEhIlIJU3IrU1KW7Pwio8teSsw0bqBCC102GnNHlBJzUEvFZPrAjOXZ0snbENuOnqTEhIhw7Npd7L/EhwmS7bLUSZknS322UGNpDZbueyXF4pU0HIKtYEJEeOGHY5i4+iSSM+W7zf5+PBGpW1WfvyW/H0rq01Dq6m3pm4tWHolBfLrpv3m59s/drPwqy2QZUVuplqTuj1O3VLOtUmFCRDopEjxzieybPRxelXBnlakxTPrlpO7/iel5OBWbWu0YPt52odrLAKxXEzFjozQPL31ljeEO9KYo1grkFBjfVGwqKfKYYq0waXBZBfwsZMeEiEhiZQ9mhcXV6+CthASEB0pl6Tl/H5757ijO36r+zRamUkIyWV17L5YkCdX9bb6y5mTVhYwQn5aLvMJiSZZ1v8h4w8MVSMXeKqCYEJFilP1x2cOBd09kIlq/txMbT8Tppinh+GGtmkAlbGt1KflrKEUtkTmU1Ina3BPygUvJaP3eTvx27EaF39OqmptCou8anG7q3pm4+gR+kehOLTaRVQ8TIqoWKYYEsGWVnRwm/zO2zcw/zlornCql5xZi6u+nZY2BnT1Jbm+sPQUAeH/reZkjKbnzMiVH/yLF3N/IvovJUoRkNXEpOfjXL9LUtEmBCRFVWxHHlbEZUYmZcodAtk45FUSKJOfR8FKiZZvIpDZjY4RJA7taGhMi0pGiKpxVtspuZrGmigZgVFKTiyHG3M2kFPlFxbKO4GwpszeftYljiaViVPpvRCrxacq5sxlgQkQSG/PTcbPfa04iYQPHTNX69kC0ye/ZHZkIQN6ksmwep/TT0vClR/Qeimtp1tof68LiEBGXVmU5e734kOwZZRY+QBqz/8uGcP5WOj7++wLScwotF1Q1MCEiSR29arijoTX9duyG1dYlIFBgwqNClHz81lTwf2uq6JliVbFUv6ScAsvc/SNVzcLlJOs+zkLAet+NXAvdeUVVK7ZQN4gnlx7BypAYfPS38Y/7sSYmRGR3blmxCWFPZBIefH8nfg29bpHlswZMXpfY56oce/tKKvkiRS5Dvj5s0eUr9XfFhEjlbKGdXsmmb4gAAHzw570rHu5SMoWtP+yW33f7E3MnW+4QZMGEiEglTOlvYcw5rvQxCaaeD3nbvb4LCbZ1Z5Baa1Ts9eLxwKVkTFgVhiQzHuNib5zkDoDIVthrB05zPfzRHkR/NlSWddvTqcnc/hpKqVmy19+FnIPDGvpkLTVkxsurTwAAYlNyLLJ8W8KEiGyavR6MLcWcq9yK3mLu+FNV3VK88kiMZM/Zsmfztl+0+jqt+XOzl1vPpUqsApcEm/weU36htzNtZ7gJS2GTGdm0ZBv7ESvjmt40O88nWHV9VSVDptzVZ8+qcyfQ9Wr0EVHSRYgttGIdu2a9O2+tvT9sYf+bggmRytn6F/rlVSfkDsGyFPD57I6898RsJZwLv9wTJXcIsknKkKafx+Uk85pf7HFUektvUUGR1uTnzinhdyaV0gTamAfYyt1PiwmRShjzA1PCld/W07dwNPqO3GGQBVV3DJJN4TfLTTO2ecXWm2H+Z2DbyTL2XkiqupCRTt0wMSFSwsFYQt8fuoq2H+ySO4wqMSEixYhOzsL0DRF4sRqjXSuBnR3L9EhxoN521jpNcFfMrAWxNlurc7H1pNJYG07GVVnG3hIXS5m/85LcIRiFCREpRqKd3PZp682Q9uKJr8p3Qk1Iz1XJ6dx6+H23D5l5RXKHIDsmRERUjtI7Lpub1Kw/EWfRjvh/n4lHbys+W0wOSqoUMTeWsu9T0OZIztLJqpK+C1Lgbfcqx4s7MiTBTmrrDDl3K81iy35z3WmLLZuILIs1RKRISrzyyLXQgz5tidx3gdgjBX7Vycqk6pd1/3GzOsdRqX7rWhs6ZjAhIjLSmZvp1V6GqWPHSPmYi6qOS7Zz2LJ9ttwZV87Q/4y4JclyrH2OTs8txO7IRMU3RVvCs8tD5Q7BaEyIVMIWTna2EGN15avwgKg0SrtgVVg4imEo8Xrrn4cpK5mhfHHp/mi8+ms4lu6/YvA99vx8v9JnHtoCWROi4OBgDB8+HA0bNoRGo8HWrVv15k+YMAEajUbvb/DgwXplUlJSMGbMGHh4eMDLywuTJk1CVlaWXpmzZ8+ib9++cHV1RePGjbFw4UJLbxpVIju/CONWhskdhs24k5WPWX+cRURcmtyhEFEZpjYr/X0m3kKR2JbSZFdpTfCyJkTZ2dno3Lkzli1bVmGZwYMHIyEhQfe3bt06vfljxoxBZGQkgoKCsG3bNgQHB2Py5Mm6+RkZGRg0aBCaNm2K8PBwfPHFF5g7dy5++OEHi22XLdBqBfZEJko28q0pVh6JQfDl21Zfr616f8t5rD8Rh5HLQiyyfIUdkyxOCa1VSjsRVIec+9OOdqMk7t8flt4/9rb/Zb3LbMiQIRgyZEilZVxcXODn52dw3sWLF7Fr1y6cOHEC/v7+AIClS5di6NChWLRoERo2bIi1a9eioKAAK1euhLOzMzp06ICIiAgsXrxYL3EqKz8/H/n5927NzcjIMHMLlWvDyTjM3nwOzo73cmJrHdcyJa5CtaeTiyFXkq0zwGBaToFV1kOkJHez+b2Xi9L60im+D9HBgwfh4+ODNm3a4PXXX8fdu/celBcaGgovLy9dMgQAAwcOhIODA44fP64r8+ijj8LZ2VlXJjAwEFFRUUhNNTyc+vz58+Hp6an7a9y4sYW2Tj4Ho5IBAAXFyunTYm5e8+/1EZLGoVZ7LybLHYJRNKhevxu1jLRsLda8HrGFR6lJEaLC8gTVUHRCNHjwYKxZswb79u3D559/jkOHDmHIkCEoLi65/TkxMRE+Pj5673FycoK3tzcSExN1ZXx9ffXKlL4uLXO/2bNnIz09XfcXF1f1EO62RoknBXM7Ftpzu7w1TjbVPfgq75tEREqm1Ep9RQ/M+MILL+j+37FjR3Tq1AktW7bEwYMHMWDAAIut18XFBS4uLhZbvhI4KDoVJqVSwnHMUAymJNO8+qb8ImWPKabUhMHe2dRpsUWLFqhXrx6io6MBAH5+fkhO1q/mLyoqQkpKiq7fkZ+fH5KS9J9aXPq6or5JaqDEGiKyTTx2k1zMTW6V3vR2Ky1X7hBUyaYSops3b+Lu3bto0KABACAgIABpaWkIDw/Xldm/fz+0Wi169OihKxMcHIzCwkJdmaCgILRp0wZ16tSx7gYoiNKvkhUenk26kJAh+YE2r1DZV9pEambp8Y2Ufh4xlawJUVZWFiIiIhAREQEAiImJQUREBGJjY5GVlYV3330Xx44dw/Xr17Fv3z6MGDECrVq1QmBgIACgXbt2GDx4MF555RWEhYUhJCQEU6dOxQsvvICGDRsCAF588UU4Oztj0qRJiIyMxIYNG/D1119jxowZcm22IsjZu7+iNbPWyvJmbT4n6fI+23FR0uWZi98d08h9d48xI07b6id6OjZN7hAkocZmO1kTopMnT6JLly7o0qULAGDGjBno0qUL5syZA0dHR5w9exZPPfUUHnzwQUyaNAndunXD4cOH9fr3rF27Fm3btsWAAQMwdOhQ9OnTR2+MIU9PT+zZswcxMTHo1q0b3n77bcyZM6fCW+7VwsFWjzY2YO/FJLy3RdrEQypSPzrgLzvu0G7P5B6qYpoN3hmqxgTBUi4kZGDcyjDFjdwva6fqfv36VfrD3L17d5XL8Pb2xu+//15pmU6dOuHw4cMmx2fPlJgP2dPw9WuPx8odgkUkpFt/IE97djszH9tsLKlkYkBSUOLgvIq+y4wsx8HeGn+rKSu/CC5OyutSZwvnHp4gzTd86REkyjBaPNkn/hSrhwmRWhnIh9ScIz304W484OUmdxhkBXL3nwGAZ747ih/G+ZuUDCWk5+KL3VEWjMo8cibEE1eflG/lZHcXQ8q7JCaLuL9pUuk1RHL8znirq+2ozrdX7v4zAHAlOQszNkaY9B5b6HfDBxBX7vrdHIyw0DMJgfK/CwV81W0KEyKVeHThAfy3TEdfQyeU5Ix8TF9/GuE3UiwbTAVnM94pZJgSajRIeuk5hVUXKuPa7SwLRSIdSz2A2J6cYdJYIbkTOCZEKpGaU4jfy3T0NVRDNGvzOWyNiMeo5aHWDE3HnjpVk+UwPZTX/YcO5uvVx4tBZWBCpFKGDmKxKTnWWbkReQ8PDyWy8ooQnaz8mgGSjlbpwyhXQeqhHZTMnmtvj8dYuKVAgZgQqZQ9/5DtyaAlh+QOgaysoNjyCYUlf/+rQmIstmwyTXU+5tE/HpMuEBvBhEilmA/ZhrxCZV9ty9Hmr63mSnkxYNmO5VFJmZIs5+SNVEmWY0lK6KBP0mFCRETVYu38IjWnEOfjM6y7UiuK592OAICfDl+TOwSrsVT/SUvna/Z2bcGEiKzPxn9EUYnSXAGT+cJtoPbAXGN/DpM7BJNY6qRrCzV58RKN3P67hUa2L2YNlkmYEKmUEn8nSozJkH+t4WBwtkzpp9mKx8NSeuTSUtPWZuYVSbKc+w+h284kSLJctWBCRETVooRENj7d+GYmG6h4MCi3QJqTpqVJNTijrX5OSpKSXSB3CDaFCRGRyikhoamuJXuvyB2CxWUXFMsdglGu3c6WaEnMiJTOHo4dZTEhIiKz8SqeLIXfrerjYLemYUJEViWEwPeHjLh7hEdDIouxhQ7Lyo9Q+eytBsfSmBCp1Lowy9zVUJVzt9JlWS8R2ZaKcraYO1I1ydm/fAuPGr7zfKJFl29tTIjIqrLyK+4YyosZIuuozoCCx65Z55EOhp63CADDlx6xyvpJfZgQkXLYQP3uM9/xad6kbsVWetZaRQlRZRdVRNXBhIgUY1ek8qtfT8WmyR2CothADktENuJ/4TdlXb8kCVFGRga2bt2KixcvSrE4UqmQ6Ltyh0BECmED/b4Vx9Z32cw/zsq6frMSoueeew7ffvstACA3Nxf+/v547rnn0KlTJ/zxxx+SBkhElqW2E4/KNpckYguVoem5hXKHYNPMSoiCg4PRt29fAMCWLVsghEBaWhq++eYbfPLJJ5IGSESWVVRsC4d6sjdSjWhN9/x0JEbuEGyaWQlReno6vL29AQC7du3CqFGj4O7ujmHDhuHKFfsfMZbM99Nh/mCV5u1NZ/DelnNmvTc9t5CdXMksI5fxBgVSFrMSosaNGyM0NBTZ2dnYtWsXBg0aBABITU2Fq6urpAGSfdl/KdmocmzWsK611XjadnRyloSRkDVYemDGRbujqr0MdtgnazMrIZo+fTrGjBmDRo0aoUGDBujXrx+Akqa0jh07ShkfEZGkbGGUZlv37YFo5NjIw2iJSjmZ86Y33ngD3bt3R1xcHJ544gk4OJTkVS1atGAfIiIihavOwIzGupmaa/F1WNOHf52XOwSyMLMSIgDw9/dHp06dEBMTg5YtW8LJyQnDhg2TMjYiIrtlDy1CRcUVPxpi0FfBVozE8n47Js/jjsh6zGoyy8nJwaRJk+Du7o4OHTogNrbki/Lmm29iwYIFkgZIRCQlNphJZ9IvJy2W2LFlk6zNrIRo9uzZOHPmDA4ePKjXiXrgwIHYsGGDZMEREUlNKSdahYRRLYcu35Y7BCLJmNVktnXrVmzYsAE9e/bU66DYoUMHXL16VbLgSL3soTmBlIl3LxGRIWbVEN2+fRs+Pj7lpmdnZ/MODiIiIrI5ZiVE/v7+2L59u+51aRL0008/ISAgQJrIyGKOXr0jdwhEJCMpL1wvJ2VKtiwiOZnVZPbZZ59hyJAhuHDhAoqKivD111/jwoULOHr0KA4dOiR1jCSx97fw9lEikkaxlm2QZB/MqiHq06cPzpw5g6KiInTs2BF79uyBj48PQkND0a1bN6ljJBViwytZClv1icgQk2uICgsL8eqrr+KDDz7Ajz/+aImYiIjsnpz1Kgt2XpRx7UTKZHINUY0aNfDHH39YIhayY3EpOXKHQASAd5kBwNXb2XKHQKQ4ZjWZjRw5Elu3bpU4FLJnvx2/IXcIRACU02SmkDAUSymfE6mHWZ2qW7dujY8//hghISHo1q0batasqTf/3//+tyTBERFJj2daW6Dh50RWZlZC9PPPP8PLywvh4eEIDw/Xm6fRaJgQERFRtQgOz0pWZlZCFBMTI3UcRERERLIxqw9RWUIICPZSJCIiIhtmdkK0Zs0adOzYEW5ubnBzc0OnTp3w66+/ShkbWcCRKxylmtSNnXWJyBCzmswWL16MDz74AFOnTkXv3r0BAEeOHMFrr72GO3fu4K233pI0SJLOSz8fR4t6NasuKDOetMhSWKFNRIaYlRAtXboUy5cvx7hx43TTnnrqKXTo0AFz585lQkREREQ2xawms4SEBPTq1avc9F69eiEhIaHaQRERERFZk1kJUatWrbBx48Zy0zds2IDWrVtXOygiIkthc6xtiEvJlTsEUhmzmsw++ugjPP/88wgODtb1IQoJCcG+ffsMJkpERErBfIiIDDGrhmjUqFE4fvw46tWrh61bt2Lr1q2oV68ewsLC8PTTT0sdI9kBjjpLRERKZlYNEQB069YNv/32m5SxEBEREcnCrBqiHTt2YPfu3eWm7969Gzt37qx2UGR/OAw/EREpmVkJ0axZs1BcXFxuuhACs2bNqnZQRESWwk7VRGSIWQnRlStX0L59+3LT27Zti+jo6GoHRcQ+R2QpShmYUSFhENE/zEqIPD09ce3atXLTo6OjUbOm8kdBJiIiIirLrIRoxIgRmD59Oq5evaqbFh0djbfffhtPPfWUZMGRZVy7ky13CESqxzpQImUxKyFauHAhatasibZt26J58+Zo3rw52rZti7p162LRokVSx0hEJBn2ISIiQ8y67d7T0xNHjx5FUFAQzpw5Azc3N3Tu3Bl9+/aVOj4iIkmxfxoRGWJSDVFoaCi2bdsGANBoNBg0aBB8fHywaNEijBo1CpMnT0Z+fr5FAiV14W36ZCn8bhGRISYlRB9//DEiIyN1r8+dO4dXXnkFTzzxBGbNmoW///4b8+fPlzxIUp/3tpyXOwQiIlIRkxKiiIgIDBgwQPd6/fr16N69O3788UfMmDED33zzDZ9lRgaxmYKUgt9FIjLEpIQoNTUVvr6+uteHDh3CkCFDdK8feeQRxMXFSRcdEZGdYsMdkbKYlBD5+voiJiYGAFBQUIBTp06hZ8+euvmZmZmoUaOG0csLDg7G8OHD0bBhQ2g0GmzdulVvvhACc+bMQYMGDeDm5oaBAwfiypUremVSUlIwZswYeHh4wMvLC5MmTUJWVpZembNnz6Jv375wdXVF48aNsXDhQlM2m4iIiOycSQnR0KFDMWvWLBw+fBizZ8+Gu7u73p1lZ8+eRcuWLY1eXnZ2Njp37oxly5YZnL9w4UJ88803WLFiBY4fP46aNWsiMDAQeXl5ujJjxoxBZGQkgoKCsG3bNgQHB2Py5Mm6+RkZGRg0aBCaNm2K8PBwfPHFF5g7dy5++OEHUzadqunYtbuYvv603GEQKQYb7oiUxaTb7ufNm4dnnnkGjz32GGrVqoVffvkFzs7OuvkrV67EoEGDjF7ekCFD9JrcyhJCYMmSJXj//fcxYsQIAMCaNWvg6+uLrVu34oUXXsDFixexa9cunDhxAv7+/gCApUuXYujQoVi0aBEaNmyItWvXoqCgACtXroSzszM6dOiAiIgILF68WC9xKis/P1/vbrmMjAyjt4kMi4hLQ0RcmtxhEBERGWRSDVG9evUQHByM1NRUpKam4umnn9abv2nTJnz44YeSBBYTE4PExEQMHDhQN83T0xM9evRAaGgogJJhALy8vHTJEAAMHDgQDg4OOH78uK7Mo48+qpe4BQYGIioqCqmpqQbXPX/+fHh6eur+GjduLMk2EZH8ODAjERli9rPMHB0dy0339vbWSzyqIzExEQD0OnGXvi6dl5iYCB8fH735Tk5O8Pb21itjaBll13G/2bNnIz09XffHjuJE9iM2JUfuEADwETpESmPWSNX2zsXFBS4uLnKHQUQWEBnPJnAiKs+sGiJr8PPzAwAkJSXpTU9KStLN8/PzQ3Jyst78oqIipKSk6JUxtIyy6yAiIiJ1U2xC1Lx5c/j5+WHfvn26aRkZGTh+/DgCAgIAAAEBAUhLS0N4eLiuzP79+6HVatGjRw9dmeDgYBQWFurKBAUFoU2bNqhTp46VtoaIiIiUTNaEKCsrCxEREYiIiABQ0pE6IiICsbGx0Gg0mD59Oj755BP89ddfOHfuHMaNG4eGDRti5MiRAIB27dph8ODBeOWVVxAWFoaQkBBMnToVL7zwAho2bAgAePHFF+Hs7IxJkyYhMjISGzZswNdff40ZM2bItNWkdsO+OSx3CEREdB9Z+xCdPHkS/fv3170uTVLGjx+P1atXY+bMmcjOzsbkyZORlpaGPn36YNeuXXB1ddW9Z+3atZg6dSoGDBgABwcHjBo1Ct98841uvqenJ/bs2YMpU6agW7duqFevHubMmVPhLfdElsY+LEREyqMRQnAE+SpkZGTA09MT6enp8PDwsOq6m83abtX1ERERyeX6gmGSLs+U87di+xARERERWQsTIiIiIlI9JkRERESkekyIiIiISPWYEBEREZHqMSEiIiIi1WNCRERERKrHhIiIiIhUjwkRERERqR4TIiIiIlI9JkRERESkekyIiIiISPWYEBEREZHqMSEiIiIi1WNCRERERKrHhIiIiIhUjwkRERERqR4TIiIiIlI9JkRERESkekyIiIiISPWYEBEREZHqMSFSMCGE3CEQERGpAhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVY0KkYHzYPRERkXUwISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiBeOzXYmIiKyDCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiBbtxN1vuEIiIiFSBCZGCvbH2lNwhEBERqYKiE6K5c+dCo9Ho/bVt21Y3Py8vD1OmTEHdunVRq1YtjBo1CklJSXrLiI2NxbBhw+Du7g4fHx+8++67KCoqsvammOU6a4iIiIiswknuAKrSoUMH7N27V/fayeleyG+99Ra2b9+OTZs2wdPTE1OnTsUzzzyDkJAQAEBxcTGGDRsGPz8/HD16FAkJCRg3bhxq1KiBzz77zOrbYioNNHKHQEREpAqKT4icnJzg5+dXbnp6ejp+/vln/P7773j88ccBAKtWrUK7du1w7Ngx9OzZE3v27MGFCxewd+9e+Pr64uGHH8a8efPwn//8B3PnzoWzs7O1N4eIiIgUSNFNZgBw5coVNGzYEC1atMCYMWMQGxsLAAgPD0dhYSEGDhyoK9u2bVs0adIEoaGhAIDQ0FB07NgRvr6+ujKBgYHIyMhAZGRkhevMz89HRkaG3p8cNKwgIiIisgpFJ0Q9evTA6tWrsWvXLixfvhwxMTHo27cvMjMzkZiYCGdnZ3h5eem9x9fXF4mJiQCAxMREvWSodH7pvIrMnz8fnp6eur/GjRtLu2FGYj5ERERkHYpuMhsyZIju/506dUKPHj3QtGlTbNy4EW5ubhZb7+zZszFjxgzd64yMDFmSIg2riIiIiKxC0TVE9/Py8sKDDz6I6Oho+Pn5oaCgAGlpaXplkpKSdH2O/Pz8yt11VvraUL+kUi4uLvDw8ND7IyIiIvtlUwlRVlYWrl69igYNGqBbt26oUaMG9u3bp5sfFRWF2NhYBAQEAAACAgJw7tw5JCcn68oEBQXBw8MD7du3t3r8pmL9EBERkXUousnsnXfewfDhw9G0aVPEx8fjww8/hKOjI0aPHg1PT09MmjQJM2bMgLe3Nzw8PPDmm28iICAAPXv2BAAMGjQI7du3x9ixY7Fw4UIkJibi/fffx5QpU+Di4iLz1hmBGREREZFVKDohunnzJkaPHo27d++ifv366NOnD44dO4b69esDAL766is4ODhg1KhRyM/PR2BgIL777jvd+x0dHbFt2za8/vrrCAgIQM2aNTF+/Hh8/PHHcm2SSZgPERERWYdGCCHkDkLpMjIy4OnpifT0dKv2J+o0dzcy8mxjVG0iIqLqur5gmKTLM+X8bVN9iNSGd5kRERFZBxMiIiIiUj0mRArGCiIiIiLrYEJEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxIVIwDkNERERkHUyIiIiISPWYEBEREZHqMSEiIiIi1WNCpGCpOYVyh0BERKQKTIiIiIhI9ZgQERERkeoxISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVY0JEREREqseESCFSswv0Xidn5MkUCRERkfowIVKAFYeuosu8IPxy9Lpu2r5LyfIFREREpDJMiBRgwc5LAIAP/4oEAPwYfA1f7rksZ0hERESq4iR3AFTepzsuyh0CERGRqrCGSGGEEHKHQEREpDpMiIiIiEj1mBApDCuIiIiIrI8JkcIwHyIiIrI+JkQKwz5ERERE1seEiIiIiFSPCZHCsH6IiIjI+pgQKQxbzIiIiKyPCRERERGpHhMihRFsNCMiIrI6JkQKk1+klTsEIiIi1WFCpDB/n4mXOwQiIiLVYUKkMPmFrCEiIiKyNiZECsMeRERERNbHhEhh4lJy5A6BiIhIdZgQKczqo9flDoGIiEh1mBARERGR6jEhklnMnWy5QyAiIlI9JkQye/q7ELlDICIiUj0mRDJLyymUOwQiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIjsnJd7DblDICIiUjwmREQ2bGLv5tV6/9T+rSSKhKzJ18MFv7/Sw6iyTg4aC0dDZB+YEMlICGU8235MjyZyh0Bm6vtgvWq9f2B7X4kiIWv6cZw/erWsh9Y+tSotV9PZEaO78/dNxmlRv6bcIciKCZGMtFbIh4zJuZ7p+gA+eqoDZg1pa/mASFL9HqwvdwjV8tiD9fFy72ZY9fIjRpV/vV9LfD+2m4WjUr5OjbwAAJteC6i03Pdj/SGgjAsvUxye2V/uEGzKtAGty0177bGWJi+nqbe7FOHYLFUlRMuWLUOzZs3g6uqKHj16ICwsTNZ4tFaoIXJ3dqyyjEajwfhezdCtaR2Lx1NWDceKq/L7tq5ezUdF/v14Kwx5yM8iy7aUNr61K6zF02jMbw75QQGJhbOTAz4c3gH92/jovg+OlTTx/Gcwk/ayvNydK53v5+lq1EWRkuyd8Rgaq/zEbCoB4NC7/dCvzb0LpFlD2iLorUcx58n2Rl1EjOraCC3qV17jaO9UkxBt2LABM2bMwIcffohTp06hc+fOCAwMRHJysmwxFVuwiui/Q9tiWKcG+HVS1f0MSk8/xnQ1+PqFh6ssM6W/cVcmbjUqTta+fqELIuY8gbo1nSusxq1f2wU7p/UtN/2NfobX71PbBTMGtcGSFx7GT+P8jYqxKov+r3Ol84/NHoAW9Sqvhu7SxKvS+a41HNC7VfkEMfhd46+i3xr4IL56vrOuz9DKCf4Y1MEP93/kZQ+oANCqiiaZ6ir7ndvx774Y27Mptr3Zx6xlfT6qIwaZ2ARY1UXA/6qogamudg08JF3ed2O66r1uWb8mmpRJLs5/FIiD7/STdJ1Ss9R3rn8b5damDuvYoNy00s9txhMPVr0AIdC0bk30bFFXb3Jr39qY2Kc5BrX3rbQvWadGnlj4bCe8Zcy67JhqEqLFixfjlVdewcsvv4z27dtjxYoVcHd3x8qVK2WLSWgF3JAn6V9AY1f88lIHTO7ph2XPtkErLw0a16p8PY5FuUBBNhwKc3XTmtVGuXLB07tjRHsv3WuUqYr/4/UATOjVDCffH4h3A6u+it/2Zh88/0jjCue7ODnAy90Z4R88gf1v99ObN7V/Kyx8thNOvDdQ74TyzeguuPBxIGZWUIvw9qAH/1m2Y6V9ZxY/1xmT+jRH2HsDqtyOkQ83RLemdfCcfyP8+75q689HdYSfpyt+mdhdN+05/0ZVLvN+i/6vc7nEBQCa1K38KlqjAV7u3Qxr/9UD0wa2xtNdGuGdwDa4vmAYHm9bfvsfe7A+Vk14BCMfbggACOzgi70zHjO47PeHtTN5OwzGWGbLWvvWxryRD6GNb+1K31PLxcng9Kc6P4BFz3XG6O5NjO5s/tFTHfBst0aY0KuZ3nrnPNkeEXOegH8zb6OWU5HKarvWvdITwzpKV1v59hMPol4tF93r/w5tC41Ggwm9m2Fi7+b4bVIP1HJxQrN6NY36HlqqNm5wBz/8NqkHXn2shW6ai1P5U1Hp97Cshc92gmsNh0ovzGqWqRW/8HEgfhrnj0vzBuPTpzsa9R5T1atVeS0dACx5/uFy01rUr4mY+UOx/+3H8M3oLnrz5o18CAff6Ye9Mx7Dm4+Xv/Gh7OcM3DsSN/B0Nbh+jUaD8x8FGpzXol5NrJnYHY4Omgp/WxVp7O2GFS91rXB+r5Z1sfrlRwx+lkD5lgA/D8PxW4tpW2+jCgoKEB4ejtmzZ+umOTg4YODAgQgNDS1XPj8/H/n5+brXGRkZlokrLwsXXSdKu9DbAP6nP+kwAFT2PVtT8k8XABdLyxUaeM+Kkn9Ky7TLW4ncfwp1a+qNbk2NP3k89IAnHvStjR8Pxxic71pB7dHMwW3wRj/9A4SjgwbFWoEezb3h7lzylR4f0BS/hN7QK9fA003vddh7A9D9030AgDruNZCaU4i6NZ3xTNdGeOaf3/jeGY8hJPoOPvwrslws9Wo5w8nRAX+83gsAEHs3B9/su6KbX9rPo1Gde+ttYqApoHm9mjgdmwagpCbnZmoORnVrhN2RiZgZ2BZuzo64ejtLV/7xtj4Y3OHeidTPwxWJGXkAgIAWdRF67S4AYPaQtpj8aOW1dR5u94ZlKE3clrzQBQtGdarwMwCAf/VtgaEdG2D25nOY0LsZdp9PxPoTcXplujbxwum4NLw3tB0+2X5RN31sz6b49VjJZ2Oo9s/BQYMFz3TErshEfDemK97aEIHdkUn4v24lJ/FeLfWvgndO6wutEHBzdoQbHDH/mZIT3xPtfbHnQiJupeZiz4WS93u41cDPR+595+rUdNbV8g1eEqybPrFPxQlV39b1cPjKHb1pLk4OyC/Slit7+ZMhWH30OuZtu1BuXv3aLpjYpzkW7blcbt6rj7WAg0aDzLxCZOYV4c+IeN28+5PRPW89imPX7uLF7k1w5maabnrpZ+/i5Ig5w9vrvefjEQ8hsIMfvtkfjTNx997z2IP14e7siJyCYkx+tAXScgvg39QbDTxd8eTSIwb3x9cvPIxp6yN0r38a549/rTmJOu41MKV/KxyPScGorg9g+aFrWPZiFzSqU/Ib8PVwwfeHrgEA/vdaLwz/9ojeb6WZgZrVwQ/54Tn/kgupwA5+2HEuATM2ngFQkkQUFGnxVOeGePd/ZzC6exO4OzvpLn7K1pA08HRFQy83hN9IhbuzIyI/Hoxrt7Pg6+GKPp/vR2oVD91+zr8RNp68iVouTlj3Sk888VVwuTJdmnih4wOeGBfQDK18amH6hgi9+Z0e8IRGoynXTDVtQGuM7dkUwL3asncD2+CL3VEAgFcfbYHZQ9sh5k42+i86qNsXAPBkp4a4kJCBRwwci11rOOL6gmH4eu8VfLX33ndu02sBek2vwzs3xN9n4su9HwC+H9sNr/4arnt9eObjBsuV+v2VngBKvlf/HdoO3T/bpze/fUMP3W+pSxMv/DzeuL6ElqIRSrnVyYLi4+PxwAMP4OjRowgIuFcFPnPmTBw6dAjHjx/XKz937lx89NFH5ZaTnp4ODw/pqrhj4pPR/IfyneFsxW+Ph+L9HSUnl+sLhunN23gyDinZBXDUaFDb1Ql9WtfDrdRchF67i5b1a2F455Irhti7ORi78jhGdW2E9NxC/BkRDy/3GuVqJm6m5uBmam65KmEAyMovQmZeYbmE5/XfwrHzfCL6t6mP9NxCbHw1AE6O+leiSRl5cHN2RGGRFufjM9C3VT043HdVL4TAiGUhSMrIw9YpvZGRW4S1x2/g5d7N0fy+g/a121k4eT0VdWs5Y0C7e7Uw68JicTAqGV+/0AUbTsRBowE+3X4R+UVaRH0yGPO2XYCXmzPeCWxjcF9n5RfB/5MgdGjoqUvASuUVFuNOVj7q1XKBaw1HaLUC1+9mo3m9mkb1MVpx6Cq8azrrTjT3i7mTjUNRyXB00OCDPyPxbLdGBpsK14XFYvvZBByJvoPOjb3w55TeunlhMSl47vtQTO3fCu8EtkFYTAp2nU/EO4EP6pLYiuQWFCP02h30allPl6SFRN/BmJ+O462BD2LawMp/Q8VagRtl9kd6biE6f7QHnRt54s+p95rnjl+7i+d/OIZ/D2it10zxwdbzugRu1YRH0LGRJ/w/2Yv+berjq+cfRmGxwM3UHEzfEIH3h7VH/douGLksBJMfbYH/Di1JXr7cE4Wl+6PxRHtfDGrvi9tZ+brEfsOJWPznj3P46vnOGPnwA7h2Jxst7vvs5vx5HmtCb+CjpzpgfK9mlW7r6B+Poam3O76oojkXAM7fStdLdP6a2luXyBtTfv3knujZoi5GLAvB5cRMHJ31OOrUdMbN1Bzd97EyN1NzULemC9ycHRGXkoP6te+9JyE9FwHz9wMAXurZBK19apfb9rzCYvzfilB0b+6ND55sf//i9RRrBXov2I/EjDxcmjcYaTmFWH4wGmP/SVhKpecU4qu9l5GdX4TwG6m4dicbAOBd0xkp2QVwd3bE6TlP4G5WAbzca8Dd2QkZeYXILSiGr4er7vuy49990b7hvfPF5lM3seNcAv7VtwV2nU/E9IGt9RKRTSfjsDsyCUtHd4GbgRqrvMLicvszr7AYtzPzTe5zlVdYjIJirS7m++e1/WCX3rQj/+kPtxqOqFvLBdduZ2Hu3xcwtX8rdG9eknjN3nwW68LiMOfJ9njoAU88930oxgU0xccjHtJbTrNZ23X/nzagNSb1bY5e8/fDzdkRh2f2r/L7Yo6MjAx4enoadf5mQmQgITJUQ9S4cWPJEyIIARTmIDu/CDVNqKrMLypGDQcHODhoDP5IpJRXWAygghqbGu7IK9JKuv6CIi0cHTSVNjUYSwiBfInjk1PZz10u1fm+Sf1dtcR339AytVqBGyk5aODpqpuXX1QMZ0eHChPOik5eFcVrzLZY6rdeulxjl59XWAwHjQZJGXm6E7FWK1CkFXA20PRV3diAimuMTVVUXFKLd/+FUWW2nL6JwmKB/+vWCDF3jLvQsPRx2dKKtQLBl29j9dHrcHd2xPKXqu6Ubcw2p+cUIqugCHVrOltt/5iSEKmiyaxevXpwdHREUlKS3vSkpCT4+ZVvw3dxcYGLi0u56ZLTaADnmqhZdRO0Hpcy5V1NfK+pqlq+1F9qKQ+oGo3Gpg9K93Nxkn9bqrM/pf4sLPHZGlqmg4OmXE1gVZ+FoeVUFq8x22Kp73Lpco1dfmm5srUSDg4aOFsgUZd6m01JhEo93eVefytj78Ky9eOOo4MG/dv6oH9bH6PfY8w2e7rXgKeCn56gik7Vzs7O6NatG/btu9d+qdVqsW/fPr0aIyIiIlInVdQQAcCMGTMwfvx4+Pv7o3v37liyZAmys7Px8ssvyx0aERERyUw1CdHzzz+P27dvY86cOUhMTMTDDz+MXbt2wdeXjy4gIiJSO1V0qq4uUzplERERkTKYcv5WRR8iIiIiosowISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9ZgQERERkeoxISIiIiLVU82zzKqj9OkmGRkZMkdCRERExio9bxvzlDImREbIzMwEADRu3FjmSIiIiMhUmZmZ8PT0rLQMH+5qBK1Wi/j4eNSuXRsajUbSZWdkZKBx48aIi4vjg2MtiPvZOrifrYf72jq4n63DUvtZCIHMzEw0bNgQDg6V9xJiDZERHBwc0KhRI4uuw8PDgz82K+B+tg7uZ+vhvrYO7mfrsMR+rqpmqBQ7VRMREZHqMSEiIiIi1WNCJDMXFxd8+OGHcHFxkTsUu8b9bB3cz9bDfW0d3M/WoYT9zE7VREREpHqsISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiGS1btgzNmjWDq6srevTogbCwMLlDUrT58+fjkUceQe3ateHj44ORI0ciKipKr0xeXh6mTJmCunXrolatWhg1ahSSkpL0ysTGxmLYsGFwd3eHj48P3n33XRQVFemVOXjwILp27QoXFxe0atUKq1evtvTmKdaCBQug0Wgwffp03TTuZ2ncunULL730EurWrQs3Nzd07NgRJ0+e1M0XQmDOnDlo0KAB3NzcMHDgQFy5ckVvGSkpKRgzZgw8PDzg5eWFSZMmISsrS6/M2bNn0bdvX7i6uqJx48ZYuHChVbZPCYqLi/HBBx+gefPmcHNzQ8uWLTFv3jy9Z1txP5snODgYw4cPR8OGDaHRaLB161a9+dbcr5s2bULbtm3h6uqKjh07YseOHaZvkCBZrF+/Xjg7O4uVK1eKyMhI8corrwgvLy+RlJQkd2iKFRgYKFatWiXOnz8vIiIixNChQ0WTJk1EVlaWrsxrr70mGjduLPbt2ydOnjwpevbsKXr16qWbX1RUJB566CExcOBAcfr0abFjxw5Rr149MXv2bF2Za9euCXd3dzFjxgxx4cIFsXTpUuHo6Ch27dpl1e1VgrCwMNGsWTPRqVMnMW3aNN107ufqS0lJEU2bNhUTJkwQx48fF9euXRO7d+8W0dHRujILFiwQnp6eYuvWreLMmTPiqaeeEs2bNxe5ubm6MoMHDxadO3cWx44dE4cPHxatWrUSo0eP1s1PT08Xvr6+YsyYMeL8+fNi3bp1ws3NTXz//fdW3V65fPrpp6Ju3bpi27ZtIiYmRmzatEnUqlVLfP3117oy3M/m2bFjh3jvvffE5s2bBQCxZcsWvfnW2q8hISHC0dFRLFy4UFy4cEG8//77okaNGuLcuXMmbQ8TIpl0795dTJkyRfe6uLhYNGzYUMyfP1/GqGxLcnKyACAOHTokhBAiLS1N1KhRQ2zatElX5uLFiwKACA0NFUKU/IAdHBxEYmKirszy5cuFh4eHyM/PF0IIMXPmTNGhQwe9dT3//PMiMDDQ0pukKJmZmaJ169YiKChIPPbYY7qEiPtZGv/5z39Enz59Kpyv1WqFn5+f+OKLL3TT0tLShIuLi1i3bp0QQogLFy4IAOLEiRO6Mjt37hQajUbcunVLCCHEd999J+rUqaPb76XrbtOmjdSbpEjDhg0TEydO1Jv2zDPPiDFjxgghuJ+lcn9CZM39+txzz4lhw4bpxdOjRw/x6quvmrQNbDKTQUFBAcLDwzFw4EDdNAcHBwwcOBChoaEyRmZb0tPTAQDe3t4AgPDwcBQWFurt17Zt26JJkya6/RoaGoqOHTvC19dXVyYwMBAZGRmIjIzUlSm7jNIyavtspkyZgmHDhpXbF9zP0vjrr7/g7++P//u//4OPjw+6dOmCH3/8UTc/JiYGiYmJevvI09MTPXr00NvPXl5e8Pf315UZOHAgHBwccPz4cV2ZRx99FM7OzroygYGBiIqKQmpqqqU3U3a9evXCvn37cPnyZQDAmTNncOTIEQwZMgQA97OlWHO/SnUsYUIkgzt37qC4uFjvZAEAvr6+SExMlCkq26LVajF9+nT07t0bDz30EAAgMTERzs7O8PLy0itbdr8mJiYa3O+l8york5GRgdzcXEtsjuKsX78ep06dwvz588vN436WxrVr17B8+XK0bt0au3fvxuuvv45///vf+OWXXwDc20+VHScSExPh4+OjN9/JyQne3t4mfRb2bNasWXjhhRfQtm1b1KhRA126dMH06dMxZswYANzPlmLN/VpRGVP3O592TzZpypQpOH/+PI4cOSJ3KHYnLi4O06ZNQ1BQEFxdXeUOx25ptVr4+/vjs88+AwB06dIF58+fx4oVKzB+/HiZo7MfGzduxNq1a/H777+jQ4cOiIiIwPTp09GwYUPuZ9LDGiIZ1KtXD46OjuXuyklKSoKfn59MUdmOqVOnYtu2bThw4AAaNWqkm+7n54eCggKkpaXplS+7X/38/Azu99J5lZXx8PCAm5ub1JujOOHh4UhOTkbXrl3h5OQEJycnHDp0CN988w2cnJzg6+vL/SyBBg0aoH379nrT2rVrh9jYWAD39lNlxwk/Pz8kJyfrzS8qKkJKSopJn4U9e/fdd3W1RB07dsTYsWPx1ltv6Wo/uZ8tw5r7taIypu53JkQycHZ2Rrdu3bBv3z7dNK1Wi3379iEgIEDGyJRNCIGpU6diy5Yt2L9/P5o3b643v1u3bqhRo4befo2KikJsbKxuvwYEBODcuXN6P8KgoCB4eHjoTk4BAQF6yygto5bPZsCAATh37hwiIiJ0f/7+/hgzZozu/9zP1de7d+9yw0ZcvnwZTZs2BQA0b94cfn5+evsoIyMDx48f19vPaWlpCA8P15XZv38/tFotevTooSsTHByMwsJCXZmgoCC0adMGderUsdj2KUVOTg4cHPRPdY6OjtBqtQC4ny3FmvtVsmOJSV2wSTLr168XLi4uYvXq1eLChQti8uTJwsvLS++uHNL3+uuvC09PT3Hw4EGRkJCg+8vJydGVee2110STJk3E/v37xcmTJ0VAQIAICAjQzS+9HXzQoEEiIiJC7Nq1S9SvX9/g7eDvvvuuuHjxoli2bJmqbgc3pOxdZkJwP0shLCxMODk5iU8//VRcuXJFrF27Vri7u4vffvtNV2bBggXCy8tL/Pnnn+Ls2bNixIgRBm9b7tKlizh+/Lg4cuSIaN26td5ty2lpacLX11eMHTtWnD9/Xqxfv164u7vb9e3gZY0fP1488MADutvuN2/eLOrVqydmzpypK8P9bJ7MzExx+vRpcfr0aQFALF68WJw+fVrcuHFDCGG9/RoSEiKcnJzEokWLxMWLF8WHH37I2+5tzdKlS0WTJk2Es7Oz6N69uzh27JjcISkaAIN/q1at0pXJzc0Vb7zxhqhTp45wd3cXTz/9tEhISNBbzvXr18WQIUOEm5ubqFevnnj77bdFYWGhXpkDBw6Ihx9+WDg7O4sWLVrorUON7k+IuJ+l8ffff4uHHnpIuLi4iLZt24offvhBb75WqxUffPCB8PX1FS4uLmLAgAEiKipKr8zdu3fF6NGjRa1atYSHh4d4+eWXRWZmpl6ZM2fOiD59+ggXFxfxwAMPiAULFlh825QiIyNDTJs2TTRp0kS4urqKFi1aiPfee0/vNm7uZ/McOHDA4DF5/PjxQgjr7teNGzeKBx98UDg7O4sOHTqI7du3m7w9GiHKDNdJREREpELsQ0RERESqx4SIiIiIVI8JEREREakeEyIiIiJSPSZEREREpHpMiIiIiEj1mBARERGR6jEhIiIiItVjQkREdu369evQaDSIiIiw2DomTJiAkSNHWmz5RGR5TIiISNEmTJgAjUZT7m/w4MFGvb9x48ZISEjAQw89ZOFIiciWOckdABFRVQYPHoxVq1bpTXNxcTHqvY6OjvDz87NEWERkR1hDRESK5+LiAj8/P72/OnXqAAA0Gg2WL1+OIUOGwM3NDS1atMD//vc/3XvvbzJLTU3FmDFjUL9+fbi5uaF169Z6yda5c+fw+OOPw83NDXXr1sXkyZORlZWlm19cXIwZM2bAy8sLdevWxcyZM3H/IyG1Wi3mz5+P5s2bw83NDZ07d9aLiYiUhwkREdm8Dz74AKNGjcKZM2cwZswYvPDCC7h48WKFZS9cuICdO3fi4sWLWL58OerVqwcAyM7ORmBgIOrUqYMTJ05g06ZN2Lt3L6ZOnap7/5dffonVq1dj5cqVOHLkCFJSUrBlyxa9dcyfPx9r1qzBihUrEBkZibfeegsvvfQSDh06ZLmdQETVI4iIFGz8+PHC0dFR1KxZU+/v008/FUIIAUC89tpreu/p0aOHeP3114UQQsTExAgA4vTp00IIIYYPHy5efvllg+v64YcfRJ06dURWVpZu2vbt24WDg4NITEwUQgjRoEEDsXDhQt38wsJC0ahRIzFixAghhBB5eXnC3d1dHD16VG/ZkyZNEqNHjzZ/RxCRRbEPEREpXv/+/bF8+XK9ad7e3rr/BwQE6M0LCAio8K6y119/HaNGjcKpU6cwaNAgjBw5Er169QIAXLx4EZ07d0bNmjV15Xv37g2tVouoqCi4uroiISEBPXr00M13cnKCv7+/rtksOjoaOTk5eOKJJ/TWW1BQgC5dupi+8URkFUyIiEjxatasiVatWkmyrCFDhuDGjRvYsWMHgoKCMGDAAEyZMgWLFi2SZPml/Y22b9+OBx54QG+esR3Bicj62IeIiGzesWPHyr1u165dheXr16+P8ePH47fffsOSJUvwww8/AADatWuHM2fOIDs7W1c2JCQEDg4OaNOmDTw9PdGgQQMcP35cN7+oqAjh4eG61+3bt4eLiwtiY2PRqlUrvb/GjRtLtclEJDHWEBGR4uXn5yMxMVFvmpOTk64z9KZNm+Dv748+ffpg7dq1CAsLw88//2xwWXPmzEG3bt3QoUMH5OfnY9u2bbrkacyYMfjwww8xfvx4zJ07F7dv38abb76JsWPHwtfXFwAwbdo0LFiwAK1bt0bbtm2xePFipKWl6ZZfu3ZtvPPOO3jrrbeg1WrRp08fpKenIyQkBB4eHhg/frwF9hARVRcTIiJSvF27dqFBgwZ609q0aYNLly4BAD766COsX78eb7zxBho0aIB169ahffv2Bpfl7OyM2bNn4/r163Bzc0Pfvn2xfv16AIC7uzt2796NadOm4ZFHHoG7uztGjRqFxYsX697/9ttvIyEhAePHj4eDgwMmTpyIp59+Gunp6boy8+bNQ/369TF//nxcu3YNXl5e6Nq1K/773/9KvWuISCIaIe4bQIOIyIZoNBps2bKFj84gomphHyIiIiJSPSZEREREpHrsQ0RENo2t/kQkBdYQERERkeoxISIiIiLVY0JEREREqseEiIiIiFSPCRERERGpHhMiIiIiUj0mRERERKR6TIiIiIhI9f4fr3JjzyJH7kMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_scores = np.array(training_scores)\n",
    "validation_scores = np.array(validation_scores)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(training_scores,'-',label='Train')\n",
    "axs.set_ylabel('Scores')\n",
    "axs.plot(validation_scores,'-',label='Val')\n",
    "axs.set_xlabel('Episode')\n",
    "axs.legend()\n",
    "print(np.min(validation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn0gYmJ3dSt8",
    "outputId": "4d16be89-fe4d-456f-a13f-f1129cf94f86"
   },
   "outputs": [],
   "source": [
    "params = list(best_model.named_parameters())\n",
    "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "for p in params:\n",
    "    # print('p')\n",
    "    # print(p[0])\n",
    "    # print(p[1].data)\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7PxtSeLVlak"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../staging_area/gait-model ../full_models/\n",
    "!zip -r ../full_models/gait-model.zip ../full_models/gait-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7184MNpqVlal"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../full_models/gait-model/ gs://ml_gait_estimation/full_models/\n",
    "!gsutil cp ../full_models/gait-model.zip gs://ml_gait_estimation/full_models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
