{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoK10It3TUN2",
    "outputId": "115d4998-5725-4fe6-d9c4-7f5597f5eeff"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_built_with_cuda()\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from save_best_model import SaveBestModel\n",
    "from sandpile import Sandpile, run_sandpile_alone\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import datetime\n",
    "from rl_agents import Policy\n",
    "from agents import RLPolicyAgent\n",
    "from util import Directions\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "#RUN THIS ON COLAB\n",
    "ON_COLAB = False\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/Phase ML Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4YR0pTPWV3Dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /staging_area/reinforce-agent/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "model_nickname = 'reinforce-agent'\n",
    "\n",
    "output_dir = f'/staging_area/{model_nickname}/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# checkpoint_dir = 'checkpoints/'\n",
    "# if not os.path.exists(output_dir+checkpoint_dir):\n",
    "#     os.makedirs(output_dir+checkpoint_dir)\n",
    "\n",
    "\n",
    "best_model_name = 'best_agent.tar'\n",
    "save_best_model = SaveBestModel(output_dir+best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "Total Trainable Params: 7685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=102, out_features=32, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Dropout(p=0.0, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (10): GELU(approximate='none')\n",
       "    (11): Dropout(p=0.0, inplace=False)\n",
       "    (12): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (13): GELU(approximate='none')\n",
       "    (14): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=32, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def enum_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        total_params+=params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(rl_policy)\n",
    "\n",
    "\n",
    "# SET UP POLICY AGENT\n",
    "N_grid = 10\n",
    "num_hidden_layers = 4\n",
    "hidden_dim = 32\n",
    "input_dim = N_grid**2 + 2# The number of input variables. \n",
    "output_dim = len(Directions) # The number of output variables. \n",
    "\n",
    "rl_policy = Policy(\n",
    "    input_dim=input_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    device=device\n",
    ")\n",
    "enum_parameters(rl_policy)\n",
    "rl_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tensor(-1.6925, device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor(1.6031, device='cuda:0', grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXIMUM_GRAINS = 4\n",
    "max_nmoves_per_episode = 1000\n",
    "\n",
    "rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "agents = [rl_policy_agent]\n",
    "\n",
    "# start new sandpile with initial grid\n",
    "sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=None, MAX_STEPS=10)\n",
    "rl_policy.select_action(sandpile, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xJFUThsu9tzX",
    "outputId": "c4a12851-65aa-495b-95ca-c2d18abfe757",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial grid\n",
      "[[2. 3. 2. 2. 3. 3. 2. 3. 3. 1.]\n",
      " [3. 3. 3. 2. 2. 1. 3. 2. 1. 2.]\n",
      " [2. 3. 1. 0. 2. 3. 2. 2. 0. 1.]\n",
      " [3. 3. 2. 2. 0. 2. 3. 3. 1. 3.]\n",
      " [3. 2. 3. 2. 3. 1. 3. 3. 3. 2.]\n",
      " [1. 3. 3. 1. 3. 2. 3. 2. 2. 1.]\n",
      " [2. 2. 0. 2. 2. 3. 3. 1. 1. 3.]\n",
      " [2. 1. 3. 3. 2. 3. 1. 3. 1. 3.]\n",
      " [2. 3. 2. 2. 0. 3. 2. 1. 2. 2.]\n",
      " [2. 2. 1. 1. 2. 3. 2. 0. 2. 3.]]\n",
      "preset_sandgrain_locs [[8 0]\n",
      " [3 5]\n",
      " [9 7]\n",
      " ...\n",
      " [8 5]\n",
      " [9 0]\n",
      " [7 1]]\n",
      "\n",
      "Training...\n",
      "\n",
      "i_episode:  1\n",
      "episode_rewards:  [1.25, -0.5, -4.25, -5.0, -5.75, -7.5, -10.25, -12.0, -15.75, -17.5]\n",
      "agent_moves:  [<Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>, <Directions.UP: 3>, <Directions.LEFT: 1>, <Directions.DOWN: 4>, <Directions.RIGHT: 2>, <Directions.DOWN: 4>, <Directions.DOWN: 4>]\n",
      "cumulative_score_episode -77.25\n",
      "n_steps_episode 10\n",
      "log_probs:  [tensor(-1.4878, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6388, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4893, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4877, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4912, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.7711, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4886, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.6369, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>), tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6032, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6032, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6032, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>), tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -883.8145017077933\n",
      "policy_loss grad  None\n",
      "entropy_loss:  16.03260612487793\n",
      "=========================== rl_policy._test_counter_i :  1686 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  2\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1687 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  3\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1688 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  4\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1689 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  5\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1690 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  6\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1691 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  7\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1692 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  8\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1693 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  9\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1694 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  10\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1695 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  11\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1696 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  12\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1697 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  13\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1698 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  14\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1699 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  15\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1700 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  16\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1701 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  17\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1702 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  18\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1703 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  19\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1704 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  20\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1705 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  21\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1706 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  22\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1707 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  23\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1708 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  24\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1709 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  25\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8098/1576149735.py:212: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print('policy_loss grad ', policy_loss.grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1710 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  26\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1711 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  27\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1712 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  28\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1713 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  29\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1714 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  30\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1715 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  31\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1716 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  32\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1717 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  33\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1718 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  34\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1719 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  35\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1720 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  36\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1721 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  37\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1722 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  38\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1723 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  39\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1724 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  40\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1725 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  41\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1726 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  42\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1727 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  43\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1728 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  44\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1729 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  45\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1730 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  46\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1731 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  47\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1732 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  48\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1733 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  49\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1734 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  50\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1735 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  51\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1736 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  52\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1737 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  53\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1738 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  54\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1739 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  55\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1740 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  56\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1741 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  57\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1742 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  58\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1743 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  59\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1744 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  60\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1745 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  61\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1746 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  62\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1747 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  63\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1748 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  64\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1749 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  65\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1750 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  66\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1751 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  67\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1752 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  68\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1753 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  69\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1754 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  70\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1755 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  71\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1756 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  72\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1757 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  73\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1758 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  74\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1759 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  75\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1760 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  76\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1761 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  77\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1762 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  78\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1763 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  79\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1764 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  80\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1765 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  81\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1766 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  82\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1767 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  83\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1768 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  84\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1769 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  85\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1770 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  86\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1771 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  87\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1772 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  88\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1773 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  89\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1774 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  90\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1775 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  91\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1776 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  92\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1777 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  93\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1778 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  94\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1779 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  95\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1780 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  96\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1781 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  97\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1782 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  98\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1783 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  99\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1784 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  100\n",
      "  Episode   100  of  100,000.    Elapsed: 0:00:01.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1785 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  101\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1786 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  102\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1787 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  103\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1788 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  104\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1789 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  105\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1790 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  106\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1791 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  107\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1792 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  108\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1793 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  109\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1794 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  110\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1795 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  111\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1796 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  112\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1797 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  113\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1798 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  114\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1799 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  115\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1800 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  116\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1801 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  117\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1802 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  118\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1803 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  119\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1804 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  120\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1805 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  121\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1806 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  122\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1807 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  123\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1808 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  124\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1809 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  125\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1810 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  126\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1811 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  127\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1812 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  128\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1813 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  129\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1814 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  130\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1815 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  131\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1816 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  132\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1817 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  133\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1818 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  134\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1819 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  135\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1820 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  136\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1821 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  137\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1822 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  138\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1823 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  139\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1824 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  140\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1825 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  141\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1826 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  142\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1827 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  143\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1828 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  144\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1829 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  145\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1830 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  146\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1831 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  147\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1832 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  148\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1833 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  149\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1834 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  150\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1835 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  151\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1836 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  152\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1837 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  153\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1838 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  154\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1839 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  155\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1840 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  156\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1841 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  157\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1842 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  158\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1843 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  159\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1844 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  160\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1845 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  161\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1846 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  162\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1847 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  163\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1848 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  164\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1849 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  165\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1850 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  166\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1851 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  167\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1852 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  168\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1853 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  169\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1854 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  170\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1855 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  171\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1856 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  172\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1857 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  173\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1858 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  174\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1859 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  175\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1860 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  176\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1861 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  177\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1862 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  178\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1863 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  179\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1864 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  180\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1865 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  181\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1866 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  182\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1867 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  183\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1868 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  184\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1869 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  185\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1870 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  186\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1871 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  187\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1872 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  188\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1873 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  189\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1874 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  190\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1875 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  191\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1876 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  192\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1877 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  193\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1878 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  194\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1879 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  195\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1880 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  196\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1881 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  197\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1882 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  198\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1883 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  199\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1884 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  200\n",
      "  Episode   200  of  100,000.    Elapsed: 0:00:02.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1885 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  201\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1886 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  202\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1887 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  203\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1888 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  204\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1889 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  205\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1890 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  206\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1891 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  207\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1892 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  208\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1893 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  209\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1894 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  210\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1895 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  211\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1896 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  212\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1897 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  213\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1898 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  214\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1899 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  215\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1900 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  216\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1901 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  217\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1902 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  218\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1903 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  219\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1904 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  220\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1905 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  221\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1906 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  222\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1907 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  223\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1908 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  224\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1909 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  225\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1910 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  226\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1911 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  227\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1912 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  228\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1913 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  229\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1914 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  230\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1915 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  231\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1916 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  232\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1917 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  233\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1918 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  234\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1919 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  235\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1920 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  236\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1921 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  237\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1922 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  238\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1923 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  239\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1924 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  240\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1925 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  241\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1926 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  242\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1927 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  243\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1928 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  244\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1929 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  245\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1930 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  246\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1931 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  247\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1932 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  248\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1933 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  249\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1934 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  250\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1935 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  251\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1936 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  252\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1937 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  253\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1938 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  254\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1939 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  255\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1940 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  256\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1941 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  257\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1942 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  258\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1943 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  259\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1944 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  260\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1945 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  261\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1946 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  262\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1947 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  263\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1948 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  264\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1949 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  265\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1950 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  266\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1951 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  267\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1952 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  268\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1953 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  269\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1954 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  270\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1955 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  271\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1956 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  272\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1957 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  273\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1958 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  274\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1959 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  275\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1960 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  276\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1961 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  277\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1962 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  278\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1963 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  279\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1964 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  280\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1965 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  281\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1966 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  282\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1967 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  283\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1968 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  284\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1969 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  285\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1970 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  286\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1971 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  287\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1972 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  288\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1973 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  289\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1974 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  290\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1975 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  291\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1976 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  292\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1977 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  293\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1978 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  294\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1979 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  295\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1980 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  296\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1981 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  297\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1982 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  298\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1983 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  299\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1984 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  300\n",
      "  Episode   300  of  100,000.    Elapsed: 0:00:02.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1985 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  301\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1986 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  302\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1987 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  303\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1988 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  304\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1989 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  305\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1990 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  306\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1991 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  307\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1992 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  308\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1993 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  309\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1994 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  310\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1995 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  311\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1996 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  312\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1997 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  313\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1998 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  314\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  1999 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  315\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2000 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  316\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2001 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  317\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2002 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  318\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2003 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  319\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2004 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  320\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2005 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  321\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2006 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  322\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2007 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  323\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2008 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  324\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2009 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  325\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2010 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  326\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2011 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  327\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2012 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  328\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2013 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  329\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2014 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  330\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2015 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  331\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2016 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  332\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2017 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  333\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2018 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  334\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2019 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  335\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2020 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  336\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2021 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  337\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2022 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  338\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2023 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  339\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2024 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  340\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2025 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  341\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2026 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  342\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2027 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  343\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2028 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  344\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2029 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  345\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2030 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  346\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2031 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  347\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2032 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  348\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2033 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  349\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2034 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  350\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2035 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  351\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2036 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  352\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2037 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  353\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2038 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  354\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2039 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  355\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2040 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  356\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2041 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  357\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2042 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  358\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2043 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  359\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2044 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  360\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2045 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  361\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2046 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  362\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2047 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  363\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2048 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  364\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2049 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  365\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2050 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  366\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2051 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  367\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2052 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  368\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2053 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  369\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2054 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  370\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2055 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  371\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2056 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  372\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2057 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  373\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2058 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  374\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2059 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  375\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2060 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  376\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2061 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  377\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2062 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  378\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2063 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  379\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2064 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  380\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2065 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  381\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2066 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  382\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2067 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  383\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2068 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  384\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2069 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  385\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2070 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  386\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2071 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  387\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2072 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  388\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2073 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  389\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2074 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  390\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2075 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  391\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2076 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  392\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2077 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  393\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2078 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  394\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2079 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  395\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2080 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  396\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2081 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  397\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2082 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  398\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2083 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  399\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2084 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  400\n",
      "  Episode   400  of  100,000.    Elapsed: 0:00:03.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2085 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  401\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2086 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  402\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2087 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  403\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2088 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  404\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2089 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  405\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2090 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  406\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2091 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  407\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2092 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  408\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2093 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  409\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2094 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  410\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2095 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  411\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2096 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  412\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2097 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  413\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2098 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  414\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2099 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  415\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2100 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  416\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2101 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  417\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2102 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  418\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2103 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  419\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2104 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  420\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2105 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  421\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2106 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  422\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2107 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  423\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2108 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  424\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2109 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  425\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2110 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  426\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2111 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  427\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2112 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  428\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2113 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  429\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2114 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  430\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2115 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  431\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2116 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  432\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2117 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  433\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2118 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  434\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2119 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  435\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2120 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  436\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2121 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  437\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2122 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  438\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2123 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  439\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2124 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  440\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2125 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  441\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2126 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  442\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2127 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  443\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2128 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  444\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2129 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  445\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2130 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  446\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2131 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  447\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2132 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  448\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2133 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  449\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2134 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  450\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2135 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  451\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2136 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  452\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2137 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  453\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2138 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  454\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2139 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  455\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2140 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  456\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2141 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  457\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2142 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  458\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2143 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  459\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2144 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  460\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2145 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  461\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2146 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  462\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2147 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  463\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2148 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  464\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2149 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  465\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2150 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  466\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2151 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  467\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2152 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  468\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2153 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  469\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2154 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  470\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2155 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  471\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2156 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  472\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2157 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  473\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2158 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  474\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2159 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  475\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2160 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  476\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2161 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  477\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2162 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  478\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2163 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  479\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2164 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  480\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2165 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  481\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2166 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  482\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2167 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  483\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2168 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  484\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2169 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  485\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2170 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  486\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2171 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  487\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2172 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  488\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2173 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  489\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2174 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  490\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2175 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  491\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2176 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  492\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2177 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  493\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2178 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  494\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2179 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  495\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2180 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  496\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2181 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  497\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2182 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  498\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2183 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  499\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2184 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  500\n",
      "  Episode   500  of  100,000.    Elapsed: 0:00:03.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2185 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  501\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2186 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  502\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2187 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  503\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2188 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  504\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2189 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  505\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2190 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  506\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2191 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  507\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2192 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  508\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2193 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  509\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2194 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  510\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2195 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  511\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2196 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  512\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2197 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  513\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2198 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  514\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2199 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  515\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2200 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  516\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2201 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  517\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2202 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  518\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2203 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  519\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2204 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  520\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2205 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  521\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2206 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  522\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2207 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  523\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2208 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  524\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2209 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  525\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2210 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  526\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2211 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  527\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2212 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  528\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2213 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  529\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2214 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  530\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2215 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  531\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2216 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  532\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2217 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  533\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2218 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  534\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2219 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  535\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2220 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  536\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2221 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  537\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2222 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  538\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2223 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  539\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2224 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  540\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2225 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  541\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2226 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  542\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2227 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  543\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2228 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  544\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2229 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  545\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2230 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  546\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2231 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  547\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2232 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  548\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2233 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  549\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2234 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  550\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2235 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  551\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2236 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  552\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2237 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  553\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2238 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  554\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2239 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  555\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2240 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  556\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2241 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  557\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2242 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  558\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2243 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  559\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2244 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  560\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2245 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  561\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2246 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  562\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2247 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  563\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2248 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  564\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2249 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  565\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2250 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  566\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2251 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  567\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2252 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  568\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2253 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  569\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2254 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  570\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2255 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  571\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2256 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  572\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2257 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  573\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2258 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  574\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2259 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  575\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2260 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  576\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2261 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  577\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2262 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  578\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2263 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  579\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2264 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  580\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2265 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  581\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2266 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  582\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2267 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  583\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2268 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  584\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2269 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  585\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2270 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  586\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2271 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  587\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2272 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  588\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2273 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  589\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2274 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  590\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2275 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  591\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2276 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  592\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2277 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  593\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2278 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  594\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2279 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  595\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2280 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  596\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2281 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  597\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2282 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  598\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2283 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  599\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2284 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  600\n",
      "  Episode   600  of  100,000.    Elapsed: 0:00:04.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2285 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  601\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2286 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  602\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2287 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  603\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2288 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  604\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2289 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  605\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2290 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  606\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2291 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  607\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2292 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  608\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2293 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  609\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2294 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  610\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2295 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  611\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2296 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  612\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2297 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  613\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2298 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  614\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2299 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  615\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2300 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  616\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2301 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  617\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2302 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  618\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2303 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  619\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2304 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  620\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2305 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  621\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2306 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  622\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2307 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  623\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2308 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  624\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2309 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  625\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2310 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  626\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2311 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  627\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2312 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  628\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2313 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  629\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2314 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  630\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2315 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  631\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2316 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  632\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2317 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  633\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2318 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  634\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2319 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  635\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2320 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  636\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2321 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  637\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2322 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  638\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2323 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  639\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2324 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  640\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2325 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  641\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2326 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  642\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2327 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  643\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2328 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  644\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2329 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  645\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2330 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  646\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2331 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  647\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2332 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  648\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2333 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  649\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2334 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  650\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2335 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  651\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2336 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  652\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2337 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  653\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2338 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  654\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2339 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  655\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2340 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  656\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2341 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  657\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2342 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  658\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2343 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  659\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2344 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  660\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2345 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  661\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2346 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  662\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2347 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  663\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2348 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  664\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2349 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  665\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2350 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  666\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2351 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  667\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2352 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  668\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2353 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  669\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2354 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  670\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2355 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  671\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2356 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  672\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2357 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  673\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2358 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  674\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2359 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  675\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2360 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  676\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2361 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  677\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2362 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  678\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2363 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  679\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2364 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  680\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2365 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  681\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2366 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  682\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2367 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  683\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2368 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  684\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2369 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  685\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2370 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  686\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2371 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  687\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2372 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  688\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2373 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  689\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2374 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  690\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2375 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  691\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2376 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  692\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2377 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  693\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2378 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  694\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2379 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  695\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2380 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  696\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2381 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  697\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2382 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  698\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2383 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  699\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2384 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  700\n",
      "  Episode   700  of  100,000.    Elapsed: 0:00:05.\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2385 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  701\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2386 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  702\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2387 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  703\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2388 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  704\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2389 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  705\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2390 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  706\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2391 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  707\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2392 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  708\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2393 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  709\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2394 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  710\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2395 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  711\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2396 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  712\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2397 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  713\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2398 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  714\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2399 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  715\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2400 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  716\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2401 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  717\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2402 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  718\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2403 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  719\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2404 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  720\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2405 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  721\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2406 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  722\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2407 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  723\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2408 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  724\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2409 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  725\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2410 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  726\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2411 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  727\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2412 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  728\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2413 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  729\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2414 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  730\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2415 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  731\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2416 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  732\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  [tensor(1.6033, device='cuda:0', grad_fn=<NegBackward0>)]\n",
      "Post standard\n",
      "policy_loss:  -2.605353355407715\n",
      "policy_loss grad  None\n",
      "entropy_loss:  1.6032633781433105\n",
      "=========================== rl_policy._test_counter_i :  2417 ===================\n",
      "  Training episode took: 0:00:00\n",
      "\n",
      "i_episode:  733\n",
      "episode_rewards:  [-1.75]\n",
      "agent_moves:  [<Directions.DOWN: 4>]\n",
      "cumulative_score_episode -1.75\n",
      "n_steps_episode 1\n",
      "log_probs:  [tensor(-1.4888, device='cuda:0', grad_fn=<SqueezeBackward1>)]\n",
      "entropies:  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 176\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_probs: \u001b[39m\u001b[38;5;124m'\u001b[39m, log_probs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# print('returns: ', returns)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentropies: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#standardize\u001b[39;00m\n\u001b[1;32m    180\u001b[0m eps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39meps\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:183\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 183\u001b[0m                 value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PRINT_OPTS\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_training_episodes = 100000\n",
    "N_val_episodes = 1000\n",
    "gamma = 0.99\n",
    "beta_entropy = 1e5\n",
    "\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "FROM_CHECKPOINT = not True\n",
    "if FROM_CHECKPOINT:\n",
    "    \n",
    "    checkpoint = torch.load(output_dir+'best_gait_model.tar')\n",
    "    g = checkpoint['model_state_dict']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Lowest Loss: {loss}')\n",
    "    save_best_model = SaveBestModel(output_dir+best_model_name, loss)\n",
    "    # print(g.keys())\n",
    "    model.load_state_dict(g)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'From epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "\n",
    "training_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "agents = [rl_policy_agent]\n",
    "\n",
    "optimizer = torch.optim.Adam(rl_policy.parameters(), lr=1e-6, betas=(0.9, 0.998), eps=1e-9, weight_decay=1e-4)\n",
    "\n",
    "# For now, generate the initial grid once \n",
    "# generate initial grid\n",
    "initial_grid_N = N_grid * N_grid * 4\n",
    "print('Generating initial grid')\n",
    "initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "print(initial_grid)\n",
    "\n",
    "\n",
    "# generate preset sandgrain locations\n",
    "preset_sandgrain_locs = np.random.randint(0, N_grid, size=(max_nmoves_per_episode, 2))\n",
    "print('preset_sandgrain_locs', preset_sandgrain_locs)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print('Training...')\n",
    "# Measure how long the training epoch takes.\n",
    "t0 = time.time()\n",
    "rl_policy.train()\n",
    "for i_episode in range(1, N_training_episodes+1):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print()\n",
    "    t_episode_start = time.time()\n",
    "    print('i_episode: ', i_episode)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Progress update every 40 batches.\n",
    "    if i_episode % 100 == 0 and not i_episode == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Episode {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i_episode, N_training_episodes, elapsed))\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "    \n",
    "\n",
    "    # # generate initial grid\n",
    "    # initial_grid_N = N_grid * N_grid * 4\n",
    "    # # print('Generating initial grid')\n",
    "    # initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print(initial_grid)\n",
    "\n",
    "\n",
    "    # rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    # agents = [rl_policy_agent]\n",
    "\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    \n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode, grain_loc_order=preset_sandgrain_locs)\n",
    "\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "    # sandpile = Sandpile(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    # rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "    rl_policy_agent.move_agent_to_point(N_grid//2, N_grid//2)\n",
    "\n",
    "    pos = rl_policy_agent.get_agent_pos()\n",
    "    # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "    \n",
    "    episode_rewards = []\n",
    "    agent_moves = []\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "    i = 0\n",
    "    # sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print('Step i: ', i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "        # print(sandpile_grid)\n",
    "        # sandpile.print_grid_and_agent_pos(rl_policy_agent)\n",
    "        # print(agent_rewards)\n",
    "        # print(game_is_running)\n",
    "        pos = rl_policy_agent.get_agent_pos()\n",
    "        # print('Agent pos (ij): ', pos[0], pos[1])\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "        entropy = rl_policy_agent.entropy\n",
    "\n",
    "        # print('action: ', action)\n",
    "        # print('log_prob: ', log_prob)\n",
    "        # print('entropy ', entropy)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "\n",
    "        # subtract expected value from just staying in the center\n",
    "        reward = reward - 1.75 * i\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        agent_moves.append(list(Directions)[action])\n",
    "\n",
    "        # input()\n",
    "\n",
    "    print('episode_rewards: ', episode_rewards)\n",
    "    print('agent_moves: ', agent_moves)\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    training_scores.append(cumulative_score_episode)\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # print('episode_rewards', episode_rewards)\n",
    "    print('cumulative_score_episode', cumulative_score_episode)\n",
    "\n",
    "    returns = deque(maxlen=max_nmoves_per_episode)\n",
    "    n_steps_episode = len(episode_rewards)\n",
    "\n",
    "    print('n_steps_episode', n_steps_episode)\n",
    "\n",
    "    #TODO: replace with reverse numpy cumsum?\n",
    "    for t in range(n_steps_episode)[::-1]:\n",
    "        discounted_return_t = returns[0] if len(returns) > 0 else 0\n",
    "        returns.appendleft(gamma * discounted_return_t + episode_rewards[t])\n",
    "\n",
    "    # print('Pre standard')\n",
    "    print('log_probs: ', log_probs)\n",
    "    # print('returns: ', returns)\n",
    "    print('entropies: ', entropies)\n",
    "\n",
    "\n",
    "    #standardize\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # if len(returns) > 1:\n",
    "    #     returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    # else:\n",
    "    #     returns = (returns - returns.mean())\n",
    "\n",
    "    # compute loss\n",
    "    print('Post standard')\n",
    "    # print('returns: ', returns)\n",
    "\n",
    "    policy_loss = []\n",
    "    # policy_loss = 0\n",
    "\n",
    "\n",
    "    for log_prob, disc_return in zip(log_probs, returns):\n",
    "        # print('log_prob ', log_prob)\n",
    "        # print('disc_return ', disc_return)\n",
    "\n",
    "        # print('log_prob.grad ', log_prob.grad_fn)\n",
    "        # print('disc_return.grad ', disc_return.grad_fn)\n",
    "        # policy_loss += (-log_prob * disc_return)\n",
    "        \n",
    "        policy_loss.append(-log_prob * disc_return)\n",
    "        # print(policy_loss)\n",
    "\n",
    "    # print(policy_loss)\n",
    "\n",
    "    policy_loss = torch.tensor(policy_loss, requires_grad=True).sum()\n",
    "    print('policy_loss: ', policy_loss.item())\n",
    "    print('policy_loss grad ', policy_loss.grad)\n",
    "    \n",
    "\n",
    "    entropies = torch.tensor(entropies)\n",
    "    entropy_loss = torch.sum(entropies)\n",
    "    print('entropy_loss: ', entropy_loss.item())\n",
    "\n",
    "    loss = policy_loss - 0*(beta_entropy * entropy_loss)\n",
    "    optimizer.zero_grad()   \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # TEST COUNTER\n",
    "    rl_policy._test_counter()\n",
    "    print('=========================== rl_policy._test_counter_i : ', rl_policy._test_counter_i, '===================')\n",
    "\n",
    "    training_time = format_time(time.time() - t_episode_start)\n",
    "    print(\"  Training episode took: {:}\".format(training_time))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    # avg_train_score = np.mean(cumulative_scores_episode)     \n",
    "    # training_scores.append(avg_train_score)\n",
    "    # print(\"\")\n",
    "    # print(\"  Average training cumulative score: {0:.4f}\".format(avg_train_score))\n",
    "\n",
    "    # input()\n",
    "\n",
    "# Measure how long this episode took.\n",
    "training_time = format_time(time.time() - t0)\n",
    "print(\"  Training took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "rl_policy.eval()\n",
    "\n",
    "for episode_i in range(N_val_episodes):\n",
    "\n",
    "    # cumulative_scores_episode = []\n",
    "\n",
    "\n",
    "    # generate initial grid\n",
    "    # run the sandpile 1000 times\n",
    "    # initial_grid_N = N_grid * N_grid * 4\n",
    "    # print('Generating initial grid')\n",
    "    # initial_grid = run_sandpile_alone(N_grid=N_grid, initial_grid=None, MAXIMUM_GRAINS=MAXIMUM_GRAINS, DROP_SAND=True, MAX_STEPS=initial_grid_N)\n",
    "    # print('initial grid')\n",
    "    # print(initial_grid)\n",
    "\n",
    "    # rl_policy_agent = RLPolicyAgent(rl_policy=rl_policy)\n",
    "    # agents = [rl_policy_agent]\n",
    "\n",
    "    # start new sandpile with initial grid\n",
    "    sandpile = Sandpile(N_grid=N_grid, initial_grid=initial_grid, MAXIMUM_GRAINS=MAXIMUM_GRAINS, agents=agents, MAX_STEPS=max_nmoves_per_episode, grain_loc_order=preset_sandgrain_locs)\n",
    "\n",
    "    # move agent to random position at beginning of episode\n",
    "    # rl_policy_agent.move_agent_to_point(random.randint(0,N_grid-1), random.randint(0,N_grid-1))\n",
    "\n",
    "    rl_policy_agent.move_agent_to_point(N_grid//2, N_grid//2)\n",
    "\n",
    "    episode_rewards = []\n",
    "    log_probs = []\n",
    "    i = 0\n",
    "    game_is_running = True\n",
    "    while game_is_running:\n",
    "        # print(i)\n",
    "        i+=1\n",
    "        sandpile_grid, agent_rewards, game_is_running = sandpile.step()\n",
    "\n",
    "        # get action and log prob\n",
    "        action = rl_policy_agent.action_idx\n",
    "        log_prob = rl_policy_agent.log_prob\n",
    "\n",
    "        print('action: ', action)\n",
    "        print('log_prob: ', log_prob)\n",
    "\n",
    "\n",
    "        #only one agent is running so agent_rewards is a list with one element\n",
    "        reward = agent_rewards[0]\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "    cumulative_score_episode = np.sum(episode_rewards)\n",
    "    validation_scores.append(cumulative_score_episode)\n",
    "\n",
    "    #save best model\n",
    "    # save_best_model(\n",
    "    #     cumulative_score_episode, episode_i+1, rl_policy, optimizer\n",
    "    # )\n",
    "\n",
    "\n",
    "    # cumulative_scores_episode.append(np.sum(episode_rewards))\n",
    "\n",
    "    # avg_val_score = np.mean(cumulative_scores_episode)\n",
    "\n",
    "\n",
    "    # validation_scores.append(avg_val_score)\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "\n",
    "# print(\"  Validation Score: {0:.4f}\".format(avg_val_score))\n",
    "print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "\n",
    "#print training vals\n",
    "# print('Validation scores')\n",
    "# print(validation_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "#save model params\n",
    "# model_name = 'rl_policy_params.pt'\n",
    "# torch.save(rl_policy.state_dict(), output_dir+model_name)\n",
    "\n",
    "# model_name = 'rl_policy_full.pt'\n",
    "# torch.save(rl_policy, output_dir+model_name)\n",
    "\n",
    "#save model params\n",
    "model_name = 'rl_policy_params.pt'\n",
    "torch.save(rl_policy.state_dict(), model_name)\n",
    "\n",
    "model_name = 'rl_policy_full.pt'\n",
    "torch.save(rl_policy, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KFrle1f3fxJP",
    "outputId": "65ca3ae9-d33d-4184-bd58-99b6038128f3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(training_scores)\n\u001b[1;32m      2\u001b[0m validation_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(validation_scores)\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "training_scores = np.array(training_scores)\n",
    "validation_scores = np.array(validation_scores)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(training_scores,'-',label='Train')\n",
    "axs.set_ylabel('Scores')\n",
    "axs.plot(validation_scores,'-',label='Val')\n",
    "axs.set_xlabel('Episode')\n",
    "axs.legend()\n",
    "print(np.min(validation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn0gYmJ3dSt8",
    "outputId": "4d16be89-fe4d-456f-a13f-f1129cf94f86"
   },
   "outputs": [],
   "source": [
    "params = list(best_model.named_parameters())\n",
    "print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "for p in params:\n",
    "    # print('p')\n",
    "    # print(p[0])\n",
    "    # print(p[1].data)\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7PxtSeLVlak"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../staging_area/gait-model ../full_models/\n",
    "!zip -r ../full_models/gait-model.zip ../full_models/gait-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7184MNpqVlal"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../full_models/gait-model/ gs://ml_gait_estimation/full_models/\n",
    "!gsutil cp ../full_models/gait-model.zip gs://ml_gait_estimation/full_models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
